{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d820296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98e3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320993d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9178e71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28830</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590882</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201944</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  damage_grade\n",
       "0       802906             3\n",
       "1        28830             2\n",
       "2        94947             3\n",
       "3       590882             2\n",
       "4       201944             3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('../../csv/train_labels.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a601cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>260591</th>\n",
       "      <th>260592</th>\n",
       "      <th>260593</th>\n",
       "      <th>260594</th>\n",
       "      <th>260595</th>\n",
       "      <th>260596</th>\n",
       "      <th>260597</th>\n",
       "      <th>260598</th>\n",
       "      <th>260599</th>\n",
       "      <th>260600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <td>802906</td>\n",
       "      <td>28830</td>\n",
       "      <td>94947</td>\n",
       "      <td>590882</td>\n",
       "      <td>201944</td>\n",
       "      <td>333020</td>\n",
       "      <td>728451</td>\n",
       "      <td>475515</td>\n",
       "      <td>441126</td>\n",
       "      <td>989500</td>\n",
       "      <td>...</td>\n",
       "      <td>560805</td>\n",
       "      <td>207683</td>\n",
       "      <td>226421</td>\n",
       "      <td>159555</td>\n",
       "      <td>827012</td>\n",
       "      <td>688636</td>\n",
       "      <td>669485</td>\n",
       "      <td>602512</td>\n",
       "      <td>151409</td>\n",
       "      <td>747594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <td>487</td>\n",
       "      <td>900</td>\n",
       "      <td>363</td>\n",
       "      <td>418</td>\n",
       "      <td>131</td>\n",
       "      <td>558</td>\n",
       "      <td>475</td>\n",
       "      <td>323</td>\n",
       "      <td>757</td>\n",
       "      <td>886</td>\n",
       "      <td>...</td>\n",
       "      <td>368</td>\n",
       "      <td>1382</td>\n",
       "      <td>767</td>\n",
       "      <td>181</td>\n",
       "      <td>268</td>\n",
       "      <td>1335</td>\n",
       "      <td>715</td>\n",
       "      <td>51</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <td>12198</td>\n",
       "      <td>2812</td>\n",
       "      <td>8973</td>\n",
       "      <td>10694</td>\n",
       "      <td>1488</td>\n",
       "      <td>6089</td>\n",
       "      <td>12066</td>\n",
       "      <td>12236</td>\n",
       "      <td>7219</td>\n",
       "      <td>994</td>\n",
       "      <td>...</td>\n",
       "      <td>5980</td>\n",
       "      <td>1903</td>\n",
       "      <td>8613</td>\n",
       "      <td>1537</td>\n",
       "      <td>4718</td>\n",
       "      <td>1621</td>\n",
       "      <td>2060</td>\n",
       "      <td>8163</td>\n",
       "      <td>1851</td>\n",
       "      <td>9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_percentage</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height_percentage</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land_surface_condition</th>\n",
       "      <td>t</td>\n",
       "      <td>o</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foundation_type</th>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>w</td>\n",
       "      <td>r</td>\n",
       "      <td>i</td>\n",
       "      <td>...</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roof_type</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>q</td>\n",
       "      <td>x</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_floor_type</th>\n",
       "      <td>f</td>\n",
       "      <td>x</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>x</td>\n",
       "      <td>v</td>\n",
       "      <td>f</td>\n",
       "      <td>v</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>v</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_floor_type</th>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>x</td>\n",
       "      <td>q</td>\n",
       "      <td>j</td>\n",
       "      <td>...</td>\n",
       "      <td>j</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>x</td>\n",
       "      <td>q</td>\n",
       "      <td>j</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>s</td>\n",
       "      <td>q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position</th>\n",
       "      <td>t</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>j</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>j</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plan_configuration</th>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>...</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>q</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_mud_mortar_brick</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_cement_mortar_brick</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_timber</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_bamboo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_rc_non_engineered</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_rc_engineered</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_superstructure_other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal_ownership_status</th>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>...</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_families</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_secondary_use_other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows Ã— 260601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0      1      2       3       4       \\\n",
       "building_id                             802906  28830  94947  590882  201944   \n",
       "geo_level_1_id                               6      8     21      22      11   \n",
       "geo_level_2_id                             487    900    363     418     131   \n",
       "geo_level_3_id                           12198   2812   8973   10694    1488   \n",
       "count_floors_pre_eq                          2      2      2       2       3   \n",
       "age                                         30     10     10      10      30   \n",
       "area_percentage                              6      8      5       6       8   \n",
       "height_percentage                            5      7      5       5       9   \n",
       "land_surface_condition                       t      o      t       t       t   \n",
       "foundation_type                              r      r      r       r       r   \n",
       "roof_type                                    n      n      n       n       n   \n",
       "ground_floor_type                            f      x      f       f       f   \n",
       "other_floor_type                             q      q      x       x       x   \n",
       "position                                     t      s      t       s       s   \n",
       "plan_configuration                           d      d      d       d       d   \n",
       "has_superstructure_adobe_mud                 1      0      0       0       1   \n",
       "has_superstructure_mud_mortar_stone          1      1      1       1       0   \n",
       "has_superstructure_stone_flag                0      0      0       0       0   \n",
       "has_superstructure_cement_mortar_stone       0      0      0       0       0   \n",
       "has_superstructure_mud_mortar_brick          0      0      0       0       0   \n",
       "has_superstructure_cement_mortar_brick       0      0      0       0       0   \n",
       "has_superstructure_timber                    0      0      0       1       0   \n",
       "has_superstructure_bamboo                    0      0      0       1       0   \n",
       "has_superstructure_rc_non_engineered         0      0      0       0       0   \n",
       "has_superstructure_rc_engineered             0      0      0       0       0   \n",
       "has_superstructure_other                     0      0      0       0       0   \n",
       "legal_ownership_status                       v      v      v       v       v   \n",
       "count_families                               1      1      1       1       1   \n",
       "has_secondary_use                            0      0      0       0       0   \n",
       "has_secondary_use_agriculture                0      0      0       0       0   \n",
       "has_secondary_use_hotel                      0      0      0       0       0   \n",
       "has_secondary_use_rental                     0      0      0       0       0   \n",
       "has_secondary_use_institution                0      0      0       0       0   \n",
       "has_secondary_use_school                     0      0      0       0       0   \n",
       "has_secondary_use_industry                   0      0      0       0       0   \n",
       "has_secondary_use_health_post                0      0      0       0       0   \n",
       "has_secondary_use_gov_office                 0      0      0       0       0   \n",
       "has_secondary_use_use_police                 0      0      0       0       0   \n",
       "has_secondary_use_other                      0      0      0       0       0   \n",
       "\n",
       "                                        5       6       7       8       \\\n",
       "building_id                             333020  728451  475515  441126   \n",
       "geo_level_1_id                               8       9      20       0   \n",
       "geo_level_2_id                             558     475     323     757   \n",
       "geo_level_3_id                            6089   12066   12236    7219   \n",
       "count_floors_pre_eq                          2       2       2       2   \n",
       "age                                         10      25       0      15   \n",
       "area_percentage                              9       3       8       8   \n",
       "height_percentage                            5       4       6       6   \n",
       "land_surface_condition                       t       n       t       t   \n",
       "foundation_type                              r       r       w       r   \n",
       "roof_type                                    n       n       q       q   \n",
       "ground_floor_type                            f       x       v       f   \n",
       "other_floor_type                             q       q       x       q   \n",
       "position                                     s       s       s       s   \n",
       "plan_configuration                           d       d       u       d   \n",
       "has_superstructure_adobe_mud                 0       0       0       0   \n",
       "has_superstructure_mud_mortar_stone          1       1       0       1   \n",
       "has_superstructure_stone_flag                0       0       0       0   \n",
       "has_superstructure_cement_mortar_stone       0       0       0       0   \n",
       "has_superstructure_mud_mortar_brick          0       0       0       0   \n",
       "has_superstructure_cement_mortar_brick       0       0       1       0   \n",
       "has_superstructure_timber                    0       0       1       1   \n",
       "has_superstructure_bamboo                    0       0       0       0   \n",
       "has_superstructure_rc_non_engineered         0       0       0       0   \n",
       "has_superstructure_rc_engineered             0       0       0       0   \n",
       "has_superstructure_other                     0       0       0       0   \n",
       "legal_ownership_status                       v       v       v       v   \n",
       "count_families                               1       1       1       1   \n",
       "has_secondary_use                            1       0       0       0   \n",
       "has_secondary_use_agriculture                1       0       0       0   \n",
       "has_secondary_use_hotel                      0       0       0       0   \n",
       "has_secondary_use_rental                     0       0       0       0   \n",
       "has_secondary_use_institution                0       0       0       0   \n",
       "has_secondary_use_school                     0       0       0       0   \n",
       "has_secondary_use_industry                   0       0       0       0   \n",
       "has_secondary_use_health_post                0       0       0       0   \n",
       "has_secondary_use_gov_office                 0       0       0       0   \n",
       "has_secondary_use_use_police                 0       0       0       0   \n",
       "has_secondary_use_other                      0       0       0       0   \n",
       "\n",
       "                                        9       ...  260591  260592  260593  \\\n",
       "building_id                             989500  ...  560805  207683  226421   \n",
       "geo_level_1_id                              26  ...      20      10       8   \n",
       "geo_level_2_id                             886  ...     368    1382     767   \n",
       "geo_level_3_id                             994  ...    5980    1903    8613   \n",
       "count_floors_pre_eq                          1  ...       1       2       2   \n",
       "age                                          0  ...      25      25       5   \n",
       "area_percentage                             13  ...       5       5      13   \n",
       "height_percentage                            4  ...       3       5       5   \n",
       "land_surface_condition                       t  ...       n       t       t   \n",
       "foundation_type                              i  ...       r       r       r   \n",
       "roof_type                                    n  ...       n       n       n   \n",
       "ground_floor_type                            v  ...       f       f       f   \n",
       "other_floor_type                             j  ...       j       q       q   \n",
       "position                                     s  ...       s       s       s   \n",
       "plan_configuration                           d  ...       d       d       d   \n",
       "has_superstructure_adobe_mud                 0  ...       0       0       0   \n",
       "has_superstructure_mud_mortar_stone          0  ...       1       1       1   \n",
       "has_superstructure_stone_flag                0  ...       0       0       0   \n",
       "has_superstructure_cement_mortar_stone       0  ...       0       0       0   \n",
       "has_superstructure_mud_mortar_brick          0  ...       0       0       0   \n",
       "has_superstructure_cement_mortar_brick       1  ...       0       0       0   \n",
       "has_superstructure_timber                    0  ...       0       1       0   \n",
       "has_superstructure_bamboo                    0  ...       0       0       0   \n",
       "has_superstructure_rc_non_engineered         0  ...       0       0       0   \n",
       "has_superstructure_rc_engineered             0  ...       0       0       0   \n",
       "has_superstructure_other                     0  ...       0       0       0   \n",
       "legal_ownership_status                       v  ...       v       v       v   \n",
       "count_families                               1  ...       1       1       1   \n",
       "has_secondary_use                            0  ...       1       0       1   \n",
       "has_secondary_use_agriculture                0  ...       1       0       1   \n",
       "has_secondary_use_hotel                      0  ...       0       0       0   \n",
       "has_secondary_use_rental                     0  ...       0       0       0   \n",
       "has_secondary_use_institution                0  ...       0       0       0   \n",
       "has_secondary_use_school                     0  ...       0       0       0   \n",
       "has_secondary_use_industry                   0  ...       0       0       0   \n",
       "has_secondary_use_health_post                0  ...       0       0       0   \n",
       "has_secondary_use_gov_office                 0  ...       0       0       0   \n",
       "has_secondary_use_use_police                 0  ...       0       0       0   \n",
       "has_secondary_use_other                      0  ...       0       0       0   \n",
       "\n",
       "                                        260594  260595  260596  260597  \\\n",
       "building_id                             159555  827012  688636  669485   \n",
       "geo_level_1_id                              27       8      25      17   \n",
       "geo_level_2_id                             181     268    1335     715   \n",
       "geo_level_3_id                            1537    4718    1621    2060   \n",
       "count_floors_pre_eq                          6       2       1       2   \n",
       "age                                          0      20      55       0   \n",
       "area_percentage                             13       8       6       6   \n",
       "height_percentage                           12       5       3       5   \n",
       "land_surface_condition                       t       t       n       t   \n",
       "foundation_type                              r       r       r       r   \n",
       "roof_type                                    n       n       n       n   \n",
       "ground_floor_type                            f       f       f       f   \n",
       "other_floor_type                             x       q       j       q   \n",
       "position                                     j       s       s       s   \n",
       "plan_configuration                           d       d       q       d   \n",
       "has_superstructure_adobe_mud                 0       0       0       0   \n",
       "has_superstructure_mud_mortar_stone          0       1       1       1   \n",
       "has_superstructure_stone_flag                0       0       0       0   \n",
       "has_superstructure_cement_mortar_stone       0       0       0       0   \n",
       "has_superstructure_mud_mortar_brick          1       0       0       0   \n",
       "has_superstructure_cement_mortar_brick       0       0       0       0   \n",
       "has_superstructure_timber                    0       0       0       0   \n",
       "has_superstructure_bamboo                    0       0       0       0   \n",
       "has_superstructure_rc_non_engineered         0       0       0       0   \n",
       "has_superstructure_rc_engineered             0       0       0       0   \n",
       "has_superstructure_other                     0       0       0       0   \n",
       "legal_ownership_status                       v       v       v       v   \n",
       "count_families                               1       1       1       1   \n",
       "has_secondary_use                            0       0       0       0   \n",
       "has_secondary_use_agriculture                0       0       0       0   \n",
       "has_secondary_use_hotel                      0       0       0       0   \n",
       "has_secondary_use_rental                     0       0       0       0   \n",
       "has_secondary_use_institution                0       0       0       0   \n",
       "has_secondary_use_school                     0       0       0       0   \n",
       "has_secondary_use_industry                   0       0       0       0   \n",
       "has_secondary_use_health_post                0       0       0       0   \n",
       "has_secondary_use_gov_office                 0       0       0       0   \n",
       "has_secondary_use_use_police                 0       0       0       0   \n",
       "has_secondary_use_other                      0       0       0       0   \n",
       "\n",
       "                                        260598  260599  260600  \n",
       "building_id                             602512  151409  747594  \n",
       "geo_level_1_id                              17      26      21  \n",
       "geo_level_2_id                              51      39       9  \n",
       "geo_level_3_id                            8163    1851    9101  \n",
       "count_floors_pre_eq                          3       2       3  \n",
       "age                                         55      10      10  \n",
       "area_percentage                              6      14       7  \n",
       "height_percentage                            7       6       6  \n",
       "land_surface_condition                       t       t       n  \n",
       "foundation_type                              r       r       r  \n",
       "roof_type                                    q       x       n  \n",
       "ground_floor_type                            f       v       f  \n",
       "other_floor_type                             q       s       q  \n",
       "position                                     s       j       j  \n",
       "plan_configuration                           d       d       d  \n",
       "has_superstructure_adobe_mud                 0       0       0  \n",
       "has_superstructure_mud_mortar_stone          1       0       1  \n",
       "has_superstructure_stone_flag                0       0       0  \n",
       "has_superstructure_cement_mortar_stone       0       0       0  \n",
       "has_superstructure_mud_mortar_brick          0       0       0  \n",
       "has_superstructure_cement_mortar_brick       0       1       0  \n",
       "has_superstructure_timber                    0       0       0  \n",
       "has_superstructure_bamboo                    0       0       0  \n",
       "has_superstructure_rc_non_engineered         0       0       0  \n",
       "has_superstructure_rc_engineered             0       0       0  \n",
       "has_superstructure_other                     0       0       0  \n",
       "legal_ownership_status                       v       v       v  \n",
       "count_families                               1       1       3  \n",
       "has_secondary_use                            0       0       0  \n",
       "has_secondary_use_agriculture                0       0       0  \n",
       "has_secondary_use_hotel                      0       0       0  \n",
       "has_secondary_use_rental                     0       0       0  \n",
       "has_secondary_use_institution                0       0       0  \n",
       "has_secondary_use_school                     0       0       0  \n",
       "has_secondary_use_industry                   0       0       0  \n",
       "has_secondary_use_health_post                0       0       0  \n",
       "has_secondary_use_gov_office                 0       0       0  \n",
       "has_secondary_use_use_police                 0       0       0  \n",
       "has_secondary_use_other                      0       0       0  \n",
       "\n",
       "[39 rows x 260601 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = pd.read_csv('../../csv/train_values.csv')\n",
    "values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f7cb6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260601 entries, 0 to 260600\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                  Non-Null Count   Dtype   \n",
      "---  ------                                  --------------   -----   \n",
      " 0   building_id                             260601 non-null  int64   \n",
      " 1   geo_level_1_id                          260601 non-null  int64   \n",
      " 2   geo_level_2_id                          260601 non-null  int64   \n",
      " 3   geo_level_3_id                          260601 non-null  int64   \n",
      " 4   count_floors_pre_eq                     260601 non-null  int64   \n",
      " 5   age                                     260601 non-null  int64   \n",
      " 6   area_percentage                         260601 non-null  int64   \n",
      " 7   height_percentage                       260601 non-null  int64   \n",
      " 8   land_surface_condition                  260601 non-null  category\n",
      " 9   foundation_type                         260601 non-null  category\n",
      " 10  roof_type                               260601 non-null  category\n",
      " 11  ground_floor_type                       260601 non-null  category\n",
      " 12  other_floor_type                        260601 non-null  category\n",
      " 13  position                                260601 non-null  category\n",
      " 14  plan_configuration                      260601 non-null  category\n",
      " 15  has_superstructure_adobe_mud            260601 non-null  int64   \n",
      " 16  has_superstructure_mud_mortar_stone     260601 non-null  int64   \n",
      " 17  has_superstructure_stone_flag           260601 non-null  int64   \n",
      " 18  has_superstructure_cement_mortar_stone  260601 non-null  int64   \n",
      " 19  has_superstructure_mud_mortar_brick     260601 non-null  int64   \n",
      " 20  has_superstructure_cement_mortar_brick  260601 non-null  int64   \n",
      " 21  has_superstructure_timber               260601 non-null  int64   \n",
      " 22  has_superstructure_bamboo               260601 non-null  int64   \n",
      " 23  has_superstructure_rc_non_engineered    260601 non-null  int64   \n",
      " 24  has_superstructure_rc_engineered        260601 non-null  int64   \n",
      " 25  has_superstructure_other                260601 non-null  int64   \n",
      " 26  legal_ownership_status                  260601 non-null  category\n",
      " 27  count_families                          260601 non-null  int64   \n",
      " 28  has_secondary_use                       260601 non-null  int64   \n",
      " 29  has_secondary_use_agriculture           260601 non-null  int64   \n",
      " 30  has_secondary_use_hotel                 260601 non-null  int64   \n",
      " 31  has_secondary_use_rental                260601 non-null  int64   \n",
      " 32  has_secondary_use_institution           260601 non-null  int64   \n",
      " 33  has_secondary_use_school                260601 non-null  int64   \n",
      " 34  has_secondary_use_industry              260601 non-null  int64   \n",
      " 35  has_secondary_use_health_post           260601 non-null  int64   \n",
      " 36  has_secondary_use_gov_office            260601 non-null  int64   \n",
      " 37  has_secondary_use_use_police            260601 non-null  int64   \n",
      " 38  has_secondary_use_other                 260601 non-null  int64   \n",
      "dtypes: category(8), int64(31)\n",
      "memory usage: 63.6 MB\n"
     ]
    }
   ],
   "source": [
    "to_be_categorized = [\"land_surface_condition\", \"foundation_type\", \"roof_type\",\\\n",
    "                     \"position\", \"ground_floor_type\", \"other_floor_type\",\\\n",
    "                     \"plan_configuration\", \"legal_ownership_status\"]\n",
    "for row in to_be_categorized:\n",
    "    values[row] = values[row].astype(\"category\")\n",
    "values.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75aea48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = dict(values.dtypes)\n",
    "for row in values.columns:\n",
    "    if datatypes[row] != \"int64\" and datatypes[row] != \"int32\" and \\\n",
    "       datatypes[row] != \"int16\" and datatypes[row] != \"int8\":\n",
    "        continue\n",
    "    if values[row].nlargest(1).item() > 32767 and values[row].nlargest(1).item() < 2**31:\n",
    "        values[row] = values[row].astype(np.int32)\n",
    "    elif values[row].nlargest(1).item() > 127:\n",
    "        values[row] = values[row].astype(np.int16)\n",
    "    else:\n",
    "        values[row] = values[row].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c389846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260601 entries, 0 to 260600\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype\n",
      "---  ------        --------------   -----\n",
      " 0   building_id   260601 non-null  int32\n",
      " 1   damage_grade  260601 non-null  int8 \n",
      "dtypes: int32(1), int8(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "labels[\"building_id\"] = labels[\"building_id\"].astype(np.int32)\n",
    "labels[\"damage_grade\"] = labels[\"damage_grade\"].astype(np.int8)\n",
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e74d71fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>roof_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260596</th>\n",
       "      <td>25</td>\n",
       "      <td>1335</td>\n",
       "      <td>1621</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260597</th>\n",
       "      <td>17</td>\n",
       "      <td>715</td>\n",
       "      <td>2060</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260598</th>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>8163</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260599</th>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>1851</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260600</th>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>9101</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260601 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geo_level_1_id  geo_level_2_id  geo_level_3_id  count_floors_pre_eq  \\\n",
       "0                   6             487           12198                    2   \n",
       "1                   8             900            2812                    2   \n",
       "2                  21             363            8973                    2   \n",
       "3                  22             418           10694                    2   \n",
       "4                  11             131            1488                    3   \n",
       "...               ...             ...             ...                  ...   \n",
       "260596             25            1335            1621                    1   \n",
       "260597             17             715            2060                    2   \n",
       "260598             17              51            8163                    3   \n",
       "260599             26              39            1851                    2   \n",
       "260600             21               9            9101                    3   \n",
       "\n",
       "        age  area_percentage  height_percentage land_surface_condition  \\\n",
       "0        30                6                  5                      t   \n",
       "1        10                8                  7                      o   \n",
       "2        10                5                  5                      t   \n",
       "3        10                6                  5                      t   \n",
       "4        30                8                  9                      t   \n",
       "...     ...              ...                ...                    ...   \n",
       "260596   55                6                  3                      n   \n",
       "260597    0                6                  5                      t   \n",
       "260598   55                6                  7                      t   \n",
       "260599   10               14                  6                      t   \n",
       "260600   10                7                  6                      n   \n",
       "\n",
       "       foundation_type roof_type  ... has_secondary_use_hotel  \\\n",
       "0                    r         n  ...                       0   \n",
       "1                    r         n  ...                       0   \n",
       "2                    r         n  ...                       0   \n",
       "3                    r         n  ...                       0   \n",
       "4                    r         n  ...                       0   \n",
       "...                ...       ...  ...                     ...   \n",
       "260596               r         n  ...                       0   \n",
       "260597               r         n  ...                       0   \n",
       "260598               r         q  ...                       0   \n",
       "260599               r         x  ...                       0   \n",
       "260600               r         n  ...                       0   \n",
       "\n",
       "       has_secondary_use_rental has_secondary_use_institution  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "...                         ...                           ...   \n",
       "260596                        0                             0   \n",
       "260597                        0                             0   \n",
       "260598                        0                             0   \n",
       "260599                        0                             0   \n",
       "260600                        0                             0   \n",
       "\n",
       "       has_secondary_use_school  has_secondary_use_industry  \\\n",
       "0                             0                           0   \n",
       "1                             0                           0   \n",
       "2                             0                           0   \n",
       "3                             0                           0   \n",
       "4                             0                           0   \n",
       "...                         ...                         ...   \n",
       "260596                        0                           0   \n",
       "260597                        0                           0   \n",
       "260598                        0                           0   \n",
       "260599                        0                           0   \n",
       "260600                        0                           0   \n",
       "\n",
       "        has_secondary_use_health_post  has_secondary_use_gov_office  \\\n",
       "0                                   0                             0   \n",
       "1                                   0                             0   \n",
       "2                                   0                             0   \n",
       "3                                   0                             0   \n",
       "4                                   0                             0   \n",
       "...                               ...                           ...   \n",
       "260596                              0                             0   \n",
       "260597                              0                             0   \n",
       "260598                              0                             0   \n",
       "260599                              0                             0   \n",
       "260600                              0                             0   \n",
       "\n",
       "        has_secondary_use_use_police  has_secondary_use_other  damage_grade  \n",
       "0                                  0                        0             3  \n",
       "1                                  0                        0             2  \n",
       "2                                  0                        0             3  \n",
       "3                                  0                        0             2  \n",
       "4                                  0                        0             3  \n",
       "...                              ...                      ...           ...  \n",
       "260596                             0                        0             2  \n",
       "260597                             0                        0             3  \n",
       "260598                             0                        0             3  \n",
       "260599                             0                        0             2  \n",
       "260600                             0                        0             3  \n",
       "\n",
       "[260601 rows x 39 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_values = values\\\n",
    "                .merge(labels, on=\"building_id\")\n",
    "important_values.drop(columns=[\"building_id\"], inplace = True)\n",
    "important_values[\"geo_level_1_id\"] = important_values[\"geo_level_1_id\"].astype(\"category\")\n",
    "important_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83228afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(important_values.drop(columns = 'damage_grade'),\n",
    "                                                    important_values['damage_grade'], test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ed64b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHotEncoding\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return(res) \n",
    "\n",
    "features_to_encode = [\"geo_level_1_id\", \"land_surface_condition\", \"foundation_type\", \"roof_type\",\\\n",
    "                     \"position\", \"ground_floor_type\", \"other_floor_type\",\\\n",
    "                     \"plan_configuration\", \"legal_ownership_status\"]\n",
    "for feature in features_to_encode:\n",
    "    X_train = encode_and_bind(X_train, feature)\n",
    "    X_test = encode_and_bind(X_test, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908ad43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_m</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_a</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103291</th>\n",
       "      <td>1274</td>\n",
       "      <td>4190</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233923</th>\n",
       "      <td>1207</td>\n",
       "      <td>12014</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166653</th>\n",
       "      <td>944</td>\n",
       "      <td>8232</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150634</th>\n",
       "      <td>488</td>\n",
       "      <td>12448</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82720</th>\n",
       "      <td>302</td>\n",
       "      <td>5339</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192476</th>\n",
       "      <td>217</td>\n",
       "      <td>10644</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>600</td>\n",
       "      <td>4813</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28030</th>\n",
       "      <td>463</td>\n",
       "      <td>4692</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>600</td>\n",
       "      <td>157</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249342</th>\n",
       "      <td>229</td>\n",
       "      <td>4027</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208480 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        geo_level_2_id  geo_level_3_id  count_floors_pre_eq  age  \\\n",
       "103291            1274            4190                    2   25   \n",
       "233923            1207           12014                    1   10   \n",
       "166653             944            8232                    3   40   \n",
       "150634             488           12448                    2    0   \n",
       "82720              302            5339                    1   10   \n",
       "...                ...             ...                  ...  ...   \n",
       "192476             217           10644                    1   25   \n",
       "17730              600            4813                    2   20   \n",
       "28030              463            4692                    2   10   \n",
       "15725              600             157                    2   50   \n",
       "249342             229            4027                    2   15   \n",
       "\n",
       "        area_percentage  height_percentage  has_superstructure_adobe_mud  \\\n",
       "103291                8                  5                             0   \n",
       "233923                9                  3                             0   \n",
       "166653                7                  6                             0   \n",
       "150634                7                  5                             0   \n",
       "82720                 5                  3                             0   \n",
       "...                 ...                ...                           ...   \n",
       "192476                4                  6                             0   \n",
       "17730                13                  8                             0   \n",
       "28030                 9                  4                             1   \n",
       "15725                 5                  8                             0   \n",
       "249342                8                  5                             0   \n",
       "\n",
       "        has_superstructure_mud_mortar_stone  has_superstructure_stone_flag  \\\n",
       "103291                                    1                              0   \n",
       "233923                                    1                              0   \n",
       "166653                                    1                              0   \n",
       "150634                                    1                              0   \n",
       "82720                                     1                              0   \n",
       "...                                     ...                            ...   \n",
       "192476                                    1                              0   \n",
       "17730                                     1                              0   \n",
       "28030                                     1                              0   \n",
       "15725                                     1                              0   \n",
       "249342                                    1                              1   \n",
       "\n",
       "        has_superstructure_cement_mortar_stone  ...  plan_configuration_m  \\\n",
       "103291                                       0  ...                     0   \n",
       "233923                                       0  ...                     0   \n",
       "166653                                       0  ...                     0   \n",
       "150634                                       0  ...                     0   \n",
       "82720                                        0  ...                     0   \n",
       "...                                        ...  ...                   ...   \n",
       "192476                                       0  ...                     0   \n",
       "17730                                        0  ...                     0   \n",
       "28030                                        0  ...                     0   \n",
       "15725                                        0  ...                     0   \n",
       "249342                                       0  ...                     0   \n",
       "\n",
       "        plan_configuration_n  plan_configuration_o  plan_configuration_q  \\\n",
       "103291                     0                     0                     0   \n",
       "233923                     0                     0                     0   \n",
       "166653                     0                     0                     0   \n",
       "150634                     0                     0                     0   \n",
       "82720                      0                     0                     0   \n",
       "...                      ...                   ...                   ...   \n",
       "192476                     0                     0                     0   \n",
       "17730                      0                     0                     0   \n",
       "28030                      0                     0                     0   \n",
       "15725                      0                     0                     0   \n",
       "249342                     0                     0                     0   \n",
       "\n",
       "        plan_configuration_s  plan_configuration_u  legal_ownership_status_a  \\\n",
       "103291                     0                     0                         0   \n",
       "233923                     0                     0                         0   \n",
       "166653                     0                     0                         0   \n",
       "150634                     1                     0                         0   \n",
       "82720                      0                     0                         0   \n",
       "...                      ...                   ...                       ...   \n",
       "192476                     0                     0                         0   \n",
       "17730                      0                     0                         0   \n",
       "28030                      0                     0                         0   \n",
       "15725                      0                     0                         0   \n",
       "249342                     0                     0                         0   \n",
       "\n",
       "        legal_ownership_status_r  legal_ownership_status_v  \\\n",
       "103291                         0                         1   \n",
       "233923                         0                         1   \n",
       "166653                         0                         1   \n",
       "150634                         0                         1   \n",
       "82720                          0                         1   \n",
       "...                          ...                       ...   \n",
       "192476                         0                         1   \n",
       "17730                          0                         1   \n",
       "28030                          0                         1   \n",
       "15725                          0                         1   \n",
       "249342                         0                         1   \n",
       "\n",
       "        legal_ownership_status_w  \n",
       "103291                         0  \n",
       "233923                         0  \n",
       "166653                         0  \n",
       "150634                         0  \n",
       "82720                          0  \n",
       "...                          ...  \n",
       "192476                         0  \n",
       "17730                          0  \n",
       "28030                          0  \n",
       "15725                          0  \n",
       "249342                         0  \n",
       "\n",
       "[208480 rows x 98 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7d3567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=5, tm_min=48, tm_sec=50, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=5, tm_min=50, tm_sec=27, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=5, tm_min=54, tm_sec=28, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=0, tm_sec=40, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=9, tm_sec=13, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=11, tm_sec=17, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=15, tm_sec=19, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=21, tm_sec=29, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=29, tm_sec=58, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=32, tm_sec=3, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=36, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=42, tm_sec=18, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=51, tm_sec=2, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=53, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=57, tm_sec=10, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=3, tm_sec=20, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=11, tm_sec=53, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=13, tm_sec=58, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=18, tm_sec=1, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=24, tm_sec=12, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=32, tm_sec=44, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=34, tm_sec=49, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=38, tm_sec=51, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=45, tm_sec=2, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=53, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=55, tm_sec=39, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=7, tm_min=59, tm_sec=42, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=5, tm_sec=53, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=14, tm_sec=28, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=16, tm_sec=33, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=20, tm_sec=36, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=26, tm_sec=45, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=35, tm_sec=15, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=37, tm_sec=20, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=41, tm_sec=23, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=47, tm_sec=33, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=56, tm_sec=0, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=8, tm_min=58, tm_sec=0, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=1, tm_sec=53, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=7, tm_sec=49, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=15, tm_sec=54, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=17, tm_sec=53, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=21, tm_sec=44, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=27, tm_sec=38, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=35, tm_sec=48, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=37, tm_sec=32, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=41, tm_sec=25, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=47, tm_sec=24, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=55, tm_sec=33, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=9, tm_min=57, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=1, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=7, tm_sec=29, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=15, tm_sec=36, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=17, tm_sec=35, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=21, tm_sec=26, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=27, tm_sec=18, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=35, tm_sec=26, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=37, tm_sec=22, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=41, tm_sec=16, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=47, tm_sec=8, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=55, tm_sec=9, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=10, tm_min=56, tm_sec=44, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=0, tm_sec=38, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=6, tm_sec=33, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=14, tm_sec=32, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=16, tm_sec=28, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=20, tm_sec=16, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=26, tm_sec=3, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=33, tm_sec=51, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=35, tm_sec=47, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=39, tm_sec=35, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=45, tm_sec=23, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=53, tm_sec=20, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=55, tm_sec=14, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=11, tm_min=58, tm_sec=53, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=4, tm_sec=26, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=12, tm_sec=6, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=13, tm_sec=57, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=17, tm_sec=37, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=23, tm_sec=13, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=30, tm_sec=57, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=32, tm_sec=46, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=36, tm_sec=25, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=41, tm_sec=57, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=49, tm_sec=37, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=51, tm_sec=27, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=12, tm_min=55, tm_sec=5, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=0, tm_sec=38, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=8, tm_sec=17, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=10, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=13, tm_sec=48, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=19, tm_sec=23, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=26, tm_sec=58, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=28, tm_sec=52, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=32, tm_sec=26, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=37, tm_sec=56, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=45, tm_sec=31, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=47, tm_sec=22, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=50, tm_sec=59, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=56, tm_sec=35, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=4, tm_sec=16, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=6, tm_sec=4, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=9, tm_sec=43, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=15, tm_sec=18, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=22, tm_sec=52, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=24, tm_sec=44, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=28, tm_sec=22, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=33, tm_sec=56, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=41, tm_sec=33, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=43, tm_sec=22, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=46, tm_sec=52, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=52, tm_sec=16, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=59, tm_sec=40, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=1, tm_sec=28, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=5, tm_sec=2, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=10, tm_sec=23, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=17, tm_sec=49, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=19, tm_sec=37, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=23, tm_sec=10, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=28, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=35, tm_sec=56, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=37, tm_sec=26, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=41, tm_sec=2, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=46, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=53, tm_sec=59, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=55, tm_sec=44, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=59, tm_sec=20, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=4, tm_sec=58, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=12, tm_sec=29, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=14, tm_sec=19, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=17, tm_sec=54, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=23, tm_sec=22, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=30, tm_sec=49, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=32, tm_sec=40, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=36, tm_sec=15, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=41, tm_sec=41, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=49, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=50, tm_sec=59, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=16, tm_min=54, tm_sec=36, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=0, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=7, tm_sec=34, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=9, tm_sec=25, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=13, tm_sec=0, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=18, tm_sec=27, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=17, tm_min=25, tm_sec=54, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.885</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.724199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.724123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.724123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.550</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.723969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.885</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.723777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subsample  gamma  learning_rate  max_depth     score\n",
       "90       0.885    1.0          0.450        6.0  0.743577\n",
       "114      0.950    0.5          0.450        6.0  0.743424\n",
       "18       0.750    1.0          0.450        6.0  0.743232\n",
       "74       0.885    0.5          0.425        6.0  0.743117\n",
       "130      0.950    1.0          0.550        6.0  0.743098\n",
       "..         ...    ...            ...        ...       ...\n",
       "97       0.885    1.5          0.425        3.0  0.724199\n",
       "49       0.825    1.0          0.425        3.0  0.724123\n",
       "29       0.750    1.5          0.450        3.0  0.724123\n",
       "12       0.750    0.5          0.550       12.0  0.723969\n",
       "73       0.885    0.5          0.425        3.0  0.723777\n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# min_child_weight = [0, 1, 2]\n",
    "# max_delta_step = [0, 5, 10]\n",
    "\n",
    "def my_grid_search():\n",
    "    print(time.gmtime())\n",
    "    i = 1\n",
    "    df = pd.DataFrame({'subsample': [],\n",
    "                       'gamma': [],\n",
    "                       'learning_rate': [],\n",
    "                       'max_depth': [],\n",
    "                       'score': []})\n",
    "    for subsample in [0.75, 0.885, 0.95]:\n",
    "        for gamma in [0.75, 1, 1.25]:\n",
    "            for learning_rate in [0.4375, 0.45, 0.4625]:\n",
    "                 for max_depth in [5, 6, 7]:\n",
    "                    model = XGBClassifier(n_estimators = 350,\n",
    "                                          booster = 'gbtree',\n",
    "                                          subsample = subsample,\n",
    "                                          gamma = gamma,\n",
    "                                          max_depth = max_depth,\n",
    "                                          learning_rate = learning_rate,\n",
    "                                          label_encoder = False,\n",
    "                                          verbosity = 0)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_preds = model.predict(X_test)\n",
    "                    score = f1_score(y_test, y_preds, average = 'micro')\n",
    "                    df = df.append(pd.Series(\n",
    "                        data={'subsample': subsample,\n",
    "                              'gamma': gamma,\n",
    "                              'learning_rate': learning_rate,\n",
    "                              'max_depth': max_depth,\n",
    "                              'score': score},\n",
    "                    name = i))\n",
    "                    print(i, time.gmtime())\n",
    "                    i += 1\n",
    "\n",
    "    return df.sort_values('score', ascending = False)\n",
    "\n",
    "current_df = my_grid_search()\n",
    "df = pd.read_csv('grid-search/res-feature-engineering.csv')\n",
    "df.append(current_df)\n",
    "df.to_csv('grid-search/res-feature-engineering.csv')\n",
    "\n",
    "current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2aedd18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=21, tm_min=22, tm_sec=10, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=21, tm_min=24, tm_sec=23, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=21, tm_min=27, tm_sec=28, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=21, tm_min=31, tm_sec=41, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinfontela/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 time.struct_time(tm_year=2021, tm_mon=7, tm_mday=13, tm_hour=21, tm_min=37, tm_sec=7, tm_wday=1, tm_yday=194, tm_isdst=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.743577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.742868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.740968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.740719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subsample  gamma  learning_rate  max_depth     score\n",
       "2      0.885    1.0           0.45        6.0  0.743577\n",
       "3      0.885    1.0           0.45        7.0  0.742868\n",
       "4      0.885    1.0           0.45        8.0  0.740968\n",
       "1      0.885    1.0           0.45        5.0  0.740719"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def my_grid_search():\n",
    "    print(time.gmtime())\n",
    "    i = 1\n",
    "    df = pd.DataFrame({'subsample': [],\n",
    "                       'gamma': [],\n",
    "                       'learning_rate': [],\n",
    "                       'max_depth': [],\n",
    "                       'score': []})\n",
    "    for subsample in [0.885]:\n",
    "        for gamma in [1]:\n",
    "            for learning_rate in [0.45]:\n",
    "                for max_depth in [5,6,7,8]:\n",
    "                    model = XGBClassifier(n_estimators = 350,\n",
    "                                          booster = 'gbtree',\n",
    "                                          subsample = subsample,\n",
    "                                          gamma = gamma,\n",
    "                                          max_depth = max_depth,\n",
    "                                          learning_rate = learning_rate,\n",
    "                                          label_encoder = False,\n",
    "                                          verbosity = 0)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_preds = model.predict(X_test)\n",
    "                    score = f1_score(y_test, y_preds, average = 'micro')\n",
    "                    df = df.append(pd.Series(\n",
    "                        data={'subsample': subsample,\n",
    "                              'gamma': gamma,\n",
    "                              'learning_rate': learning_rate,\n",
    "                              'max_depth': max_depth,\n",
    "                              'score': score},\n",
    "                    name = i))\n",
    "                    print(i, time.gmtime())\n",
    "                    i += 1\n",
    "\n",
    "    return df.sort_values('score', ascending = False)\n",
    "\n",
    "df = my_grid_search()\n",
    "# df = pd.read_csv('grid-search/res-feature-engineering.csv')\n",
    "# df.append(current_df)\n",
    "df.to_csv('grid-search/res-feature-engineering.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020e2c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'grid-search/res-no-feature-engineering.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9884b5366f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grid-search/res-no-feature-engineering.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'grid-search/res-no-feature-engineering.csv'"
     ]
    }
   ],
   "source": [
    "pd.read_csv('grid-search/res-no-feature-engineering.csv')\\\n",
    "    .nlargest(20, 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7eacf",
   "metadata": {},
   "source": [
    "# Entreno tres de los mejores modelos con Voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "926e2c39",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sgd_model = SGDClassifier(verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6afd9cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3601.01, NNZs: 98, Bias: 264.138417, T: 208480, Avg. loss: 1174570.836659\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2796.96, NNZs: 98, Bias: 289.620542, T: 416960, Avg. loss: 154604.296074\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2495.56, NNZs: 98, Bias: 300.226883, T: 625440, Avg. loss: 90221.656636\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2255.56, NNZs: 98, Bias: 306.642253, T: 833920, Avg. loss: 63285.430727\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2075.95, NNZs: 98, Bias: 309.972339, T: 1042400, Avg. loss: 49672.479125\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1942.29, NNZs: 98, Bias: 312.685808, T: 1250880, Avg. loss: 40383.105789\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1831.07, NNZs: 98, Bias: 313.968071, T: 1459360, Avg. loss: 34227.012937\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1728.40, NNZs: 98, Bias: 314.705803, T: 1667840, Avg. loss: 29484.283134\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1651.35, NNZs: 98, Bias: 313.923451, T: 1876320, Avg. loss: 26067.402484\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1583.57, NNZs: 98, Bias: 313.650554, T: 2084800, Avg. loss: 23217.803914\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1525.13, NNZs: 98, Bias: 312.962689, T: 2293280, Avg. loss: 20937.940808\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1470.34, NNZs: 98, Bias: 312.147443, T: 2501760, Avg. loss: 19247.901869\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1428.39, NNZs: 98, Bias: 311.694616, T: 2710240, Avg. loss: 17690.738228\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1385.52, NNZs: 98, Bias: 310.827422, T: 2918720, Avg. loss: 16254.983479\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1349.89, NNZs: 98, Bias: 310.181810, T: 3127200, Avg. loss: 15130.912722\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1317.27, NNZs: 98, Bias: 309.402421, T: 3335680, Avg. loss: 14231.530733\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1286.41, NNZs: 98, Bias: 308.682687, T: 3544160, Avg. loss: 13338.148576\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1264.18, NNZs: 98, Bias: 308.099587, T: 3752640, Avg. loss: 12589.149025\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1240.33, NNZs: 98, Bias: 307.599611, T: 3961120, Avg. loss: 11876.969713\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1219.67, NNZs: 98, Bias: 306.872802, T: 4169600, Avg. loss: 11234.613160\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1198.63, NNZs: 98, Bias: 306.452616, T: 4378080, Avg. loss: 10691.776942\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1179.55, NNZs: 98, Bias: 305.775636, T: 4586560, Avg. loss: 10178.656776\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1165.19, NNZs: 98, Bias: 305.304053, T: 4795040, Avg. loss: 9740.958082\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1149.40, NNZs: 98, Bias: 304.888123, T: 5003520, Avg. loss: 9358.197200\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1136.53, NNZs: 98, Bias: 304.410422, T: 5212000, Avg. loss: 8942.083785\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1123.17, NNZs: 98, Bias: 304.188834, T: 5420480, Avg. loss: 8586.134609\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1108.57, NNZs: 98, Bias: 303.879713, T: 5628960, Avg. loss: 8277.939449\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1097.05, NNZs: 98, Bias: 303.597187, T: 5837440, Avg. loss: 7937.764600\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1087.08, NNZs: 98, Bias: 303.021153, T: 6045920, Avg. loss: 7649.139888\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1078.34, NNZs: 98, Bias: 302.490882, T: 6254400, Avg. loss: 7426.267337\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1066.55, NNZs: 98, Bias: 302.090962, T: 6462880, Avg. loss: 7180.143042\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1056.76, NNZs: 98, Bias: 301.863237, T: 6671360, Avg. loss: 6955.465286\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1048.45, NNZs: 98, Bias: 301.533391, T: 6879840, Avg. loss: 6696.428404\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1040.93, NNZs: 98, Bias: 301.161998, T: 7088320, Avg. loss: 6491.090453\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1034.16, NNZs: 98, Bias: 300.932608, T: 7296800, Avg. loss: 6342.678471\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1027.29, NNZs: 98, Bias: 300.700173, T: 7505280, Avg. loss: 6183.630072\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1020.75, NNZs: 98, Bias: 300.415285, T: 7713760, Avg. loss: 5979.353856\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1015.11, NNZs: 98, Bias: 300.093480, T: 7922240, Avg. loss: 5824.467210\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1007.59, NNZs: 98, Bias: 299.880066, T: 8130720, Avg. loss: 5638.157397\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1000.98, NNZs: 98, Bias: 299.698087, T: 8339200, Avg. loss: 5497.760493\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 996.87, NNZs: 98, Bias: 299.452344, T: 8547680, Avg. loss: 5376.697465\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 991.73, NNZs: 98, Bias: 299.213088, T: 8756160, Avg. loss: 5243.071658\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 985.85, NNZs: 98, Bias: 299.020292, T: 8964640, Avg. loss: 5131.903875\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 980.60, NNZs: 98, Bias: 298.839201, T: 9173120, Avg. loss: 5007.507398\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 976.01, NNZs: 98, Bias: 298.666115, T: 9381600, Avg. loss: 4866.640737\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 971.68, NNZs: 98, Bias: 298.533241, T: 9590080, Avg. loss: 4780.697874\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 967.35, NNZs: 98, Bias: 298.286922, T: 9798560, Avg. loss: 4651.030199\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 962.57, NNZs: 98, Bias: 298.187979, T: 10007040, Avg. loss: 4575.436457\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 957.88, NNZs: 98, Bias: 297.954775, T: 10215520, Avg. loss: 4470.324125\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 953.99, NNZs: 98, Bias: 297.865595, T: 10424000, Avg. loss: 4375.198014\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 950.98, NNZs: 98, Bias: 297.782603, T: 10632480, Avg. loss: 4289.143485\n",
      "Total training time: 3.05 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 947.69, NNZs: 98, Bias: 297.651769, T: 10840960, Avg. loss: 4206.443327\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 944.30, NNZs: 98, Bias: 297.483771, T: 11049440, Avg. loss: 4131.882415\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 940.22, NNZs: 98, Bias: 297.384518, T: 11257920, Avg. loss: 4047.489778\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 937.20, NNZs: 98, Bias: 297.316321, T: 11466400, Avg. loss: 3981.143880\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 933.92, NNZs: 98, Bias: 297.313052, T: 11674880, Avg. loss: 3911.715108\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 931.17, NNZs: 98, Bias: 297.257004, T: 11883360, Avg. loss: 3815.191763\n",
      "Total training time: 3.39 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 928.52, NNZs: 98, Bias: 297.249513, T: 12091840, Avg. loss: 3761.537266\n",
      "Total training time: 3.46 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 924.54, NNZs: 98, Bias: 297.139592, T: 12300320, Avg. loss: 3694.262767\n",
      "Total training time: 3.52 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 920.95, NNZs: 98, Bias: 297.064607, T: 12508800, Avg. loss: 3641.828661\n",
      "Total training time: 3.58 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 918.26, NNZs: 98, Bias: 297.066779, T: 12717280, Avg. loss: 3571.864890\n",
      "Total training time: 3.65 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 915.21, NNZs: 98, Bias: 296.979289, T: 12925760, Avg. loss: 3519.798038\n",
      "Total training time: 3.71 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 912.47, NNZs: 98, Bias: 296.940263, T: 13134240, Avg. loss: 3439.769265\n",
      "Total training time: 3.77 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 909.50, NNZs: 98, Bias: 296.865503, T: 13342720, Avg. loss: 3399.172547\n",
      "Total training time: 3.83 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 907.20, NNZs: 98, Bias: 296.829991, T: 13551200, Avg. loss: 3346.579513\n",
      "Total training time: 3.88 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 905.19, NNZs: 98, Bias: 296.795651, T: 13759680, Avg. loss: 3311.812127\n",
      "Total training time: 3.94 seconds.\n",
      "-- Epoch 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 902.53, NNZs: 98, Bias: 296.726796, T: 13968160, Avg. loss: 3236.527354\n",
      "Total training time: 4.00 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 899.42, NNZs: 98, Bias: 296.740299, T: 14176640, Avg. loss: 3174.105366\n",
      "Total training time: 4.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 896.89, NNZs: 98, Bias: 296.692670, T: 14385120, Avg. loss: 3136.399963\n",
      "Total training time: 4.11 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 894.82, NNZs: 98, Bias: 296.788741, T: 14593600, Avg. loss: 3117.903774\n",
      "Total training time: 4.16 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 892.09, NNZs: 98, Bias: 296.802498, T: 14802080, Avg. loss: 3046.444721\n",
      "Total training time: 4.22 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 890.03, NNZs: 98, Bias: 296.780032, T: 15010560, Avg. loss: 3033.429347\n",
      "Total training time: 4.27 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 888.08, NNZs: 98, Bias: 296.765495, T: 15219040, Avg. loss: 2979.139443\n",
      "Total training time: 4.33 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 886.22, NNZs: 98, Bias: 296.734866, T: 15427520, Avg. loss: 2923.948005\n",
      "Total training time: 4.38 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 884.14, NNZs: 98, Bias: 296.734214, T: 15636000, Avg. loss: 2878.414947\n",
      "Total training time: 4.44 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 882.39, NNZs: 98, Bias: 296.796669, T: 15844480, Avg. loss: 2842.525741\n",
      "Total training time: 4.50 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 880.38, NNZs: 98, Bias: 296.825372, T: 16052960, Avg. loss: 2791.426767\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 878.13, NNZs: 98, Bias: 296.809047, T: 16261440, Avg. loss: 2764.061991\n",
      "Total training time: 4.60 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 875.80, NNZs: 98, Bias: 296.811345, T: 16469920, Avg. loss: 2733.073980\n",
      "Total training time: 4.66 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 873.90, NNZs: 98, Bias: 296.830011, T: 16678400, Avg. loss: 2695.404185\n",
      "Total training time: 4.72 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 872.08, NNZs: 98, Bias: 296.868673, T: 16886880, Avg. loss: 2663.717664\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 870.12, NNZs: 98, Bias: 296.830783, T: 17095360, Avg. loss: 2630.423600\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 868.14, NNZs: 98, Bias: 296.817721, T: 17303840, Avg. loss: 2597.356413\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 866.20, NNZs: 98, Bias: 296.899798, T: 17512320, Avg. loss: 2576.066887\n",
      "Total training time: 4.94 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 864.40, NNZs: 98, Bias: 296.881559, T: 17720800, Avg. loss: 2551.976101\n",
      "Total training time: 4.99 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 862.47, NNZs: 98, Bias: 296.902851, T: 17929280, Avg. loss: 2502.041250\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 860.48, NNZs: 98, Bias: 296.910016, T: 18137760, Avg. loss: 2461.436286\n",
      "Total training time: 5.11 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 858.44, NNZs: 98, Bias: 296.856376, T: 18346240, Avg. loss: 2439.162351\n",
      "Total training time: 5.16 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 856.91, NNZs: 98, Bias: 296.804304, T: 18554720, Avg. loss: 2423.294253\n",
      "Total training time: 5.22 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 855.21, NNZs: 98, Bias: 296.798509, T: 18763200, Avg. loss: 2408.578071\n",
      "Total training time: 5.27 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 853.30, NNZs: 98, Bias: 296.868648, T: 18971680, Avg. loss: 2367.962153\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 851.70, NNZs: 98, Bias: 296.873359, T: 19180160, Avg. loss: 2353.927787\n",
      "Total training time: 5.38 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 849.93, NNZs: 98, Bias: 296.928292, T: 19388640, Avg. loss: 2318.517259\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 848.31, NNZs: 98, Bias: 296.981572, T: 19597120, Avg. loss: 2283.518389\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 846.83, NNZs: 98, Bias: 297.012437, T: 19805600, Avg. loss: 2261.836726\n",
      "Total training time: 5.55 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 845.17, NNZs: 98, Bias: 297.022589, T: 20014080, Avg. loss: 2221.563146\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 843.66, NNZs: 98, Bias: 297.063769, T: 20222560, Avg. loss: 2210.644373\n",
      "Total training time: 5.66 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 841.99, NNZs: 98, Bias: 297.085116, T: 20431040, Avg. loss: 2191.504805\n",
      "Total training time: 5.71 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 840.35, NNZs: 98, Bias: 297.074954, T: 20639520, Avg. loss: 2163.705364\n",
      "Total training time: 5.77 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 838.71, NNZs: 98, Bias: 297.093443, T: 20848000, Avg. loss: 2149.929173\n",
      "Total training time: 5.83 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 837.15, NNZs: 98, Bias: 297.124898, T: 21056480, Avg. loss: 2121.305802\n",
      "Total training time: 5.88 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 835.74, NNZs: 98, Bias: 297.151737, T: 21264960, Avg. loss: 2110.452076\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 834.30, NNZs: 98, Bias: 297.161938, T: 21473440, Avg. loss: 2073.459828\n",
      "Total training time: 5.99 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 832.82, NNZs: 98, Bias: 297.157310, T: 21681920, Avg. loss: 2057.159949\n",
      "Total training time: 6.05 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 831.51, NNZs: 98, Bias: 297.243824, T: 21890400, Avg. loss: 2046.779779\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 830.30, NNZs: 98, Bias: 297.325251, T: 22098880, Avg. loss: 2022.359219\n",
      "Total training time: 6.16 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 828.87, NNZs: 98, Bias: 297.315320, T: 22307360, Avg. loss: 1987.373538\n",
      "Total training time: 6.22 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 827.65, NNZs: 98, Bias: 297.397468, T: 22515840, Avg. loss: 1976.414939\n",
      "Total training time: 6.27 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 826.13, NNZs: 98, Bias: 297.437255, T: 22724320, Avg. loss: 1958.666665\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 824.62, NNZs: 98, Bias: 297.441602, T: 22932800, Avg. loss: 1933.105064\n",
      "Total training time: 6.38 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 823.22, NNZs: 98, Bias: 297.487919, T: 23141280, Avg. loss: 1926.328685\n",
      "Total training time: 6.44 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 822.01, NNZs: 98, Bias: 297.559060, T: 23349760, Avg. loss: 1915.065938\n",
      "Total training time: 6.49 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 820.71, NNZs: 98, Bias: 297.634941, T: 23558240, Avg. loss: 1896.196139\n",
      "Total training time: 6.55 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 819.51, NNZs: 98, Bias: 297.655194, T: 23766720, Avg. loss: 1882.039395\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 818.22, NNZs: 98, Bias: 297.697071, T: 23975200, Avg. loss: 1858.695376\n",
      "Total training time: 6.66 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 816.92, NNZs: 98, Bias: 297.721107, T: 24183680, Avg. loss: 1834.509111\n",
      "Total training time: 6.71 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 815.59, NNZs: 98, Bias: 297.752694, T: 24392160, Avg. loss: 1838.195796\n",
      "Total training time: 6.77 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 814.57, NNZs: 98, Bias: 297.837414, T: 24600640, Avg. loss: 1813.312776\n",
      "Total training time: 6.82 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 813.24, NNZs: 98, Bias: 297.854453, T: 24809120, Avg. loss: 1784.692195\n",
      "Total training time: 6.88 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 811.97, NNZs: 98, Bias: 297.908163, T: 25017600, Avg. loss: 1767.092726\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 810.83, NNZs: 98, Bias: 297.940694, T: 25226080, Avg. loss: 1767.242691\n",
      "Total training time: 6.99 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 809.54, NNZs: 98, Bias: 297.936727, T: 25434560, Avg. loss: 1739.546149\n",
      "Total training time: 7.04 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 808.31, NNZs: 98, Bias: 297.939901, T: 25643040, Avg. loss: 1730.065451\n",
      "Total training time: 7.10 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 807.04, NNZs: 98, Bias: 297.990377, T: 25851520, Avg. loss: 1721.439350\n",
      "Total training time: 7.16 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 805.92, NNZs: 98, Bias: 297.993446, T: 26060000, Avg. loss: 1702.590160\n",
      "Total training time: 7.21 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 804.86, NNZs: 98, Bias: 298.066816, T: 26268480, Avg. loss: 1693.040027\n",
      "Total training time: 7.27 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 803.83, NNZs: 98, Bias: 298.093642, T: 26476960, Avg. loss: 1670.548861\n",
      "Total training time: 7.33 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 802.63, NNZs: 98, Bias: 298.156044, T: 26685440, Avg. loss: 1663.842956\n",
      "Total training time: 7.38 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 801.61, NNZs: 98, Bias: 298.164697, T: 26893920, Avg. loss: 1642.403441\n",
      "Total training time: 7.43 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 800.55, NNZs: 98, Bias: 298.228358, T: 27102400, Avg. loss: 1642.384821\n",
      "Total training time: 7.49 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 799.51, NNZs: 98, Bias: 298.300000, T: 27310880, Avg. loss: 1618.146821\n",
      "Total training time: 7.55 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 798.39, NNZs: 98, Bias: 298.356512, T: 27519360, Avg. loss: 1607.359721\n",
      "Total training time: 7.60 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 797.41, NNZs: 98, Bias: 298.417651, T: 27727840, Avg. loss: 1601.659433\n",
      "Total training time: 7.66 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 796.28, NNZs: 98, Bias: 298.424206, T: 27936320, Avg. loss: 1590.394673\n",
      "Total training time: 7.71 seconds.\n",
      "-- Epoch 135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 795.19, NNZs: 98, Bias: 298.465164, T: 28144800, Avg. loss: 1575.322023\n",
      "Total training time: 7.77 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 794.16, NNZs: 98, Bias: 298.519325, T: 28353280, Avg. loss: 1559.721833\n",
      "Total training time: 7.83 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 792.98, NNZs: 98, Bias: 298.552928, T: 28561760, Avg. loss: 1551.275566\n",
      "Total training time: 7.88 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 791.83, NNZs: 98, Bias: 298.595806, T: 28770240, Avg. loss: 1536.204249\n",
      "Total training time: 7.93 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 790.77, NNZs: 98, Bias: 298.624768, T: 28978720, Avg. loss: 1518.993023\n",
      "Total training time: 8.00 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 789.71, NNZs: 98, Bias: 298.651888, T: 29187200, Avg. loss: 1504.834538\n",
      "Total training time: 8.05 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 788.83, NNZs: 98, Bias: 298.673426, T: 29395680, Avg. loss: 1508.139559\n",
      "Total training time: 8.11 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 787.76, NNZs: 98, Bias: 298.702917, T: 29604160, Avg. loss: 1484.729899\n",
      "Total training time: 8.17 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 786.76, NNZs: 98, Bias: 298.759502, T: 29812640, Avg. loss: 1485.491286\n",
      "Total training time: 8.23 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 785.63, NNZs: 98, Bias: 298.804019, T: 30021120, Avg. loss: 1461.966780\n",
      "Total training time: 8.29 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 784.58, NNZs: 98, Bias: 298.839953, T: 30229600, Avg. loss: 1455.760661\n",
      "Total training time: 8.35 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 783.63, NNZs: 98, Bias: 298.896915, T: 30438080, Avg. loss: 1452.685310\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 782.68, NNZs: 98, Bias: 298.922045, T: 30646560, Avg. loss: 1438.840950\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 781.82, NNZs: 98, Bias: 298.967937, T: 30855040, Avg. loss: 1433.185867\n",
      "Total training time: 8.53 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 780.83, NNZs: 98, Bias: 298.999217, T: 31063520, Avg. loss: 1423.767073\n",
      "Total training time: 8.59 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 779.89, NNZs: 98, Bias: 299.056993, T: 31272000, Avg. loss: 1415.574880\n",
      "Total training time: 8.64 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 778.90, NNZs: 98, Bias: 299.092423, T: 31480480, Avg. loss: 1397.711812\n",
      "Total training time: 8.70 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 778.01, NNZs: 98, Bias: 299.131060, T: 31688960, Avg. loss: 1398.884624\n",
      "Total training time: 8.76 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 777.21, NNZs: 98, Bias: 299.165714, T: 31897440, Avg. loss: 1379.626060\n",
      "Total training time: 8.82 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 776.13, NNZs: 98, Bias: 299.219752, T: 32105920, Avg. loss: 1373.746231\n",
      "Total training time: 8.88 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 775.07, NNZs: 98, Bias: 299.269378, T: 32314400, Avg. loss: 1355.372327\n",
      "Total training time: 8.94 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 774.07, NNZs: 98, Bias: 299.304262, T: 32522880, Avg. loss: 1348.966572\n",
      "Total training time: 9.00 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 773.14, NNZs: 98, Bias: 299.330557, T: 32731360, Avg. loss: 1348.299460\n",
      "Total training time: 9.05 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 772.18, NNZs: 98, Bias: 299.337779, T: 32939840, Avg. loss: 1332.749319\n",
      "Total training time: 9.11 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 771.23, NNZs: 98, Bias: 299.404056, T: 33148320, Avg. loss: 1325.179238\n",
      "Total training time: 9.17 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 770.29, NNZs: 98, Bias: 299.471682, T: 33356800, Avg. loss: 1320.385302\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 769.32, NNZs: 98, Bias: 299.512384, T: 33565280, Avg. loss: 1316.909997\n",
      "Total training time: 9.29 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 768.42, NNZs: 98, Bias: 299.569644, T: 33773760, Avg. loss: 1309.930684\n",
      "Total training time: 9.35 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 767.57, NNZs: 98, Bias: 299.585775, T: 33982240, Avg. loss: 1295.128685\n",
      "Total training time: 9.41 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 766.68, NNZs: 98, Bias: 299.642666, T: 34190720, Avg. loss: 1278.485919\n",
      "Total training time: 9.47 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 765.74, NNZs: 98, Bias: 299.670904, T: 34399200, Avg. loss: 1269.623516\n",
      "Total training time: 9.53 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 764.83, NNZs: 98, Bias: 299.690022, T: 34607680, Avg. loss: 1268.744228\n",
      "Total training time: 9.58 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 764.00, NNZs: 98, Bias: 299.751422, T: 34816160, Avg. loss: 1264.675207\n",
      "Total training time: 9.65 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 763.21, NNZs: 98, Bias: 299.769141, T: 35024640, Avg. loss: 1244.527316\n",
      "Total training time: 9.70 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 762.40, NNZs: 98, Bias: 299.800184, T: 35233120, Avg. loss: 1246.012367\n",
      "Total training time: 9.77 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 761.61, NNZs: 98, Bias: 299.846038, T: 35441600, Avg. loss: 1236.802789\n",
      "Total training time: 9.83 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 760.78, NNZs: 98, Bias: 299.863742, T: 35650080, Avg. loss: 1230.826419\n",
      "Total training time: 9.89 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 759.97, NNZs: 98, Bias: 299.893652, T: 35858560, Avg. loss: 1223.082560\n",
      "Total training time: 9.94 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 759.10, NNZs: 98, Bias: 299.943170, T: 36067040, Avg. loss: 1212.348501\n",
      "Total training time: 9.99 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 758.25, NNZs: 98, Bias: 299.966425, T: 36275520, Avg. loss: 1206.540553\n",
      "Total training time: 10.05 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 757.48, NNZs: 98, Bias: 300.012048, T: 36484000, Avg. loss: 1203.900774\n",
      "Total training time: 10.11 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 756.69, NNZs: 98, Bias: 300.058526, T: 36692480, Avg. loss: 1195.626737\n",
      "Total training time: 10.17 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 755.95, NNZs: 98, Bias: 300.115074, T: 36900960, Avg. loss: 1187.707541\n",
      "Total training time: 10.23 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 755.12, NNZs: 98, Bias: 300.164828, T: 37109440, Avg. loss: 1178.521686\n",
      "Total training time: 10.28 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 754.32, NNZs: 98, Bias: 300.206741, T: 37317920, Avg. loss: 1174.941529\n",
      "Total training time: 10.37 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 753.54, NNZs: 98, Bias: 300.235374, T: 37526400, Avg. loss: 1161.714747\n",
      "Total training time: 10.45 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 752.77, NNZs: 98, Bias: 300.292488, T: 37734880, Avg. loss: 1165.461509\n",
      "Total training time: 10.52 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 752.04, NNZs: 98, Bias: 300.328932, T: 37943360, Avg. loss: 1162.504051\n",
      "Total training time: 10.58 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 751.28, NNZs: 98, Bias: 300.373858, T: 38151840, Avg. loss: 1143.860128\n",
      "Total training time: 10.64 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 750.42, NNZs: 98, Bias: 300.435262, T: 38360320, Avg. loss: 1139.357178\n",
      "Total training time: 10.70 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 749.57, NNZs: 98, Bias: 300.435810, T: 38568800, Avg. loss: 1124.281026\n",
      "Total training time: 10.78 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 748.79, NNZs: 98, Bias: 300.457339, T: 38777280, Avg. loss: 1128.625816\n",
      "Total training time: 10.86 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 748.13, NNZs: 98, Bias: 300.502072, T: 38985760, Avg. loss: 1118.612178\n",
      "Total training time: 10.94 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 747.30, NNZs: 98, Bias: 300.541480, T: 39194240, Avg. loss: 1108.593456\n",
      "Total training time: 11.02 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 746.53, NNZs: 98, Bias: 300.580907, T: 39402720, Avg. loss: 1111.964208\n",
      "Total training time: 11.10 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 745.74, NNZs: 98, Bias: 300.616034, T: 39611200, Avg. loss: 1103.423400\n",
      "Total training time: 11.18 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 745.04, NNZs: 98, Bias: 300.661367, T: 39819680, Avg. loss: 1096.325389\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 744.30, NNZs: 98, Bias: 300.713989, T: 40028160, Avg. loss: 1087.517119\n",
      "Total training time: 11.32 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 743.54, NNZs: 98, Bias: 300.771081, T: 40236640, Avg. loss: 1080.444538\n",
      "Total training time: 11.37 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 742.76, NNZs: 98, Bias: 300.809026, T: 40445120, Avg. loss: 1076.588635\n",
      "Total training time: 11.43 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 741.97, NNZs: 98, Bias: 300.846984, T: 40653600, Avg. loss: 1072.985755\n",
      "Total training time: 11.50 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 741.26, NNZs: 98, Bias: 300.885488, T: 40862080, Avg. loss: 1068.048797\n",
      "Total training time: 11.58 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 740.52, NNZs: 98, Bias: 300.916494, T: 41070560, Avg. loss: 1052.493798\n",
      "Total training time: 11.66 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 739.82, NNZs: 98, Bias: 300.948095, T: 41279040, Avg. loss: 1059.732806\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 739.10, NNZs: 98, Bias: 300.997099, T: 41487520, Avg. loss: 1052.719536\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 738.30, NNZs: 98, Bias: 301.028590, T: 41696000, Avg. loss: 1043.672517\n",
      "Total training time: 11.85 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 737.55, NNZs: 98, Bias: 301.046777, T: 41904480, Avg. loss: 1036.509492\n",
      "Total training time: 11.91 seconds.\n",
      "-- Epoch 202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 736.86, NNZs: 98, Bias: 301.075791, T: 42112960, Avg. loss: 1034.778593\n",
      "Total training time: 11.97 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 736.15, NNZs: 98, Bias: 301.110879, T: 42321440, Avg. loss: 1025.261092\n",
      "Total training time: 12.03 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 735.45, NNZs: 98, Bias: 301.146934, T: 42529920, Avg. loss: 1027.621916\n",
      "Total training time: 12.09 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 734.71, NNZs: 98, Bias: 301.165218, T: 42738400, Avg. loss: 1018.169072\n",
      "Total training time: 12.15 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 734.00, NNZs: 98, Bias: 301.210284, T: 42946880, Avg. loss: 1021.563959\n",
      "Total training time: 12.21 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 733.32, NNZs: 98, Bias: 301.229286, T: 43155360, Avg. loss: 1008.961010\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 732.54, NNZs: 98, Bias: 301.275301, T: 43363840, Avg. loss: 1004.831979\n",
      "Total training time: 12.31 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 731.88, NNZs: 98, Bias: 301.324742, T: 43572320, Avg. loss: 995.595319\n",
      "Total training time: 12.37 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 731.14, NNZs: 98, Bias: 301.357052, T: 43780800, Avg. loss: 994.078005\n",
      "Total training time: 12.43 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 730.42, NNZs: 98, Bias: 301.417459, T: 43989280, Avg. loss: 984.879910\n",
      "Total training time: 12.48 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 729.67, NNZs: 98, Bias: 301.448759, T: 44197760, Avg. loss: 974.254814\n",
      "Total training time: 12.54 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 728.96, NNZs: 98, Bias: 301.485051, T: 44406240, Avg. loss: 971.562400\n",
      "Total training time: 12.60 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 728.28, NNZs: 98, Bias: 301.530056, T: 44614720, Avg. loss: 970.940561\n",
      "Total training time: 12.66 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 727.59, NNZs: 98, Bias: 301.584813, T: 44823200, Avg. loss: 971.480620\n",
      "Total training time: 12.71 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 726.89, NNZs: 98, Bias: 301.637360, T: 45031680, Avg. loss: 967.898778\n",
      "Total training time: 12.77 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 726.12, NNZs: 98, Bias: 301.685648, T: 45240160, Avg. loss: 962.085991\n",
      "Total training time: 12.82 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 725.44, NNZs: 98, Bias: 301.720888, T: 45448640, Avg. loss: 954.433054\n",
      "Total training time: 12.88 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 724.74, NNZs: 98, Bias: 301.731449, T: 45657120, Avg. loss: 944.500822\n",
      "Total training time: 12.95 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 724.04, NNZs: 98, Bias: 301.787398, T: 45865600, Avg. loss: 944.521799\n",
      "Total training time: 13.03 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 723.38, NNZs: 98, Bias: 301.815447, T: 46074080, Avg. loss: 941.764702\n",
      "Total training time: 13.11 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 722.71, NNZs: 98, Bias: 301.867421, T: 46282560, Avg. loss: 936.579756\n",
      "Total training time: 13.17 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 722.05, NNZs: 98, Bias: 301.907522, T: 46491040, Avg. loss: 933.083418\n",
      "Total training time: 13.23 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 721.39, NNZs: 98, Bias: 301.941007, T: 46699520, Avg. loss: 927.905038\n",
      "Total training time: 13.28 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 720.83, NNZs: 98, Bias: 301.977317, T: 46908000, Avg. loss: 922.699905\n",
      "Total training time: 13.34 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 720.13, NNZs: 98, Bias: 301.999015, T: 47116480, Avg. loss: 913.829448\n",
      "Total training time: 13.40 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 719.45, NNZs: 98, Bias: 302.028225, T: 47324960, Avg. loss: 910.680233\n",
      "Total training time: 13.45 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 718.83, NNZs: 98, Bias: 302.062391, T: 47533440, Avg. loss: 911.378977\n",
      "Total training time: 13.50 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 718.24, NNZs: 98, Bias: 302.094057, T: 47741920, Avg. loss: 907.455896\n",
      "Total training time: 13.56 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 717.63, NNZs: 98, Bias: 302.122039, T: 47950400, Avg. loss: 904.810834\n",
      "Total training time: 13.63 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 717.02, NNZs: 98, Bias: 302.161782, T: 48158880, Avg. loss: 894.903791\n",
      "Total training time: 13.72 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 716.40, NNZs: 98, Bias: 302.186366, T: 48367360, Avg. loss: 895.997798\n",
      "Total training time: 13.80 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 715.74, NNZs: 98, Bias: 302.222897, T: 48575840, Avg. loss: 895.809274\n",
      "Total training time: 13.86 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 715.08, NNZs: 98, Bias: 302.240578, T: 48784320, Avg. loss: 883.423722\n",
      "Total training time: 13.91 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 714.51, NNZs: 98, Bias: 302.287843, T: 48992800, Avg. loss: 882.942428\n",
      "Total training time: 13.97 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 713.90, NNZs: 98, Bias: 302.310880, T: 49201280, Avg. loss: 877.451362\n",
      "Total training time: 14.03 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 713.22, NNZs: 98, Bias: 302.351458, T: 49409760, Avg. loss: 877.402887\n",
      "Total training time: 14.08 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 712.68, NNZs: 98, Bias: 302.388006, T: 49618240, Avg. loss: 871.002680\n",
      "Total training time: 14.15 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 712.08, NNZs: 98, Bias: 302.418784, T: 49826720, Avg. loss: 869.836066\n",
      "Total training time: 14.22 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 711.47, NNZs: 98, Bias: 302.450015, T: 50035200, Avg. loss: 863.316922\n",
      "Total training time: 14.30 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 710.87, NNZs: 98, Bias: 302.499087, T: 50243680, Avg. loss: 862.953341\n",
      "Total training time: 14.35 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 710.28, NNZs: 98, Bias: 302.534455, T: 50452160, Avg. loss: 857.234644\n",
      "Total training time: 14.41 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 709.68, NNZs: 98, Bias: 302.576411, T: 50660640, Avg. loss: 856.108393\n",
      "Total training time: 14.46 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 709.06, NNZs: 98, Bias: 302.630996, T: 50869120, Avg. loss: 853.500278\n",
      "Total training time: 14.52 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 708.46, NNZs: 98, Bias: 302.667026, T: 51077600, Avg. loss: 848.014949\n",
      "Total training time: 14.59 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 707.81, NNZs: 98, Bias: 302.703959, T: 51286080, Avg. loss: 842.168336\n",
      "Total training time: 14.66 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 707.23, NNZs: 98, Bias: 302.739376, T: 51494560, Avg. loss: 838.882953\n",
      "Total training time: 14.72 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 706.61, NNZs: 98, Bias: 302.789165, T: 51703040, Avg. loss: 839.330077\n",
      "Total training time: 14.78 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 706.11, NNZs: 98, Bias: 302.808872, T: 51911520, Avg. loss: 828.957840\n",
      "Total training time: 14.83 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 705.49, NNZs: 98, Bias: 302.851354, T: 52120000, Avg. loss: 828.178857\n",
      "Total training time: 14.88 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 704.87, NNZs: 98, Bias: 302.879517, T: 52328480, Avg. loss: 821.278441\n",
      "Total training time: 14.94 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 704.28, NNZs: 98, Bias: 302.902781, T: 52536960, Avg. loss: 823.939435\n",
      "Total training time: 15.00 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 703.64, NNZs: 98, Bias: 302.951213, T: 52745440, Avg. loss: 817.791416\n",
      "Total training time: 15.05 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 703.05, NNZs: 98, Bias: 302.971843, T: 52953920, Avg. loss: 807.757236\n",
      "Total training time: 15.13 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 702.48, NNZs: 98, Bias: 302.995013, T: 53162400, Avg. loss: 808.264115\n",
      "Total training time: 15.20 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 701.89, NNZs: 98, Bias: 303.042885, T: 53370880, Avg. loss: 806.668831\n",
      "Total training time: 15.26 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 701.33, NNZs: 98, Bias: 303.080638, T: 53579360, Avg. loss: 802.778605\n",
      "Total training time: 15.32 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 700.76, NNZs: 98, Bias: 303.107455, T: 53787840, Avg. loss: 802.809104\n",
      "Total training time: 15.37 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 700.16, NNZs: 98, Bias: 303.142176, T: 53996320, Avg. loss: 788.022530\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 699.57, NNZs: 98, Bias: 303.163457, T: 54204800, Avg. loss: 789.662283\n",
      "Total training time: 15.50 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 699.03, NNZs: 98, Bias: 303.176337, T: 54413280, Avg. loss: 794.188191\n",
      "Total training time: 15.57 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 698.46, NNZs: 98, Bias: 303.209343, T: 54621760, Avg. loss: 791.094661\n",
      "Total training time: 15.63 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 697.82, NNZs: 98, Bias: 303.245515, T: 54830240, Avg. loss: 781.589827\n",
      "Total training time: 15.69 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 697.28, NNZs: 98, Bias: 303.274635, T: 55038720, Avg. loss: 778.238152\n",
      "Total training time: 15.75 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 696.78, NNZs: 98, Bias: 303.306034, T: 55247200, Avg. loss: 774.360388\n",
      "Total training time: 15.80 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 696.18, NNZs: 98, Bias: 303.354612, T: 55455680, Avg. loss: 772.119244\n",
      "Total training time: 15.86 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 695.66, NNZs: 98, Bias: 303.366666, T: 55664160, Avg. loss: 772.914718\n",
      "Total training time: 15.91 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 695.11, NNZs: 98, Bias: 303.387465, T: 55872640, Avg. loss: 767.196432\n",
      "Total training time: 15.97 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 694.60, NNZs: 98, Bias: 303.410152, T: 56081120, Avg. loss: 763.948832\n",
      "Total training time: 16.03 seconds.\n",
      "-- Epoch 270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 694.09, NNZs: 98, Bias: 303.438976, T: 56289600, Avg. loss: 761.133808\n",
      "Total training time: 16.11 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 693.48, NNZs: 98, Bias: 303.472680, T: 56498080, Avg. loss: 756.044601\n",
      "Total training time: 16.17 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 692.93, NNZs: 98, Bias: 303.501140, T: 56706560, Avg. loss: 756.996511\n",
      "Total training time: 16.23 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 692.36, NNZs: 98, Bias: 303.522794, T: 56915040, Avg. loss: 749.959206\n",
      "Total training time: 16.28 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 691.82, NNZs: 98, Bias: 303.561211, T: 57123520, Avg. loss: 753.369050\n",
      "Total training time: 16.34 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 691.32, NNZs: 98, Bias: 303.602463, T: 57332000, Avg. loss: 751.699401\n",
      "Total training time: 16.39 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 690.72, NNZs: 98, Bias: 303.632767, T: 57540480, Avg. loss: 746.591740\n",
      "Total training time: 16.45 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 690.15, NNZs: 98, Bias: 303.659132, T: 57748960, Avg. loss: 742.357387\n",
      "Total training time: 16.50 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 689.57, NNZs: 98, Bias: 303.692509, T: 57957440, Avg. loss: 743.276002\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 689.02, NNZs: 98, Bias: 303.713510, T: 58165920, Avg. loss: 731.217013\n",
      "Total training time: 16.63 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 688.53, NNZs: 98, Bias: 303.734430, T: 58374400, Avg. loss: 730.961685\n",
      "Total training time: 16.70 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 687.98, NNZs: 98, Bias: 303.759398, T: 58582880, Avg. loss: 733.481449\n",
      "Total training time: 16.76 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 687.42, NNZs: 98, Bias: 303.801308, T: 58791360, Avg. loss: 729.271971\n",
      "Total training time: 16.82 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 686.88, NNZs: 98, Bias: 303.824221, T: 58999840, Avg. loss: 727.852803\n",
      "Total training time: 16.88 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 686.32, NNZs: 98, Bias: 303.833693, T: 59208320, Avg. loss: 724.365935\n",
      "Total training time: 16.93 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 685.76, NNZs: 98, Bias: 303.852737, T: 59416800, Avg. loss: 721.267620\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 685.25, NNZs: 98, Bias: 303.884967, T: 59625280, Avg. loss: 716.302409\n",
      "Total training time: 17.04 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 684.78, NNZs: 98, Bias: 303.900871, T: 59833760, Avg. loss: 713.830235\n",
      "Total training time: 17.10 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 684.29, NNZs: 98, Bias: 303.930252, T: 60042240, Avg. loss: 715.084040\n",
      "Total training time: 17.15 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 683.80, NNZs: 98, Bias: 303.956686, T: 60250720, Avg. loss: 708.453409\n",
      "Total training time: 17.22 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 683.24, NNZs: 98, Bias: 303.982024, T: 60459200, Avg. loss: 703.969565\n",
      "Total training time: 17.30 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 682.77, NNZs: 98, Bias: 303.987315, T: 60667680, Avg. loss: 705.194793\n",
      "Total training time: 17.37 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 682.21, NNZs: 98, Bias: 304.017090, T: 60876160, Avg. loss: 699.681082\n",
      "Total training time: 17.42 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 681.66, NNZs: 98, Bias: 304.047091, T: 61084640, Avg. loss: 700.418336\n",
      "Total training time: 17.47 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 681.11, NNZs: 98, Bias: 304.060807, T: 61293120, Avg. loss: 691.134684\n",
      "Total training time: 17.53 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 680.63, NNZs: 98, Bias: 304.079054, T: 61501600, Avg. loss: 696.891756\n",
      "Total training time: 17.60 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 680.09, NNZs: 98, Bias: 304.102582, T: 61710080, Avg. loss: 688.828746\n",
      "Total training time: 17.67 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 679.57, NNZs: 98, Bias: 304.140104, T: 61918560, Avg. loss: 688.883547\n",
      "Total training time: 17.72 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 679.07, NNZs: 98, Bias: 304.172358, T: 62127040, Avg. loss: 689.795496\n",
      "Total training time: 17.79 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 678.48, NNZs: 98, Bias: 304.201920, T: 62335520, Avg. loss: 685.488052\n",
      "Total training time: 17.84 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 677.97, NNZs: 98, Bias: 304.229931, T: 62544000, Avg. loss: 686.084123\n",
      "Total training time: 17.90 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 677.44, NNZs: 98, Bias: 304.254506, T: 62752480, Avg. loss: 678.141847\n",
      "Total training time: 17.95 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 676.93, NNZs: 98, Bias: 304.278840, T: 62960960, Avg. loss: 675.372631\n",
      "Total training time: 18.01 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 676.37, NNZs: 98, Bias: 304.303730, T: 63169440, Avg. loss: 676.574579\n",
      "Total training time: 18.06 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 675.95, NNZs: 98, Bias: 304.314642, T: 63377920, Avg. loss: 672.153449\n",
      "Total training time: 18.12 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 675.45, NNZs: 98, Bias: 304.350543, T: 63586400, Avg. loss: 673.998361\n",
      "Total training time: 18.19 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 674.99, NNZs: 98, Bias: 304.380528, T: 63794880, Avg. loss: 670.700337\n",
      "Total training time: 18.28 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 674.46, NNZs: 98, Bias: 304.409318, T: 64003360, Avg. loss: 663.827366\n",
      "Total training time: 18.36 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 673.94, NNZs: 98, Bias: 304.429446, T: 64211840, Avg. loss: 655.451386\n",
      "Total training time: 18.41 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 673.44, NNZs: 98, Bias: 304.454458, T: 64420320, Avg. loss: 662.308527\n",
      "Total training time: 18.47 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 673.01, NNZs: 98, Bias: 304.476773, T: 64628800, Avg. loss: 661.355091\n",
      "Total training time: 18.53 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 672.49, NNZs: 98, Bias: 304.507206, T: 64837280, Avg. loss: 657.677080\n",
      "Total training time: 18.58 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 672.00, NNZs: 98, Bias: 304.526587, T: 65045760, Avg. loss: 656.432785\n",
      "Total training time: 18.63 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 671.53, NNZs: 98, Bias: 304.551141, T: 65254240, Avg. loss: 652.175612\n",
      "Total training time: 18.69 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 671.05, NNZs: 98, Bias: 304.575162, T: 65462720, Avg. loss: 652.925622\n",
      "Total training time: 18.76 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 670.59, NNZs: 98, Bias: 304.604887, T: 65671200, Avg. loss: 649.082424\n",
      "Total training time: 18.85 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 670.07, NNZs: 98, Bias: 304.626773, T: 65879680, Avg. loss: 649.463218\n",
      "Total training time: 18.92 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 669.64, NNZs: 98, Bias: 304.643293, T: 66088160, Avg. loss: 645.255090\n",
      "Total training time: 18.97 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 669.15, NNZs: 98, Bias: 304.663846, T: 66296640, Avg. loss: 642.958545\n",
      "Total training time: 19.03 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 668.67, NNZs: 98, Bias: 304.691100, T: 66505120, Avg. loss: 643.770253\n",
      "Total training time: 19.08 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 668.22, NNZs: 98, Bias: 304.719474, T: 66713600, Avg. loss: 636.660870\n",
      "Total training time: 19.14 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 667.70, NNZs: 98, Bias: 304.736534, T: 66922080, Avg. loss: 638.798338\n",
      "Total training time: 19.20 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 667.24, NNZs: 98, Bias: 304.749666, T: 67130560, Avg. loss: 629.242331\n",
      "Total training time: 19.27 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 666.76, NNZs: 98, Bias: 304.769130, T: 67339040, Avg. loss: 633.182109\n",
      "Total training time: 19.35 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 666.30, NNZs: 98, Bias: 304.781152, T: 67547520, Avg. loss: 630.608405\n",
      "Total training time: 19.41 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 665.84, NNZs: 98, Bias: 304.796071, T: 67756000, Avg. loss: 625.227873\n",
      "Total training time: 19.47 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 665.35, NNZs: 98, Bias: 304.822598, T: 67964480, Avg. loss: 628.203095\n",
      "Total training time: 19.52 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 664.88, NNZs: 98, Bias: 304.840379, T: 68172960, Avg. loss: 625.774855\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 664.40, NNZs: 98, Bias: 304.865577, T: 68381440, Avg. loss: 624.337015\n",
      "Total training time: 19.63 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 663.91, NNZs: 98, Bias: 304.874041, T: 68589920, Avg. loss: 617.463682\n",
      "Total training time: 19.69 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 663.40, NNZs: 98, Bias: 304.888165, T: 68798400, Avg. loss: 616.175534\n",
      "Total training time: 19.74 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 662.95, NNZs: 98, Bias: 304.902236, T: 69006880, Avg. loss: 617.040977\n",
      "Total training time: 19.80 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 662.45, NNZs: 98, Bias: 304.925528, T: 69215360, Avg. loss: 612.736657\n",
      "Total training time: 19.88 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 661.96, NNZs: 98, Bias: 304.944842, T: 69423840, Avg. loss: 608.613615\n",
      "Total training time: 19.96 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 661.52, NNZs: 98, Bias: 304.947438, T: 69632320, Avg. loss: 608.212034\n",
      "Total training time: 20.02 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 661.12, NNZs: 98, Bias: 304.960059, T: 69840800, Avg. loss: 609.279168\n",
      "Total training time: 20.07 seconds.\n",
      "-- Epoch 336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 660.65, NNZs: 98, Bias: 304.981078, T: 70049280, Avg. loss: 604.620841\n",
      "Total training time: 20.13 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 660.18, NNZs: 98, Bias: 305.010029, T: 70257760, Avg. loss: 606.365872\n",
      "Total training time: 20.19 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 659.77, NNZs: 98, Bias: 305.024379, T: 70466240, Avg. loss: 598.240830\n",
      "Total training time: 20.24 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 659.31, NNZs: 98, Bias: 305.046052, T: 70674720, Avg. loss: 602.181521\n",
      "Total training time: 20.31 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 658.90, NNZs: 98, Bias: 305.052551, T: 70883200, Avg. loss: 599.428074\n",
      "Total training time: 20.39 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 658.45, NNZs: 98, Bias: 305.079593, T: 71091680, Avg. loss: 597.037628\n",
      "Total training time: 20.46 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 657.95, NNZs: 98, Bias: 305.103478, T: 71300160, Avg. loss: 594.042951\n",
      "Total training time: 20.51 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 657.51, NNZs: 98, Bias: 305.127847, T: 71508640, Avg. loss: 595.182714\n",
      "Total training time: 20.57 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 657.02, NNZs: 98, Bias: 305.150191, T: 71717120, Avg. loss: 592.137108\n",
      "Total training time: 20.63 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 656.59, NNZs: 98, Bias: 305.158816, T: 71925600, Avg. loss: 589.444810\n",
      "Total training time: 20.68 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 656.18, NNZs: 98, Bias: 305.178958, T: 72134080, Avg. loss: 587.083532\n",
      "Total training time: 20.74 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 655.72, NNZs: 98, Bias: 305.201105, T: 72342560, Avg. loss: 587.438103\n",
      "Total training time: 20.79 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 655.23, NNZs: 98, Bias: 305.217675, T: 72551040, Avg. loss: 583.101211\n",
      "Total training time: 20.85 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 654.81, NNZs: 98, Bias: 305.244114, T: 72759520, Avg. loss: 583.202050\n",
      "Total training time: 20.93 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 654.35, NNZs: 98, Bias: 305.259619, T: 72968000, Avg. loss: 577.395283\n",
      "Total training time: 20.99 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 653.92, NNZs: 98, Bias: 305.274117, T: 73176480, Avg. loss: 582.233634\n",
      "Total training time: 21.05 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 653.46, NNZs: 98, Bias: 305.293487, T: 73384960, Avg. loss: 579.189735\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 653.01, NNZs: 98, Bias: 305.309420, T: 73593440, Avg. loss: 577.707556\n",
      "Total training time: 21.16 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 652.57, NNZs: 98, Bias: 305.328673, T: 73801920, Avg. loss: 571.722348\n",
      "Total training time: 21.22 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 652.18, NNZs: 98, Bias: 305.348027, T: 74010400, Avg. loss: 574.627558\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 651.80, NNZs: 98, Bias: 305.375011, T: 74218880, Avg. loss: 573.508222\n",
      "Total training time: 21.33 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 651.35, NNZs: 98, Bias: 305.403805, T: 74427360, Avg. loss: 572.688250\n",
      "Total training time: 21.41 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 650.92, NNZs: 98, Bias: 305.417625, T: 74635840, Avg. loss: 566.702634\n",
      "Total training time: 21.48 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 650.48, NNZs: 98, Bias: 305.429004, T: 74844320, Avg. loss: 563.915281\n",
      "Total training time: 21.56 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 650.05, NNZs: 98, Bias: 305.443146, T: 75052800, Avg. loss: 563.215733\n",
      "Total training time: 21.62 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 649.58, NNZs: 98, Bias: 305.464038, T: 75261280, Avg. loss: 561.804695\n",
      "Total training time: 21.68 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 649.14, NNZs: 98, Bias: 305.478765, T: 75469760, Avg. loss: 560.465571\n",
      "Total training time: 21.74 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 648.72, NNZs: 98, Bias: 305.507885, T: 75678240, Avg. loss: 560.520477\n",
      "Total training time: 21.79 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 648.31, NNZs: 98, Bias: 305.512247, T: 75886720, Avg. loss: 555.538951\n",
      "Total training time: 21.85 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 647.87, NNZs: 98, Bias: 305.542501, T: 76095200, Avg. loss: 558.003013\n",
      "Total training time: 21.93 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 647.44, NNZs: 98, Bias: 305.560356, T: 76303680, Avg. loss: 555.349550\n",
      "Total training time: 22.00 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 647.01, NNZs: 98, Bias: 305.575542, T: 76512160, Avg. loss: 553.961369\n",
      "Total training time: 22.07 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 646.59, NNZs: 98, Bias: 305.597734, T: 76720640, Avg. loss: 551.344011\n",
      "Total training time: 22.12 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 646.18, NNZs: 98, Bias: 305.615566, T: 76929120, Avg. loss: 547.943862\n",
      "Total training time: 22.17 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 645.72, NNZs: 98, Bias: 305.630875, T: 77137600, Avg. loss: 546.248521\n",
      "Total training time: 22.23 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 645.31, NNZs: 98, Bias: 305.648087, T: 77346080, Avg. loss: 542.741509\n",
      "Total training time: 22.29 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 644.91, NNZs: 98, Bias: 305.662553, T: 77554560, Avg. loss: 544.454140\n",
      "Total training time: 22.34 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 644.55, NNZs: 98, Bias: 305.684568, T: 77763040, Avg. loss: 543.420947\n",
      "Total training time: 22.40 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 644.13, NNZs: 98, Bias: 305.698565, T: 77971520, Avg. loss: 541.339119\n",
      "Total training time: 22.48 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 643.69, NNZs: 98, Bias: 305.722252, T: 78180000, Avg. loss: 541.435713\n",
      "Total training time: 22.56 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 643.24, NNZs: 98, Bias: 305.741286, T: 78388480, Avg. loss: 535.523756\n",
      "Total training time: 22.62 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 642.87, NNZs: 98, Bias: 305.752633, T: 78596960, Avg. loss: 536.509265\n",
      "Total training time: 22.67 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 642.44, NNZs: 98, Bias: 305.764577, T: 78805440, Avg. loss: 536.587533\n",
      "Total training time: 22.74 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 642.01, NNZs: 98, Bias: 305.774208, T: 79013920, Avg. loss: 533.163216\n",
      "Total training time: 22.79 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 641.58, NNZs: 98, Bias: 305.797342, T: 79222400, Avg. loss: 532.385312\n",
      "Total training time: 22.84 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 641.16, NNZs: 98, Bias: 305.818140, T: 79430880, Avg. loss: 529.746449\n",
      "Total training time: 22.90 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 640.75, NNZs: 98, Bias: 305.836112, T: 79639360, Avg. loss: 529.938311\n",
      "Total training time: 22.95 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 640.35, NNZs: 98, Bias: 305.849153, T: 79847840, Avg. loss: 525.441951\n",
      "Total training time: 23.01 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 639.98, NNZs: 98, Bias: 305.864020, T: 80056320, Avg. loss: 527.243245\n",
      "Total training time: 23.06 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 639.57, NNZs: 98, Bias: 305.877248, T: 80264800, Avg. loss: 521.513158\n",
      "Total training time: 23.12 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 639.13, NNZs: 98, Bias: 305.888323, T: 80473280, Avg. loss: 521.665847\n",
      "Total training time: 23.18 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 638.73, NNZs: 98, Bias: 305.905816, T: 80681760, Avg. loss: 518.785103\n",
      "Total training time: 23.23 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 638.34, NNZs: 98, Bias: 305.905692, T: 80890240, Avg. loss: 520.759096\n",
      "Total training time: 23.28 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 637.93, NNZs: 98, Bias: 305.918774, T: 81098720, Avg. loss: 520.284588\n",
      "Total training time: 23.34 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 637.53, NNZs: 98, Bias: 305.934284, T: 81307200, Avg. loss: 517.474788\n",
      "Total training time: 23.39 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 637.13, NNZs: 98, Bias: 305.950005, T: 81515680, Avg. loss: 515.902099\n",
      "Total training time: 23.45 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 636.75, NNZs: 98, Bias: 305.961881, T: 81724160, Avg. loss: 513.651164\n",
      "Total training time: 23.50 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 636.36, NNZs: 98, Bias: 305.982773, T: 81932640, Avg. loss: 515.071533\n",
      "Total training time: 23.56 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 635.95, NNZs: 98, Bias: 305.996176, T: 82141120, Avg. loss: 509.136762\n",
      "Total training time: 23.61 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 635.56, NNZs: 98, Bias: 306.018670, T: 82349600, Avg. loss: 514.873121\n",
      "Total training time: 23.67 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 635.18, NNZs: 98, Bias: 306.039045, T: 82558080, Avg. loss: 509.652600\n",
      "Total training time: 23.72 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 634.79, NNZs: 98, Bias: 306.060820, T: 82766560, Avg. loss: 506.671776\n",
      "Total training time: 23.78 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 634.38, NNZs: 98, Bias: 306.069993, T: 82975040, Avg. loss: 508.769853\n",
      "Total training time: 23.84 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 633.97, NNZs: 98, Bias: 306.078892, T: 83183520, Avg. loss: 504.549908\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 633.55, NNZs: 98, Bias: 306.094739, T: 83392000, Avg. loss: 504.871225\n",
      "Total training time: 23.94 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 633.15, NNZs: 98, Bias: 306.113311, T: 83600480, Avg. loss: 506.503405\n",
      "Total training time: 24.00 seconds.\n",
      "-- Epoch 402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 632.77, NNZs: 98, Bias: 306.119646, T: 83808960, Avg. loss: 500.397489\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 632.34, NNZs: 98, Bias: 306.139534, T: 84017440, Avg. loss: 499.344545\n",
      "Total training time: 24.12 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 631.90, NNZs: 98, Bias: 306.157720, T: 84225920, Avg. loss: 503.301570\n",
      "Total training time: 24.20 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 631.49, NNZs: 98, Bias: 306.179418, T: 84434400, Avg. loss: 500.312009\n",
      "Total training time: 24.28 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 631.08, NNZs: 98, Bias: 306.190661, T: 84642880, Avg. loss: 496.471096\n",
      "Total training time: 24.33 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 630.69, NNZs: 98, Bias: 306.201754, T: 84851360, Avg. loss: 499.535994\n",
      "Total training time: 24.39 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 630.28, NNZs: 98, Bias: 306.218466, T: 85059840, Avg. loss: 493.272868\n",
      "Total training time: 24.44 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 629.90, NNZs: 98, Bias: 306.232915, T: 85268320, Avg. loss: 493.918463\n",
      "Total training time: 24.50 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 629.51, NNZs: 98, Bias: 306.239952, T: 85476800, Avg. loss: 489.428517\n",
      "Total training time: 24.56 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 629.12, NNZs: 98, Bias: 306.252103, T: 85685280, Avg. loss: 492.734368\n",
      "Total training time: 24.64 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 628.73, NNZs: 98, Bias: 306.268887, T: 85893760, Avg. loss: 488.597243\n",
      "Total training time: 24.72 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 628.37, NNZs: 98, Bias: 306.284584, T: 86102240, Avg. loss: 489.274146\n",
      "Total training time: 24.78 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 627.98, NNZs: 98, Bias: 306.298852, T: 86310720, Avg. loss: 485.007869\n",
      "Total training time: 24.84 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 627.57, NNZs: 98, Bias: 306.318983, T: 86519200, Avg. loss: 482.520047\n",
      "Total training time: 24.89 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 627.18, NNZs: 98, Bias: 306.340104, T: 86727680, Avg. loss: 484.453751\n",
      "Total training time: 24.95 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 626.78, NNZs: 98, Bias: 306.352525, T: 86936160, Avg. loss: 481.381575\n",
      "Total training time: 25.01 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 626.38, NNZs: 98, Bias: 306.371481, T: 87144640, Avg. loss: 481.862276\n",
      "Total training time: 25.07 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 625.99, NNZs: 98, Bias: 306.389132, T: 87353120, Avg. loss: 480.249439\n",
      "Total training time: 25.14 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 625.61, NNZs: 98, Bias: 306.410624, T: 87561600, Avg. loss: 481.234092\n",
      "Total training time: 25.21 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 625.20, NNZs: 98, Bias: 306.420421, T: 87770080, Avg. loss: 475.821488\n",
      "Total training time: 25.26 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 624.85, NNZs: 98, Bias: 306.424413, T: 87978560, Avg. loss: 476.437129\n",
      "Total training time: 25.31 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 624.47, NNZs: 98, Bias: 306.422721, T: 88187040, Avg. loss: 476.328412\n",
      "Total training time: 25.37 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 624.11, NNZs: 98, Bias: 306.438126, T: 88395520, Avg. loss: 473.979335\n",
      "Total training time: 25.43 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 623.73, NNZs: 98, Bias: 306.460050, T: 88604000, Avg. loss: 474.098220\n",
      "Total training time: 25.48 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 623.36, NNZs: 98, Bias: 306.462529, T: 88812480, Avg. loss: 470.909127\n",
      "Total training time: 25.53 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 623.02, NNZs: 98, Bias: 306.477721, T: 89020960, Avg. loss: 472.339266\n",
      "Total training time: 25.59 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 622.65, NNZs: 98, Bias: 306.492751, T: 89229440, Avg. loss: 471.043443\n",
      "Total training time: 25.65 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 622.26, NNZs: 98, Bias: 306.514018, T: 89437920, Avg. loss: 471.055430\n",
      "Total training time: 25.73 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 621.89, NNZs: 98, Bias: 306.523061, T: 89646400, Avg. loss: 468.034311\n",
      "Total training time: 25.81 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 621.49, NNZs: 98, Bias: 306.539655, T: 89854880, Avg. loss: 466.275748\n",
      "Total training time: 25.89 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 621.11, NNZs: 98, Bias: 306.565545, T: 90063360, Avg. loss: 464.332379\n",
      "Total training time: 25.95 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 620.75, NNZs: 98, Bias: 306.568981, T: 90271840, Avg. loss: 461.068887\n",
      "Total training time: 26.01 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 620.42, NNZs: 98, Bias: 306.580153, T: 90480320, Avg. loss: 461.782554\n",
      "Total training time: 26.06 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 620.06, NNZs: 98, Bias: 306.584345, T: 90688800, Avg. loss: 460.610998\n",
      "Total training time: 26.12 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 619.69, NNZs: 98, Bias: 306.595027, T: 90897280, Avg. loss: 459.768526\n",
      "Total training time: 26.18 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 619.35, NNZs: 98, Bias: 306.606234, T: 91105760, Avg. loss: 459.409915\n",
      "Total training time: 26.26 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 618.97, NNZs: 98, Bias: 306.624317, T: 91314240, Avg. loss: 455.607249\n",
      "Total training time: 26.34 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 618.59, NNZs: 98, Bias: 306.630009, T: 91522720, Avg. loss: 456.037469\n",
      "Total training time: 26.40 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 618.26, NNZs: 98, Bias: 306.641353, T: 91731200, Avg. loss: 456.509722\n",
      "Total training time: 26.46 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 617.88, NNZs: 98, Bias: 306.655406, T: 91939680, Avg. loss: 455.865144\n",
      "Total training time: 26.51 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 617.53, NNZs: 98, Bias: 306.666050, T: 92148160, Avg. loss: 455.001806\n",
      "Total training time: 26.57 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 617.18, NNZs: 98, Bias: 306.682735, T: 92356640, Avg. loss: 454.519318\n",
      "Total training time: 26.62 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 616.83, NNZs: 98, Bias: 306.696038, T: 92565120, Avg. loss: 452.626696\n",
      "Total training time: 26.68 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 616.46, NNZs: 98, Bias: 306.706940, T: 92773600, Avg. loss: 449.155123\n",
      "Total training time: 26.74 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 616.07, NNZs: 98, Bias: 306.720613, T: 92982080, Avg. loss: 449.704106\n",
      "Total training time: 26.83 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 615.74, NNZs: 98, Bias: 306.724701, T: 93190560, Avg. loss: 448.791066\n",
      "Total training time: 26.91 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 615.38, NNZs: 98, Bias: 306.739491, T: 93399040, Avg. loss: 447.879595\n",
      "Total training time: 26.98 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 615.01, NNZs: 98, Bias: 306.750719, T: 93607520, Avg. loss: 446.535852\n",
      "Total training time: 27.03 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 614.64, NNZs: 98, Bias: 306.775368, T: 93816000, Avg. loss: 445.981580\n",
      "Total training time: 27.10 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 614.27, NNZs: 98, Bias: 306.784740, T: 94024480, Avg. loss: 445.331537\n",
      "Total training time: 27.17 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 613.89, NNZs: 98, Bias: 306.801840, T: 94232960, Avg. loss: 443.774067\n",
      "Total training time: 27.23 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 613.56, NNZs: 98, Bias: 306.812863, T: 94441440, Avg. loss: 443.664473\n",
      "Total training time: 27.29 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 613.20, NNZs: 98, Bias: 306.820050, T: 94649920, Avg. loss: 440.933030\n",
      "Total training time: 27.34 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 612.85, NNZs: 98, Bias: 306.829234, T: 94858400, Avg. loss: 442.015721\n",
      "Total training time: 27.40 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 612.47, NNZs: 98, Bias: 306.839340, T: 95066880, Avg. loss: 439.921763\n",
      "Total training time: 27.45 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 612.09, NNZs: 98, Bias: 306.843646, T: 95275360, Avg. loss: 433.472128\n",
      "Total training time: 27.51 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 611.77, NNZs: 98, Bias: 306.858123, T: 95483840, Avg. loss: 438.323803\n",
      "Total training time: 27.57 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 611.40, NNZs: 98, Bias: 306.864402, T: 95692320, Avg. loss: 436.011713\n",
      "Total training time: 27.63 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 611.03, NNZs: 98, Bias: 306.866071, T: 95900800, Avg. loss: 432.712765\n",
      "Total training time: 27.69 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 610.67, NNZs: 98, Bias: 306.882526, T: 96109280, Avg. loss: 434.266406\n",
      "Total training time: 27.75 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 610.33, NNZs: 98, Bias: 306.890634, T: 96317760, Avg. loss: 431.515381\n",
      "Total training time: 27.81 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 609.96, NNZs: 98, Bias: 306.903705, T: 96526240, Avg. loss: 432.969264\n",
      "Total training time: 27.86 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 609.60, NNZs: 98, Bias: 306.917362, T: 96734720, Avg. loss: 432.481744\n",
      "Total training time: 27.92 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 609.26, NNZs: 98, Bias: 306.924487, T: 96943200, Avg. loss: 428.375730\n",
      "Total training time: 27.98 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 608.93, NNZs: 98, Bias: 306.934687, T: 97151680, Avg. loss: 430.845182\n",
      "Total training time: 28.04 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 608.57, NNZs: 98, Bias: 306.948055, T: 97360160, Avg. loss: 430.471673\n",
      "Total training time: 28.09 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 608.20, NNZs: 98, Bias: 306.959964, T: 97568640, Avg. loss: 424.476005\n",
      "Total training time: 28.14 seconds.\n",
      "-- Epoch 469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 607.85, NNZs: 98, Bias: 306.964877, T: 97777120, Avg. loss: 424.986706\n",
      "Total training time: 28.20 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 607.50, NNZs: 98, Bias: 306.980407, T: 97985600, Avg. loss: 425.477065\n",
      "Total training time: 28.26 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 607.14, NNZs: 98, Bias: 306.989274, T: 98194080, Avg. loss: 424.368101\n",
      "Total training time: 28.32 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 606.79, NNZs: 98, Bias: 306.998329, T: 98402560, Avg. loss: 420.852227\n",
      "Total training time: 28.37 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 606.46, NNZs: 98, Bias: 307.010913, T: 98611040, Avg. loss: 423.360824\n",
      "Total training time: 28.43 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 606.11, NNZs: 98, Bias: 307.018613, T: 98819520, Avg. loss: 421.855875\n",
      "Total training time: 28.49 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 605.78, NNZs: 98, Bias: 307.020737, T: 99028000, Avg. loss: 420.526171\n",
      "Total training time: 28.55 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 605.45, NNZs: 98, Bias: 307.026793, T: 99236480, Avg. loss: 416.351798\n",
      "Total training time: 28.60 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 605.11, NNZs: 98, Bias: 307.041188, T: 99444960, Avg. loss: 420.173067\n",
      "Total training time: 28.66 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 604.76, NNZs: 98, Bias: 307.048325, T: 99653440, Avg. loss: 416.514789\n",
      "Total training time: 28.71 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 604.42, NNZs: 98, Bias: 307.049625, T: 99861920, Avg. loss: 417.416841\n",
      "Total training time: 28.77 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 604.09, NNZs: 98, Bias: 307.060840, T: 100070400, Avg. loss: 418.495191\n",
      "Total training time: 28.83 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 603.77, NNZs: 98, Bias: 307.073518, T: 100278880, Avg. loss: 414.944901\n",
      "Total training time: 28.89 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 603.44, NNZs: 98, Bias: 307.087961, T: 100487360, Avg. loss: 414.978461\n",
      "Total training time: 28.95 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 603.09, NNZs: 98, Bias: 307.090947, T: 100695840, Avg. loss: 414.203680\n",
      "Total training time: 29.01 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 602.74, NNZs: 98, Bias: 307.104837, T: 100904320, Avg. loss: 411.062845\n",
      "Total training time: 29.06 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 602.37, NNZs: 98, Bias: 307.114143, T: 101112800, Avg. loss: 408.702765\n",
      "Total training time: 29.12 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 602.07, NNZs: 98, Bias: 307.118488, T: 101321280, Avg. loss: 413.566686\n",
      "Total training time: 29.18 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 601.75, NNZs: 98, Bias: 307.130716, T: 101529760, Avg. loss: 410.233398\n",
      "Total training time: 29.23 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 601.39, NNZs: 98, Bias: 307.134559, T: 101738240, Avg. loss: 409.655626\n",
      "Total training time: 29.28 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 601.05, NNZs: 98, Bias: 307.141926, T: 101946720, Avg. loss: 408.429370\n",
      "Total training time: 29.34 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 600.73, NNZs: 98, Bias: 307.149956, T: 102155200, Avg. loss: 407.192331\n",
      "Total training time: 29.40 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 600.38, NNZs: 98, Bias: 307.163258, T: 102363680, Avg. loss: 405.546430\n",
      "Total training time: 29.46 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 600.05, NNZs: 98, Bias: 307.178383, T: 102572160, Avg. loss: 407.218412\n",
      "Total training time: 29.52 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 599.73, NNZs: 98, Bias: 307.193093, T: 102780640, Avg. loss: 406.069515\n",
      "Total training time: 29.57 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 599.37, NNZs: 98, Bias: 307.210783, T: 102989120, Avg. loss: 405.897112\n",
      "Total training time: 29.63 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 599.08, NNZs: 98, Bias: 307.222522, T: 103197600, Avg. loss: 402.506015\n",
      "Total training time: 29.69 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 598.74, NNZs: 98, Bias: 307.238881, T: 103406080, Avg. loss: 402.375707\n",
      "Total training time: 29.77 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 598.39, NNZs: 98, Bias: 307.251540, T: 103614560, Avg. loss: 400.395620\n",
      "Total training time: 29.85 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 598.06, NNZs: 98, Bias: 307.264264, T: 103823040, Avg. loss: 401.246732\n",
      "Total training time: 29.90 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 597.70, NNZs: 98, Bias: 307.274369, T: 104031520, Avg. loss: 398.475744\n",
      "Total training time: 29.95 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 597.39, NNZs: 98, Bias: 307.275804, T: 104240000, Avg. loss: 401.660191\n",
      "Total training time: 30.01 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 597.04, NNZs: 98, Bias: 307.285390, T: 104448480, Avg. loss: 398.133360\n",
      "Total training time: 30.06 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 596.73, NNZs: 98, Bias: 307.296489, T: 104656960, Avg. loss: 398.903314\n",
      "Total training time: 30.12 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 596.39, NNZs: 98, Bias: 307.301258, T: 104865440, Avg. loss: 395.960896\n",
      "Total training time: 30.19 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 596.06, NNZs: 98, Bias: 307.313449, T: 105073920, Avg. loss: 393.662494\n",
      "Total training time: 30.25 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 595.73, NNZs: 98, Bias: 307.318107, T: 105282400, Avg. loss: 394.377387\n",
      "Total training time: 30.31 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 595.41, NNZs: 98, Bias: 307.325698, T: 105490880, Avg. loss: 392.980558\n",
      "Total training time: 30.36 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 595.09, NNZs: 98, Bias: 307.327403, T: 105699360, Avg. loss: 390.559386\n",
      "Total training time: 30.42 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 594.74, NNZs: 98, Bias: 307.337604, T: 105907840, Avg. loss: 390.117790\n",
      "Total training time: 30.48 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 594.41, NNZs: 98, Bias: 307.343828, T: 106116320, Avg. loss: 391.909412\n",
      "Total training time: 30.54 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 594.09, NNZs: 98, Bias: 307.352396, T: 106324800, Avg. loss: 390.169306\n",
      "Total training time: 30.60 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 593.78, NNZs: 98, Bias: 307.363954, T: 106533280, Avg. loss: 390.596608\n",
      "Total training time: 30.66 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 593.44, NNZs: 98, Bias: 307.367993, T: 106741760, Avg. loss: 391.071316\n",
      "Total training time: 30.71 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 593.11, NNZs: 98, Bias: 307.374456, T: 106950240, Avg. loss: 388.806224\n",
      "Total training time: 30.77 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 592.81, NNZs: 98, Bias: 307.375201, T: 107158720, Avg. loss: 388.487384\n",
      "Total training time: 30.82 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 592.50, NNZs: 98, Bias: 307.383312, T: 107367200, Avg. loss: 389.086005\n",
      "Total training time: 30.88 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 592.17, NNZs: 98, Bias: 307.387221, T: 107575680, Avg. loss: 385.012631\n",
      "Total training time: 30.93 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 591.83, NNZs: 98, Bias: 307.399014, T: 107784160, Avg. loss: 384.923761\n",
      "Total training time: 31.00 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 591.52, NNZs: 98, Bias: 307.402995, T: 107992640, Avg. loss: 385.492772\n",
      "Total training time: 31.05 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 591.22, NNZs: 98, Bias: 307.409559, T: 108201120, Avg. loss: 385.129962\n",
      "Total training time: 31.11 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 590.91, NNZs: 98, Bias: 307.412420, T: 108409600, Avg. loss: 382.590532\n",
      "Total training time: 31.18 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 590.60, NNZs: 98, Bias: 307.423296, T: 108618080, Avg. loss: 380.626240\n",
      "Total training time: 31.26 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 590.26, NNZs: 98, Bias: 307.426602, T: 108826560, Avg. loss: 378.219565\n",
      "Total training time: 31.32 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 589.95, NNZs: 98, Bias: 307.433120, T: 109035040, Avg. loss: 381.799162\n",
      "Total training time: 31.37 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 589.62, NNZs: 98, Bias: 307.442565, T: 109243520, Avg. loss: 377.559012\n",
      "Total training time: 31.43 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 589.30, NNZs: 98, Bias: 307.448883, T: 109452000, Avg. loss: 380.349279\n",
      "Total training time: 31.50 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 588.99, NNZs: 98, Bias: 307.451616, T: 109660480, Avg. loss: 377.927086\n",
      "Total training time: 31.55 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 588.68, NNZs: 98, Bias: 307.456719, T: 109868960, Avg. loss: 377.726169\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 588.37, NNZs: 98, Bias: 307.460627, T: 110077440, Avg. loss: 376.608086\n",
      "Total training time: 31.67 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 588.06, NNZs: 98, Bias: 307.467253, T: 110285920, Avg. loss: 377.665492\n",
      "Total training time: 31.73 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 587.74, NNZs: 98, Bias: 307.470791, T: 110494400, Avg. loss: 372.697299\n",
      "Total training time: 31.78 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 587.44, NNZs: 98, Bias: 307.479836, T: 110702880, Avg. loss: 373.850698\n",
      "Total training time: 31.83 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 587.09, NNZs: 98, Bias: 307.487144, T: 110911360, Avg. loss: 371.745642\n",
      "Total training time: 31.89 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 586.79, NNZs: 98, Bias: 307.488952, T: 111119840, Avg. loss: 371.386918\n",
      "Total training time: 31.95 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 586.47, NNZs: 98, Bias: 307.499019, T: 111328320, Avg. loss: 374.801745\n",
      "Total training time: 32.01 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 586.16, NNZs: 98, Bias: 307.512388, T: 111536800, Avg. loss: 370.730853\n",
      "Total training time: 32.06 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 585.83, NNZs: 98, Bias: 307.515437, T: 111745280, Avg. loss: 369.683077\n",
      "Total training time: 32.12 seconds.\n",
      "-- Epoch 537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 585.50, NNZs: 98, Bias: 307.519465, T: 111953760, Avg. loss: 368.804666\n",
      "Total training time: 32.18 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 585.19, NNZs: 98, Bias: 307.525889, T: 112162240, Avg. loss: 370.921523\n",
      "Total training time: 32.24 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 584.89, NNZs: 98, Bias: 307.538095, T: 112370720, Avg. loss: 370.948840\n",
      "Total training time: 32.29 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 584.56, NNZs: 98, Bias: 307.545743, T: 112579200, Avg. loss: 368.261460\n",
      "Total training time: 32.37 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 584.24, NNZs: 98, Bias: 307.547693, T: 112787680, Avg. loss: 367.750045\n",
      "Total training time: 32.45 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 583.95, NNZs: 98, Bias: 307.550618, T: 112996160, Avg. loss: 367.452291\n",
      "Total training time: 32.50 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 583.65, NNZs: 98, Bias: 307.554160, T: 113204640, Avg. loss: 367.144723\n",
      "Total training time: 32.56 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 583.32, NNZs: 98, Bias: 307.565193, T: 113413120, Avg. loss: 365.430945\n",
      "Total training time: 32.61 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 583.00, NNZs: 98, Bias: 307.571715, T: 113621600, Avg. loss: 364.466491\n",
      "Total training time: 32.67 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 582.70, NNZs: 98, Bias: 307.585607, T: 113830080, Avg. loss: 362.588897\n",
      "Total training time: 32.72 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 582.40, NNZs: 98, Bias: 307.597642, T: 114038560, Avg. loss: 362.763412\n",
      "Total training time: 32.80 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 582.11, NNZs: 98, Bias: 307.594228, T: 114247040, Avg. loss: 358.909920\n",
      "Total training time: 32.85 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 581.79, NNZs: 98, Bias: 307.600525, T: 114455520, Avg. loss: 362.564284\n",
      "Total training time: 32.91 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 581.49, NNZs: 98, Bias: 307.596159, T: 114664000, Avg. loss: 360.318129\n",
      "Total training time: 32.97 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 581.20, NNZs: 98, Bias: 307.604353, T: 114872480, Avg. loss: 359.524490\n",
      "Total training time: 33.02 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 580.89, NNZs: 98, Bias: 307.620002, T: 115080960, Avg. loss: 361.070070\n",
      "Total training time: 33.07 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 580.56, NNZs: 98, Bias: 307.625295, T: 115289440, Avg. loss: 359.047478\n",
      "Total training time: 33.13 seconds.\n",
      "Convergence after 553 epochs took 33.13 seconds\n",
      "-- Epoch 1\n",
      "Norm: 832.57, NNZs: 98, Bias: 228.012952, T: 208480, Avg. loss: 3282025.038085\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 592.52, NNZs: 98, Bias: 238.126096, T: 416960, Avg. loss: 431402.040322\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 572.90, NNZs: 98, Bias: 245.985737, T: 625440, Avg. loss: 253395.003400\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 558.82, NNZs: 98, Bias: 249.102995, T: 833920, Avg. loss: 179082.913613\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 559.00, NNZs: 98, Bias: 250.638583, T: 1042400, Avg. loss: 140161.870205\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 534.81, NNZs: 98, Bias: 250.700726, T: 1250880, Avg. loss: 114590.101808\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 528.91, NNZs: 98, Bias: 251.360194, T: 1459360, Avg. loss: 96370.191230\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 528.43, NNZs: 98, Bias: 252.809605, T: 1667840, Avg. loss: 84088.793106\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 531.15, NNZs: 98, Bias: 253.621625, T: 1876320, Avg. loss: 73732.169647\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 523.01, NNZs: 98, Bias: 253.858758, T: 2084800, Avg. loss: 65796.460422\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 522.43, NNZs: 98, Bias: 254.237164, T: 2293280, Avg. loss: 59936.226628\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 520.04, NNZs: 98, Bias: 254.858161, T: 2501760, Avg. loss: 54223.759816\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 517.47, NNZs: 98, Bias: 254.834979, T: 2710240, Avg. loss: 49702.509128\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 517.17, NNZs: 98, Bias: 254.994873, T: 2918720, Avg. loss: 46096.730805\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 515.23, NNZs: 98, Bias: 255.235258, T: 3127200, Avg. loss: 43441.955139\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 514.19, NNZs: 98, Bias: 254.889335, T: 3335680, Avg. loss: 40355.596749\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 513.45, NNZs: 98, Bias: 254.830364, T: 3544160, Avg. loss: 37969.050213\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 513.50, NNZs: 98, Bias: 254.956350, T: 3752640, Avg. loss: 35791.341186\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 513.63, NNZs: 98, Bias: 254.971018, T: 3961120, Avg. loss: 34019.074479\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 512.58, NNZs: 98, Bias: 254.599176, T: 4169600, Avg. loss: 32220.329994\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 512.12, NNZs: 98, Bias: 254.806679, T: 4378080, Avg. loss: 30459.839031\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 511.50, NNZs: 98, Bias: 254.489653, T: 4586560, Avg. loss: 29173.420256\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 511.02, NNZs: 98, Bias: 254.481361, T: 4795040, Avg. loss: 27933.073830\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 510.75, NNZs: 98, Bias: 254.328974, T: 5003520, Avg. loss: 26684.853171\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 510.25, NNZs: 98, Bias: 254.067400, T: 5212000, Avg. loss: 25581.062608\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 509.81, NNZs: 98, Bias: 254.139843, T: 5420480, Avg. loss: 24576.413154\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 509.60, NNZs: 98, Bias: 254.154790, T: 5628960, Avg. loss: 23566.782004\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 509.52, NNZs: 98, Bias: 254.083721, T: 5837440, Avg. loss: 22841.701430\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 509.46, NNZs: 98, Bias: 254.296023, T: 6045920, Avg. loss: 21823.340209\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 508.75, NNZs: 98, Bias: 254.166170, T: 6254400, Avg. loss: 21172.417772\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 508.31, NNZs: 98, Bias: 254.318776, T: 6462880, Avg. loss: 20397.182005\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 508.32, NNZs: 98, Bias: 254.189833, T: 6671360, Avg. loss: 19862.188886\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 508.15, NNZs: 98, Bias: 254.207249, T: 6879840, Avg. loss: 19205.094992\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 507.96, NNZs: 98, Bias: 254.217118, T: 7088320, Avg. loss: 18657.525901\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 507.67, NNZs: 98, Bias: 254.303415, T: 7296800, Avg. loss: 18157.496854\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 507.33, NNZs: 98, Bias: 254.158573, T: 7505280, Avg. loss: 17580.816532\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 506.99, NNZs: 98, Bias: 253.873782, T: 7713760, Avg. loss: 17096.431947\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 506.68, NNZs: 98, Bias: 253.950618, T: 7922240, Avg. loss: 16672.074602\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 506.65, NNZs: 98, Bias: 253.827471, T: 8130720, Avg. loss: 16306.315277\n",
      "Total training time: 2.87 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 506.52, NNZs: 98, Bias: 253.856082, T: 8339200, Avg. loss: 15784.383405\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 506.13, NNZs: 98, Bias: 253.871673, T: 8547680, Avg. loss: 15492.065505\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 505.90, NNZs: 98, Bias: 253.624276, T: 8756160, Avg. loss: 15102.880512\n",
      "Total training time: 3.08 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 505.49, NNZs: 98, Bias: 253.322801, T: 8964640, Avg. loss: 14727.534262\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 505.05, NNZs: 98, Bias: 253.314087, T: 9173120, Avg. loss: 14318.927618\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 505.06, NNZs: 98, Bias: 253.179037, T: 9381600, Avg. loss: 14068.581339\n",
      "Total training time: 3.29 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 504.67, NNZs: 98, Bias: 252.925416, T: 9590080, Avg. loss: 13667.096636\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 504.38, NNZs: 98, Bias: 252.727209, T: 9798560, Avg. loss: 13453.807586\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 503.90, NNZs: 98, Bias: 252.570213, T: 10007040, Avg. loss: 13120.861625\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 503.95, NNZs: 98, Bias: 252.578615, T: 10215520, Avg. loss: 12973.318775\n",
      "Total training time: 3.57 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 503.73, NNZs: 98, Bias: 252.461341, T: 10424000, Avg. loss: 12717.766694\n",
      "Total training time: 3.64 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 503.39, NNZs: 98, Bias: 252.316059, T: 10632480, Avg. loss: 12308.823331\n",
      "Total training time: 3.72 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 503.19, NNZs: 98, Bias: 252.272309, T: 10840960, Avg. loss: 12127.393298\n",
      "Total training time: 3.78 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 502.70, NNZs: 98, Bias: 252.139131, T: 11049440, Avg. loss: 11928.427166\n",
      "Total training time: 3.85 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 502.58, NNZs: 98, Bias: 252.011195, T: 11257920, Avg. loss: 11731.683492\n",
      "Total training time: 3.92 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 502.16, NNZs: 98, Bias: 252.072373, T: 11466400, Avg. loss: 11492.173608\n",
      "Total training time: 3.99 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 501.88, NNZs: 98, Bias: 251.895535, T: 11674880, Avg. loss: 11259.944533\n",
      "Total training time: 4.06 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 501.80, NNZs: 98, Bias: 251.923944, T: 11883360, Avg. loss: 11050.747514\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 501.48, NNZs: 98, Bias: 251.765576, T: 12091840, Avg. loss: 10875.846442\n",
      "Total training time: 4.20 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 501.22, NNZs: 98, Bias: 251.633427, T: 12300320, Avg. loss: 10706.296064\n",
      "Total training time: 4.27 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 501.00, NNZs: 98, Bias: 251.535280, T: 12508800, Avg. loss: 10502.320603\n",
      "Total training time: 4.34 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 500.59, NNZs: 98, Bias: 251.309468, T: 12717280, Avg. loss: 10351.524152\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 500.23, NNZs: 98, Bias: 251.159746, T: 12925760, Avg. loss: 10143.507973\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 500.24, NNZs: 98, Bias: 251.078053, T: 13134240, Avg. loss: 10017.911699\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 499.93, NNZs: 98, Bias: 250.929103, T: 13342720, Avg. loss: 9848.249710\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 499.75, NNZs: 98, Bias: 250.820571, T: 13551200, Avg. loss: 9731.380555\n",
      "Total training time: 4.69 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 499.51, NNZs: 98, Bias: 250.752598, T: 13759680, Avg. loss: 9528.516312\n",
      "Total training time: 4.76 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 499.16, NNZs: 98, Bias: 250.662990, T: 13968160, Avg. loss: 9384.726364\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 498.87, NNZs: 98, Bias: 250.432766, T: 14176640, Avg. loss: 9214.731427\n",
      "Total training time: 4.89 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 498.61, NNZs: 98, Bias: 250.324459, T: 14385120, Avg. loss: 9125.795294\n",
      "Total training time: 4.95 seconds.\n",
      "-- Epoch 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 498.37, NNZs: 98, Bias: 250.260834, T: 14593600, Avg. loss: 8969.709558\n",
      "Total training time: 5.03 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 498.07, NNZs: 98, Bias: 250.151132, T: 14802080, Avg. loss: 8866.379643\n",
      "Total training time: 5.09 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 497.87, NNZs: 98, Bias: 250.053572, T: 15010560, Avg. loss: 8731.471166\n",
      "Total training time: 5.15 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 497.67, NNZs: 98, Bias: 249.900003, T: 15219040, Avg. loss: 8618.816039\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 497.39, NNZs: 98, Bias: 249.650627, T: 15427520, Avg. loss: 8487.678330\n",
      "Total training time: 5.30 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 497.16, NNZs: 98, Bias: 249.572123, T: 15636000, Avg. loss: 8395.266045\n",
      "Total training time: 5.37 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 496.98, NNZs: 98, Bias: 249.376640, T: 15844480, Avg. loss: 8275.865013\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 496.75, NNZs: 98, Bias: 249.318788, T: 16052960, Avg. loss: 8165.218687\n",
      "Total training time: 5.51 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 496.48, NNZs: 98, Bias: 249.159932, T: 16261440, Avg. loss: 8053.983043\n",
      "Total training time: 5.58 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 496.22, NNZs: 98, Bias: 249.113272, T: 16469920, Avg. loss: 7953.568236\n",
      "Total training time: 5.66 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 496.02, NNZs: 98, Bias: 248.982760, T: 16678400, Avg. loss: 7849.566661\n",
      "Total training time: 5.72 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 495.86, NNZs: 98, Bias: 248.906840, T: 16886880, Avg. loss: 7715.817006\n",
      "Total training time: 5.80 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 495.70, NNZs: 98, Bias: 248.863880, T: 17095360, Avg. loss: 7655.822766\n",
      "Total training time: 5.86 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 495.40, NNZs: 98, Bias: 248.711757, T: 17303840, Avg. loss: 7563.001825\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 495.33, NNZs: 98, Bias: 248.586635, T: 17512320, Avg. loss: 7479.712814\n",
      "Total training time: 6.00 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 495.06, NNZs: 98, Bias: 248.362088, T: 17720800, Avg. loss: 7367.511661\n",
      "Total training time: 6.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 494.80, NNZs: 98, Bias: 248.221742, T: 17929280, Avg. loss: 7268.564381\n",
      "Total training time: 6.13 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 494.55, NNZs: 98, Bias: 248.116691, T: 18137760, Avg. loss: 7200.037469\n",
      "Total training time: 6.20 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 494.34, NNZs: 98, Bias: 247.987916, T: 18346240, Avg. loss: 7125.664104\n",
      "Total training time: 6.27 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 494.17, NNZs: 98, Bias: 247.916646, T: 18554720, Avg. loss: 7043.967293\n",
      "Total training time: 6.34 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 493.96, NNZs: 98, Bias: 247.797227, T: 18763200, Avg. loss: 6933.272314\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 493.75, NNZs: 98, Bias: 247.686408, T: 18971680, Avg. loss: 6896.949688\n",
      "Total training time: 6.48 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 493.53, NNZs: 98, Bias: 247.549383, T: 19180160, Avg. loss: 6806.268947\n",
      "Total training time: 6.55 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 493.27, NNZs: 98, Bias: 247.493051, T: 19388640, Avg. loss: 6726.064898\n",
      "Total training time: 6.62 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 493.02, NNZs: 98, Bias: 247.420420, T: 19597120, Avg. loss: 6655.663808\n",
      "Total training time: 6.69 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 492.87, NNZs: 98, Bias: 247.300469, T: 19805600, Avg. loss: 6622.584161\n",
      "Total training time: 6.76 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 492.69, NNZs: 98, Bias: 247.130077, T: 20014080, Avg. loss: 6529.024619\n",
      "Total training time: 6.83 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 492.47, NNZs: 98, Bias: 247.022096, T: 20222560, Avg. loss: 6451.253499\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 492.33, NNZs: 98, Bias: 246.875533, T: 20431040, Avg. loss: 6398.231017\n",
      "Total training time: 6.97 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 492.08, NNZs: 98, Bias: 246.748134, T: 20639520, Avg. loss: 6320.689984\n",
      "Total training time: 7.07 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 491.85, NNZs: 98, Bias: 246.574433, T: 20848000, Avg. loss: 6280.779524\n",
      "Total training time: 7.15 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 491.54, NNZs: 98, Bias: 246.459828, T: 21056480, Avg. loss: 6155.722236\n",
      "Total training time: 7.24 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 491.26, NNZs: 98, Bias: 246.326139, T: 21264960, Avg. loss: 6147.257971\n",
      "Total training time: 7.31 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 491.05, NNZs: 98, Bias: 246.159112, T: 21473440, Avg. loss: 6097.419924\n",
      "Total training time: 7.38 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 490.81, NNZs: 98, Bias: 246.038103, T: 21681920, Avg. loss: 6003.111827\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 490.64, NNZs: 98, Bias: 245.945118, T: 21890400, Avg. loss: 5939.170129\n",
      "Total training time: 7.52 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 490.49, NNZs: 98, Bias: 245.753480, T: 22098880, Avg. loss: 5941.953713\n",
      "Total training time: 7.58 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 490.26, NNZs: 98, Bias: 245.590048, T: 22307360, Avg. loss: 5835.548230\n",
      "Total training time: 7.66 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 490.04, NNZs: 98, Bias: 245.507921, T: 22515840, Avg. loss: 5796.480120\n",
      "Total training time: 7.73 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 489.77, NNZs: 98, Bias: 245.413705, T: 22724320, Avg. loss: 5735.229114\n",
      "Total training time: 7.80 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 489.56, NNZs: 98, Bias: 245.336066, T: 22932800, Avg. loss: 5669.134763\n",
      "Total training time: 7.87 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 489.33, NNZs: 98, Bias: 245.227096, T: 23141280, Avg. loss: 5629.329316\n",
      "Total training time: 7.93 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 489.13, NNZs: 98, Bias: 245.135792, T: 23349760, Avg. loss: 5581.332134\n",
      "Total training time: 8.00 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 488.91, NNZs: 98, Bias: 245.052948, T: 23558240, Avg. loss: 5526.643970\n",
      "Total training time: 8.07 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 488.67, NNZs: 98, Bias: 244.878442, T: 23766720, Avg. loss: 5474.475862\n",
      "Total training time: 8.14 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 488.48, NNZs: 98, Bias: 244.755736, T: 23975200, Avg. loss: 5449.318442\n",
      "Total training time: 8.22 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 488.29, NNZs: 98, Bias: 244.591827, T: 24183680, Avg. loss: 5384.936222\n",
      "Total training time: 8.29 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 488.03, NNZs: 98, Bias: 244.485780, T: 24392160, Avg. loss: 5325.918203\n",
      "Total training time: 8.36 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 487.84, NNZs: 98, Bias: 244.357781, T: 24600640, Avg. loss: 5299.495225\n",
      "Total training time: 8.43 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 487.60, NNZs: 98, Bias: 244.218227, T: 24809120, Avg. loss: 5242.918900\n",
      "Total training time: 8.51 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 487.39, NNZs: 98, Bias: 244.120368, T: 25017600, Avg. loss: 5219.005230\n",
      "Total training time: 8.58 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 487.16, NNZs: 98, Bias: 243.976741, T: 25226080, Avg. loss: 5167.936124\n",
      "Total training time: 8.65 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 486.95, NNZs: 98, Bias: 243.828985, T: 25434560, Avg. loss: 5097.224642\n",
      "Total training time: 8.72 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 486.74, NNZs: 98, Bias: 243.687831, T: 25643040, Avg. loss: 5111.178071\n",
      "Total training time: 8.79 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 486.54, NNZs: 98, Bias: 243.524225, T: 25851520, Avg. loss: 5060.183928\n",
      "Total training time: 8.86 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 486.32, NNZs: 98, Bias: 243.468964, T: 26060000, Avg. loss: 5015.302840\n",
      "Total training time: 8.92 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 486.12, NNZs: 98, Bias: 243.388671, T: 26268480, Avg. loss: 4972.060237\n",
      "Total training time: 8.99 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 485.91, NNZs: 98, Bias: 243.231707, T: 26476960, Avg. loss: 4938.056280\n",
      "Total training time: 9.06 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 485.72, NNZs: 98, Bias: 243.138081, T: 26685440, Avg. loss: 4878.913664\n",
      "Total training time: 9.14 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 485.58, NNZs: 98, Bias: 243.048860, T: 26893920, Avg. loss: 4876.523077\n",
      "Total training time: 9.22 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 485.35, NNZs: 98, Bias: 242.970460, T: 27102400, Avg. loss: 4806.853385\n",
      "Total training time: 9.30 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 485.17, NNZs: 98, Bias: 242.846845, T: 27310880, Avg. loss: 4773.213741\n",
      "Total training time: 9.38 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 485.01, NNZs: 98, Bias: 242.705302, T: 27519360, Avg. loss: 4731.557984\n",
      "Total training time: 9.46 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 484.76, NNZs: 98, Bias: 242.539500, T: 27727840, Avg. loss: 4694.481967\n",
      "Total training time: 9.54 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 484.56, NNZs: 98, Bias: 242.424411, T: 27936320, Avg. loss: 4649.809569\n",
      "Total training time: 9.62 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 484.35, NNZs: 98, Bias: 242.270749, T: 28144800, Avg. loss: 4625.650114\n",
      "Total training time: 9.70 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 484.18, NNZs: 98, Bias: 242.131722, T: 28353280, Avg. loss: 4606.464196\n",
      "Total training time: 9.77 seconds.\n",
      "-- Epoch 137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 483.99, NNZs: 98, Bias: 242.085335, T: 28561760, Avg. loss: 4555.121781\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 483.83, NNZs: 98, Bias: 241.944264, T: 28770240, Avg. loss: 4519.021176\n",
      "Total training time: 9.91 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 483.60, NNZs: 98, Bias: 241.753467, T: 28978720, Avg. loss: 4467.630950\n",
      "Total training time: 9.98 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 483.43, NNZs: 98, Bias: 241.619457, T: 29187200, Avg. loss: 4468.854648\n",
      "Total training time: 10.05 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 483.24, NNZs: 98, Bias: 241.487255, T: 29395680, Avg. loss: 4444.647769\n",
      "Total training time: 10.13 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 483.08, NNZs: 98, Bias: 241.370629, T: 29604160, Avg. loss: 4398.762212\n",
      "Total training time: 10.20 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 482.85, NNZs: 98, Bias: 241.304990, T: 29812640, Avg. loss: 4356.004996\n",
      "Total training time: 10.28 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 482.64, NNZs: 98, Bias: 241.226598, T: 30021120, Avg. loss: 4365.342896\n",
      "Total training time: 10.35 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 482.41, NNZs: 98, Bias: 241.091937, T: 30229600, Avg. loss: 4313.012471\n",
      "Total training time: 10.41 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 482.27, NNZs: 98, Bias: 241.024863, T: 30438080, Avg. loss: 4272.099553\n",
      "Total training time: 10.48 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 482.03, NNZs: 98, Bias: 240.957658, T: 30646560, Avg. loss: 4254.451182\n",
      "Total training time: 10.56 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 481.87, NNZs: 98, Bias: 240.856595, T: 30855040, Avg. loss: 4214.004505\n",
      "Total training time: 10.63 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 481.67, NNZs: 98, Bias: 240.766399, T: 31063520, Avg. loss: 4200.833661\n",
      "Total training time: 10.70 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 481.45, NNZs: 98, Bias: 240.660512, T: 31272000, Avg. loss: 4153.196591\n",
      "Total training time: 10.76 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 481.33, NNZs: 98, Bias: 240.530697, T: 31480480, Avg. loss: 4141.681607\n",
      "Total training time: 10.83 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 481.17, NNZs: 98, Bias: 240.439842, T: 31688960, Avg. loss: 4088.040337\n",
      "Total training time: 10.90 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 480.96, NNZs: 98, Bias: 240.305771, T: 31897440, Avg. loss: 4074.942072\n",
      "Total training time: 10.97 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 480.74, NNZs: 98, Bias: 240.197006, T: 32105920, Avg. loss: 4052.766699\n",
      "Total training time: 11.04 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 480.56, NNZs: 98, Bias: 240.072535, T: 32314400, Avg. loss: 4045.183837\n",
      "Total training time: 11.10 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 480.39, NNZs: 98, Bias: 239.982800, T: 32522880, Avg. loss: 4002.193751\n",
      "Total training time: 11.18 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 480.24, NNZs: 98, Bias: 239.790624, T: 32731360, Avg. loss: 3954.619040\n",
      "Total training time: 11.27 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 480.12, NNZs: 98, Bias: 239.707421, T: 32939840, Avg. loss: 3953.924262\n",
      "Total training time: 11.36 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 479.96, NNZs: 98, Bias: 239.626273, T: 33148320, Avg. loss: 3915.527661\n",
      "Total training time: 11.42 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 479.78, NNZs: 98, Bias: 239.493343, T: 33356800, Avg. loss: 3889.805238\n",
      "Total training time: 11.49 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 479.59, NNZs: 98, Bias: 239.360305, T: 33565280, Avg. loss: 3851.747421\n",
      "Total training time: 11.56 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 479.43, NNZs: 98, Bias: 239.287332, T: 33773760, Avg. loss: 3872.396307\n",
      "Total training time: 11.63 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 479.26, NNZs: 98, Bias: 239.159856, T: 33982240, Avg. loss: 3821.892393\n",
      "Total training time: 11.72 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 479.04, NNZs: 98, Bias: 239.060420, T: 34190720, Avg. loss: 3789.466280\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 478.86, NNZs: 98, Bias: 238.999521, T: 34399200, Avg. loss: 3775.060470\n",
      "Total training time: 11.90 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 478.71, NNZs: 98, Bias: 238.870232, T: 34607680, Avg. loss: 3739.190826\n",
      "Total training time: 11.99 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 478.56, NNZs: 98, Bias: 238.770284, T: 34816160, Avg. loss: 3717.630832\n",
      "Total training time: 12.08 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 478.39, NNZs: 98, Bias: 238.636087, T: 35024640, Avg. loss: 3705.184905\n",
      "Total training time: 12.16 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 478.18, NNZs: 98, Bias: 238.468626, T: 35233120, Avg. loss: 3692.475252\n",
      "Total training time: 12.24 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 477.96, NNZs: 98, Bias: 238.305839, T: 35441600, Avg. loss: 3651.322479\n",
      "Total training time: 12.34 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 477.75, NNZs: 98, Bias: 238.234919, T: 35650080, Avg. loss: 3645.009299\n",
      "Total training time: 12.44 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 477.59, NNZs: 98, Bias: 238.135046, T: 35858560, Avg. loss: 3623.114530\n",
      "Total training time: 12.53 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 477.43, NNZs: 98, Bias: 238.028303, T: 36067040, Avg. loss: 3605.060112\n",
      "Total training time: 12.61 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 477.21, NNZs: 98, Bias: 237.912724, T: 36275520, Avg. loss: 3588.629965\n",
      "Total training time: 12.68 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 476.99, NNZs: 98, Bias: 237.845701, T: 36484000, Avg. loss: 3554.396703\n",
      "Total training time: 12.75 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 476.80, NNZs: 98, Bias: 237.719378, T: 36692480, Avg. loss: 3526.847394\n",
      "Total training time: 12.82 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 476.57, NNZs: 98, Bias: 237.587007, T: 36900960, Avg. loss: 3513.696119\n",
      "Total training time: 12.89 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 476.38, NNZs: 98, Bias: 237.476271, T: 37109440, Avg. loss: 3498.846228\n",
      "Total training time: 12.99 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 476.20, NNZs: 98, Bias: 237.349367, T: 37317920, Avg. loss: 3488.023956\n",
      "Total training time: 13.08 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 476.03, NNZs: 98, Bias: 237.259766, T: 37526400, Avg. loss: 3460.111734\n",
      "Total training time: 13.14 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 475.84, NNZs: 98, Bias: 237.145978, T: 37734880, Avg. loss: 3432.901785\n",
      "Total training time: 13.21 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 475.68, NNZs: 98, Bias: 237.013009, T: 37943360, Avg. loss: 3410.224400\n",
      "Total training time: 13.29 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 475.55, NNZs: 98, Bias: 236.858535, T: 38151840, Avg. loss: 3415.662893\n",
      "Total training time: 13.36 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 475.41, NNZs: 98, Bias: 236.763940, T: 38360320, Avg. loss: 3378.830102\n",
      "Total training time: 13.43 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 475.23, NNZs: 98, Bias: 236.654276, T: 38568800, Avg. loss: 3368.731299\n",
      "Total training time: 13.50 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 475.05, NNZs: 98, Bias: 236.587838, T: 38777280, Avg. loss: 3349.797621\n",
      "Total training time: 13.57 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 474.90, NNZs: 98, Bias: 236.476168, T: 38985760, Avg. loss: 3345.381687\n",
      "Total training time: 13.64 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 474.74, NNZs: 98, Bias: 236.373685, T: 39194240, Avg. loss: 3312.948028\n",
      "Total training time: 13.71 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 474.55, NNZs: 98, Bias: 236.264975, T: 39402720, Avg. loss: 3286.946417\n",
      "Total training time: 13.78 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 474.36, NNZs: 98, Bias: 236.213369, T: 39611200, Avg. loss: 3281.100668\n",
      "Total training time: 13.85 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 474.21, NNZs: 98, Bias: 236.114886, T: 39819680, Avg. loss: 3262.814924\n",
      "Total training time: 13.92 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 474.03, NNZs: 98, Bias: 236.020819, T: 40028160, Avg. loss: 3230.044905\n",
      "Total training time: 13.99 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 473.88, NNZs: 98, Bias: 235.916681, T: 40236640, Avg. loss: 3215.445803\n",
      "Total training time: 14.06 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 473.71, NNZs: 98, Bias: 235.803126, T: 40445120, Avg. loss: 3229.582654\n",
      "Total training time: 14.13 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 473.55, NNZs: 98, Bias: 235.701527, T: 40653600, Avg. loss: 3190.637175\n",
      "Total training time: 14.20 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 473.34, NNZs: 98, Bias: 235.613640, T: 40862080, Avg. loss: 3177.441629\n",
      "Total training time: 14.27 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 473.18, NNZs: 98, Bias: 235.510084, T: 41070560, Avg. loss: 3155.941878\n",
      "Total training time: 14.33 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 472.99, NNZs: 98, Bias: 235.384757, T: 41279040, Avg. loss: 3149.902120\n",
      "Total training time: 14.41 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 472.84, NNZs: 98, Bias: 235.287629, T: 41487520, Avg. loss: 3129.543987\n",
      "Total training time: 14.47 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 472.66, NNZs: 98, Bias: 235.161877, T: 41696000, Avg. loss: 3108.365707\n",
      "Total training time: 14.54 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 472.47, NNZs: 98, Bias: 235.064218, T: 41904480, Avg. loss: 3105.374888\n",
      "Total training time: 14.61 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 472.29, NNZs: 98, Bias: 234.928032, T: 42112960, Avg. loss: 3080.371577\n",
      "Total training time: 14.68 seconds.\n",
      "-- Epoch 203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 472.13, NNZs: 98, Bias: 234.802753, T: 42321440, Avg. loss: 3070.490852\n",
      "Total training time: 14.75 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 471.94, NNZs: 98, Bias: 234.719989, T: 42529920, Avg. loss: 3049.249947\n",
      "Total training time: 14.82 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 471.76, NNZs: 98, Bias: 234.624760, T: 42738400, Avg. loss: 3030.708198\n",
      "Total training time: 14.89 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 471.57, NNZs: 98, Bias: 234.505494, T: 42946880, Avg. loss: 3018.597442\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 471.43, NNZs: 98, Bias: 234.360134, T: 43155360, Avg. loss: 3004.512559\n",
      "Total training time: 15.03 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 471.26, NNZs: 98, Bias: 234.268163, T: 43363840, Avg. loss: 2999.764272\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 471.10, NNZs: 98, Bias: 234.157063, T: 43572320, Avg. loss: 2972.091147\n",
      "Total training time: 15.17 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 470.99, NNZs: 98, Bias: 234.065229, T: 43780800, Avg. loss: 2969.123163\n",
      "Total training time: 15.24 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 470.81, NNZs: 98, Bias: 233.978416, T: 43989280, Avg. loss: 2951.044854\n",
      "Total training time: 15.31 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 470.66, NNZs: 98, Bias: 233.915410, T: 44197760, Avg. loss: 2928.558160\n",
      "Total training time: 15.38 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 470.49, NNZs: 98, Bias: 233.791983, T: 44406240, Avg. loss: 2926.112186\n",
      "Total training time: 15.45 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 470.31, NNZs: 98, Bias: 233.725279, T: 44614720, Avg. loss: 2903.606422\n",
      "Total training time: 15.52 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 470.12, NNZs: 98, Bias: 233.653723, T: 44823200, Avg. loss: 2896.527611\n",
      "Total training time: 15.59 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 469.99, NNZs: 98, Bias: 233.576485, T: 45031680, Avg. loss: 2893.427141\n",
      "Total training time: 15.66 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 469.80, NNZs: 98, Bias: 233.478772, T: 45240160, Avg. loss: 2872.791775\n",
      "Total training time: 15.73 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 469.65, NNZs: 98, Bias: 233.342209, T: 45448640, Avg. loss: 2847.003664\n",
      "Total training time: 15.80 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 469.49, NNZs: 98, Bias: 233.262592, T: 45657120, Avg. loss: 2847.517098\n",
      "Total training time: 15.87 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 469.35, NNZs: 98, Bias: 233.186966, T: 45865600, Avg. loss: 2818.295334\n",
      "Total training time: 15.94 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 469.17, NNZs: 98, Bias: 233.072529, T: 46074080, Avg. loss: 2798.866274\n",
      "Total training time: 16.01 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 469.04, NNZs: 98, Bias: 232.956921, T: 46282560, Avg. loss: 2806.866588\n",
      "Total training time: 16.08 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 468.90, NNZs: 98, Bias: 232.847140, T: 46491040, Avg. loss: 2788.969913\n",
      "Total training time: 16.15 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 468.71, NNZs: 98, Bias: 232.755951, T: 46699520, Avg. loss: 2783.073955\n",
      "Total training time: 16.21 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 468.54, NNZs: 98, Bias: 232.676199, T: 46908000, Avg. loss: 2751.796029\n",
      "Total training time: 16.29 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 468.39, NNZs: 98, Bias: 232.596666, T: 47116480, Avg. loss: 2761.450194\n",
      "Total training time: 16.35 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 468.23, NNZs: 98, Bias: 232.515931, T: 47324960, Avg. loss: 2734.573947\n",
      "Total training time: 16.42 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 468.09, NNZs: 98, Bias: 232.432858, T: 47533440, Avg. loss: 2723.627322\n",
      "Total training time: 16.49 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 467.92, NNZs: 98, Bias: 232.317603, T: 47741920, Avg. loss: 2715.356319\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 467.77, NNZs: 98, Bias: 232.228375, T: 47950400, Avg. loss: 2706.937706\n",
      "Total training time: 16.63 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 467.59, NNZs: 98, Bias: 232.121238, T: 48158880, Avg. loss: 2699.087475\n",
      "Total training time: 16.70 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 467.45, NNZs: 98, Bias: 232.043794, T: 48367360, Avg. loss: 2684.323888\n",
      "Total training time: 16.77 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 467.25, NNZs: 98, Bias: 231.965390, T: 48575840, Avg. loss: 2657.206564\n",
      "Total training time: 16.84 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 467.10, NNZs: 98, Bias: 231.893114, T: 48784320, Avg. loss: 2646.598328\n",
      "Total training time: 16.91 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 466.91, NNZs: 98, Bias: 231.814784, T: 48992800, Avg. loss: 2645.561107\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 466.73, NNZs: 98, Bias: 231.723560, T: 49201280, Avg. loss: 2636.536205\n",
      "Total training time: 17.05 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 466.58, NNZs: 98, Bias: 231.630672, T: 49409760, Avg. loss: 2610.779574\n",
      "Total training time: 17.12 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 466.41, NNZs: 98, Bias: 231.555558, T: 49618240, Avg. loss: 2603.041045\n",
      "Total training time: 17.19 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 466.27, NNZs: 98, Bias: 231.434449, T: 49826720, Avg. loss: 2616.730502\n",
      "Total training time: 17.26 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 466.11, NNZs: 98, Bias: 231.320128, T: 50035200, Avg. loss: 2577.559788\n",
      "Total training time: 17.33 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 465.95, NNZs: 98, Bias: 231.198270, T: 50243680, Avg. loss: 2574.893158\n",
      "Total training time: 17.40 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 465.81, NNZs: 98, Bias: 231.112871, T: 50452160, Avg. loss: 2565.475428\n",
      "Total training time: 17.46 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 465.65, NNZs: 98, Bias: 230.985481, T: 50660640, Avg. loss: 2553.585018\n",
      "Total training time: 17.53 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 465.52, NNZs: 98, Bias: 230.860755, T: 50869120, Avg. loss: 2543.859354\n",
      "Total training time: 17.60 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 465.40, NNZs: 98, Bias: 230.756144, T: 51077600, Avg. loss: 2539.337065\n",
      "Total training time: 17.67 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 465.23, NNZs: 98, Bias: 230.690520, T: 51286080, Avg. loss: 2515.923349\n",
      "Total training time: 17.74 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 465.06, NNZs: 98, Bias: 230.572423, T: 51494560, Avg. loss: 2493.811386\n",
      "Total training time: 17.81 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 464.96, NNZs: 98, Bias: 230.480932, T: 51703040, Avg. loss: 2503.959185\n",
      "Total training time: 17.90 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 464.82, NNZs: 98, Bias: 230.369746, T: 51911520, Avg. loss: 2493.615017\n",
      "Total training time: 17.99 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 464.66, NNZs: 98, Bias: 230.297065, T: 52120000, Avg. loss: 2486.538142\n",
      "Total training time: 18.05 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 464.50, NNZs: 98, Bias: 230.231037, T: 52328480, Avg. loss: 2476.739092\n",
      "Total training time: 18.12 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 464.35, NNZs: 98, Bias: 230.130164, T: 52536960, Avg. loss: 2468.288682\n",
      "Total training time: 18.19 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 464.16, NNZs: 98, Bias: 230.062964, T: 52745440, Avg. loss: 2445.909168\n",
      "Total training time: 18.27 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 464.02, NNZs: 98, Bias: 229.984440, T: 52953920, Avg. loss: 2449.409580\n",
      "Total training time: 18.36 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 463.87, NNZs: 98, Bias: 229.875980, T: 53162400, Avg. loss: 2436.539722\n",
      "Total training time: 18.45 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 463.70, NNZs: 98, Bias: 229.779132, T: 53370880, Avg. loss: 2408.649629\n",
      "Total training time: 18.52 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 463.55, NNZs: 98, Bias: 229.675322, T: 53579360, Avg. loss: 2400.672330\n",
      "Total training time: 18.59 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 463.42, NNZs: 98, Bias: 229.602833, T: 53787840, Avg. loss: 2414.426076\n",
      "Total training time: 18.67 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 463.27, NNZs: 98, Bias: 229.507268, T: 53996320, Avg. loss: 2405.221852\n",
      "Total training time: 18.74 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 463.11, NNZs: 98, Bias: 229.425020, T: 54204800, Avg. loss: 2392.698693\n",
      "Total training time: 18.81 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 462.97, NNZs: 98, Bias: 229.355959, T: 54413280, Avg. loss: 2385.375502\n",
      "Total training time: 18.91 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 462.81, NNZs: 98, Bias: 229.281463, T: 54621760, Avg. loss: 2368.584920\n",
      "Total training time: 19.00 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 462.68, NNZs: 98, Bias: 229.149016, T: 54830240, Avg. loss: 2360.097679\n",
      "Total training time: 19.07 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 462.51, NNZs: 98, Bias: 229.050117, T: 55038720, Avg. loss: 2332.829983\n",
      "Total training time: 19.13 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 462.37, NNZs: 98, Bias: 228.957265, T: 55247200, Avg. loss: 2343.587470\n",
      "Total training time: 19.20 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 462.23, NNZs: 98, Bias: 228.855190, T: 55455680, Avg. loss: 2335.571565\n",
      "Total training time: 19.27 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 462.09, NNZs: 98, Bias: 228.730593, T: 55664160, Avg. loss: 2324.122368\n",
      "Total training time: 19.35 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 461.95, NNZs: 98, Bias: 228.662460, T: 55872640, Avg. loss: 2312.771726\n",
      "Total training time: 19.42 seconds.\n",
      "-- Epoch 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 461.83, NNZs: 98, Bias: 228.556013, T: 56081120, Avg. loss: 2315.652852\n",
      "Total training time: 19.49 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 461.63, NNZs: 98, Bias: 228.479842, T: 56289600, Avg. loss: 2300.882520\n",
      "Total training time: 19.56 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 461.51, NNZs: 98, Bias: 228.400770, T: 56498080, Avg. loss: 2287.991296\n",
      "Total training time: 19.64 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 461.35, NNZs: 98, Bias: 228.300825, T: 56706560, Avg. loss: 2277.557116\n",
      "Total training time: 19.72 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 461.19, NNZs: 98, Bias: 228.223898, T: 56915040, Avg. loss: 2258.669338\n",
      "Total training time: 19.81 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 461.05, NNZs: 98, Bias: 228.176354, T: 57123520, Avg. loss: 2267.273174\n",
      "Total training time: 19.88 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 460.91, NNZs: 98, Bias: 228.075373, T: 57332000, Avg. loss: 2252.922474\n",
      "Total training time: 19.96 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 460.75, NNZs: 98, Bias: 228.016156, T: 57540480, Avg. loss: 2242.785009\n",
      "Total training time: 20.02 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 460.62, NNZs: 98, Bias: 227.931863, T: 57748960, Avg. loss: 2235.268578\n",
      "Total training time: 20.09 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 460.47, NNZs: 98, Bias: 227.859757, T: 57957440, Avg. loss: 2226.747032\n",
      "Total training time: 20.16 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 460.31, NNZs: 98, Bias: 227.770048, T: 58165920, Avg. loss: 2209.822313\n",
      "Total training time: 20.25 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 460.16, NNZs: 98, Bias: 227.704466, T: 58374400, Avg. loss: 2206.089558\n",
      "Total training time: 20.33 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 459.99, NNZs: 98, Bias: 227.621546, T: 58582880, Avg. loss: 2210.202061\n",
      "Total training time: 20.41 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 459.83, NNZs: 98, Bias: 227.537889, T: 58791360, Avg. loss: 2198.114985\n",
      "Total training time: 20.48 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 459.68, NNZs: 98, Bias: 227.440929, T: 58999840, Avg. loss: 2195.652240\n",
      "Total training time: 20.55 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 459.54, NNZs: 98, Bias: 227.344669, T: 59208320, Avg. loss: 2190.942833\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 459.39, NNZs: 98, Bias: 227.270164, T: 59416800, Avg. loss: 2176.115113\n",
      "Total training time: 20.72 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 459.25, NNZs: 98, Bias: 227.200930, T: 59625280, Avg. loss: 2155.062190\n",
      "Total training time: 20.80 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 459.11, NNZs: 98, Bias: 227.125263, T: 59833760, Avg. loss: 2163.048868\n",
      "Total training time: 20.89 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 458.94, NNZs: 98, Bias: 227.036648, T: 60042240, Avg. loss: 2153.368418\n",
      "Total training time: 20.98 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 458.79, NNZs: 98, Bias: 226.970306, T: 60250720, Avg. loss: 2130.625470\n",
      "Total training time: 21.04 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 458.64, NNZs: 98, Bias: 226.895202, T: 60459200, Avg. loss: 2136.262821\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 458.48, NNZs: 98, Bias: 226.804871, T: 60667680, Avg. loss: 2133.613186\n",
      "Total training time: 21.19 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 458.34, NNZs: 98, Bias: 226.726575, T: 60876160, Avg. loss: 2131.122360\n",
      "Total training time: 21.25 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 458.22, NNZs: 98, Bias: 226.632964, T: 61084640, Avg. loss: 2106.290578\n",
      "Total training time: 21.32 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 458.10, NNZs: 98, Bias: 226.550951, T: 61293120, Avg. loss: 2103.167694\n",
      "Total training time: 21.39 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 457.93, NNZs: 98, Bias: 226.470466, T: 61501600, Avg. loss: 2101.192686\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 457.81, NNZs: 98, Bias: 226.391203, T: 61710080, Avg. loss: 2090.269434\n",
      "Total training time: 21.52 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 457.68, NNZs: 98, Bias: 226.322481, T: 61918560, Avg. loss: 2079.063148\n",
      "Total training time: 21.58 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 457.54, NNZs: 98, Bias: 226.234773, T: 62127040, Avg. loss: 2079.306028\n",
      "Total training time: 21.65 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 457.39, NNZs: 98, Bias: 226.170843, T: 62335520, Avg. loss: 2064.265549\n",
      "Total training time: 21.72 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 457.24, NNZs: 98, Bias: 226.100369, T: 62544000, Avg. loss: 2061.528714\n",
      "Total training time: 21.78 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 457.10, NNZs: 98, Bias: 226.016900, T: 62752480, Avg. loss: 2059.577031\n",
      "Total training time: 21.85 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 456.96, NNZs: 98, Bias: 225.938495, T: 62960960, Avg. loss: 2056.900622\n",
      "Total training time: 21.91 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 456.80, NNZs: 98, Bias: 225.855231, T: 63169440, Avg. loss: 2044.540614\n",
      "Total training time: 21.98 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 456.66, NNZs: 98, Bias: 225.784919, T: 63377920, Avg. loss: 2036.121095\n",
      "Total training time: 22.05 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 456.53, NNZs: 98, Bias: 225.706775, T: 63586400, Avg. loss: 2026.596563\n",
      "Total training time: 22.12 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 456.40, NNZs: 98, Bias: 225.625628, T: 63794880, Avg. loss: 2021.833267\n",
      "Total training time: 22.21 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 456.28, NNZs: 98, Bias: 225.534233, T: 64003360, Avg. loss: 2009.968724\n",
      "Total training time: 22.30 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 456.14, NNZs: 98, Bias: 225.476523, T: 64211840, Avg. loss: 2009.895159\n",
      "Total training time: 22.38 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 455.99, NNZs: 98, Bias: 225.387438, T: 64420320, Avg. loss: 2001.513732\n",
      "Total training time: 22.46 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 455.86, NNZs: 98, Bias: 225.312136, T: 64628800, Avg. loss: 1990.026284\n",
      "Total training time: 22.53 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 455.71, NNZs: 98, Bias: 225.252034, T: 64837280, Avg. loss: 1988.106588\n",
      "Total training time: 22.60 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 455.57, NNZs: 98, Bias: 225.169808, T: 65045760, Avg. loss: 1980.066336\n",
      "Total training time: 22.66 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 455.44, NNZs: 98, Bias: 225.087552, T: 65254240, Avg. loss: 1977.055552\n",
      "Total training time: 22.73 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 455.29, NNZs: 98, Bias: 225.006128, T: 65462720, Avg. loss: 1970.650994\n",
      "Total training time: 22.79 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 455.16, NNZs: 98, Bias: 224.920728, T: 65671200, Avg. loss: 1962.988335\n",
      "Total training time: 22.86 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 455.02, NNZs: 98, Bias: 224.834688, T: 65879680, Avg. loss: 1956.617334\n",
      "Total training time: 22.92 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 454.90, NNZs: 98, Bias: 224.731791, T: 66088160, Avg. loss: 1968.035670\n",
      "Total training time: 22.99 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 454.76, NNZs: 98, Bias: 224.659878, T: 66296640, Avg. loss: 1948.044163\n",
      "Total training time: 23.07 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 454.63, NNZs: 98, Bias: 224.572663, T: 66505120, Avg. loss: 1942.313002\n",
      "Total training time: 23.16 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 454.50, NNZs: 98, Bias: 224.495070, T: 66713600, Avg. loss: 1932.728218\n",
      "Total training time: 23.25 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 454.33, NNZs: 98, Bias: 224.445821, T: 66922080, Avg. loss: 1925.498423\n",
      "Total training time: 23.31 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 454.20, NNZs: 98, Bias: 224.342717, T: 67130560, Avg. loss: 1927.810015\n",
      "Total training time: 23.39 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 454.07, NNZs: 98, Bias: 224.284715, T: 67339040, Avg. loss: 1925.074145\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 453.94, NNZs: 98, Bias: 224.189839, T: 67547520, Avg. loss: 1908.304606\n",
      "Total training time: 23.55 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 453.81, NNZs: 98, Bias: 224.119488, T: 67756000, Avg. loss: 1904.140954\n",
      "Total training time: 23.63 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 453.67, NNZs: 98, Bias: 224.038745, T: 67964480, Avg. loss: 1902.601764\n",
      "Total training time: 23.71 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 453.51, NNZs: 98, Bias: 223.960446, T: 68172960, Avg. loss: 1882.940865\n",
      "Total training time: 23.80 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 453.38, NNZs: 98, Bias: 223.898329, T: 68381440, Avg. loss: 1888.599888\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 453.23, NNZs: 98, Bias: 223.815797, T: 68589920, Avg. loss: 1880.799052\n",
      "Total training time: 23.97 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 453.11, NNZs: 98, Bias: 223.751724, T: 68798400, Avg. loss: 1879.237569\n",
      "Total training time: 24.04 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 453.00, NNZs: 98, Bias: 223.657219, T: 69006880, Avg. loss: 1873.231665\n",
      "Total training time: 24.11 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 452.85, NNZs: 98, Bias: 223.596321, T: 69215360, Avg. loss: 1864.309333\n",
      "Total training time: 24.21 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 452.72, NNZs: 98, Bias: 223.503843, T: 69423840, Avg. loss: 1862.493757\n",
      "Total training time: 24.30 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 452.59, NNZs: 98, Bias: 223.407043, T: 69632320, Avg. loss: 1847.889488\n",
      "Total training time: 24.39 seconds.\n",
      "-- Epoch 335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 452.45, NNZs: 98, Bias: 223.323932, T: 69840800, Avg. loss: 1841.769331\n",
      "Total training time: 24.47 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 452.30, NNZs: 98, Bias: 223.250158, T: 70049280, Avg. loss: 1839.954371\n",
      "Total training time: 24.55 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 452.19, NNZs: 98, Bias: 223.188704, T: 70257760, Avg. loss: 1837.942702\n",
      "Total training time: 24.62 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 452.06, NNZs: 98, Bias: 223.111225, T: 70466240, Avg. loss: 1836.102079\n",
      "Total training time: 24.69 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 451.93, NNZs: 98, Bias: 223.050437, T: 70674720, Avg. loss: 1821.338658\n",
      "Total training time: 24.76 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 451.77, NNZs: 98, Bias: 222.966404, T: 70883200, Avg. loss: 1819.374911\n",
      "Total training time: 24.83 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 451.65, NNZs: 98, Bias: 222.881729, T: 71091680, Avg. loss: 1817.032628\n",
      "Total training time: 24.90 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 451.51, NNZs: 98, Bias: 222.804189, T: 71300160, Avg. loss: 1815.584411\n",
      "Total training time: 24.98 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 451.40, NNZs: 98, Bias: 222.716940, T: 71508640, Avg. loss: 1803.047438\n",
      "Total training time: 25.05 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 451.25, NNZs: 98, Bias: 222.663178, T: 71717120, Avg. loss: 1794.163851\n",
      "Total training time: 25.12 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 451.11, NNZs: 98, Bias: 222.604447, T: 71925600, Avg. loss: 1791.706331\n",
      "Total training time: 25.20 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 450.99, NNZs: 98, Bias: 222.528110, T: 72134080, Avg. loss: 1781.760469\n",
      "Total training time: 25.27 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 450.84, NNZs: 98, Bias: 222.453365, T: 72342560, Avg. loss: 1782.832964\n",
      "Total training time: 25.34 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 450.70, NNZs: 98, Bias: 222.398154, T: 72551040, Avg. loss: 1777.619855\n",
      "Total training time: 25.41 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 450.59, NNZs: 98, Bias: 222.313504, T: 72759520, Avg. loss: 1763.108520\n",
      "Total training time: 25.48 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 450.46, NNZs: 98, Bias: 222.244040, T: 72968000, Avg. loss: 1764.586502\n",
      "Total training time: 25.56 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 450.34, NNZs: 98, Bias: 222.171372, T: 73176480, Avg. loss: 1761.975251\n",
      "Total training time: 25.63 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 450.19, NNZs: 98, Bias: 222.087043, T: 73384960, Avg. loss: 1749.606731\n",
      "Total training time: 25.70 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 450.06, NNZs: 98, Bias: 222.037003, T: 73593440, Avg. loss: 1758.098012\n",
      "Total training time: 25.77 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 449.94, NNZs: 98, Bias: 221.941486, T: 73801920, Avg. loss: 1744.606563\n",
      "Total training time: 25.85 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 449.83, NNZs: 98, Bias: 221.866531, T: 74010400, Avg. loss: 1734.146960\n",
      "Total training time: 25.92 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 449.70, NNZs: 98, Bias: 221.806347, T: 74218880, Avg. loss: 1738.201870\n",
      "Total training time: 25.99 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 449.56, NNZs: 98, Bias: 221.749570, T: 74427360, Avg. loss: 1719.172046\n",
      "Total training time: 26.06 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 449.42, NNZs: 98, Bias: 221.673100, T: 74635840, Avg. loss: 1715.758140\n",
      "Total training time: 26.13 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 449.30, NNZs: 98, Bias: 221.582883, T: 74844320, Avg. loss: 1723.687921\n",
      "Total training time: 26.20 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 449.17, NNZs: 98, Bias: 221.505372, T: 75052800, Avg. loss: 1718.373002\n",
      "Total training time: 26.27 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 449.04, NNZs: 98, Bias: 221.445903, T: 75261280, Avg. loss: 1706.313281\n",
      "Total training time: 26.35 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 448.92, NNZs: 98, Bias: 221.369745, T: 75469760, Avg. loss: 1711.106160\n",
      "Total training time: 26.42 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 448.78, NNZs: 98, Bias: 221.280965, T: 75678240, Avg. loss: 1697.933885\n",
      "Total training time: 26.49 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 448.67, NNZs: 98, Bias: 221.218655, T: 75886720, Avg. loss: 1691.874950\n",
      "Total training time: 26.58 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 448.53, NNZs: 98, Bias: 221.161043, T: 76095200, Avg. loss: 1698.032039\n",
      "Total training time: 26.66 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 448.42, NNZs: 98, Bias: 221.093213, T: 76303680, Avg. loss: 1687.630619\n",
      "Total training time: 26.75 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 448.30, NNZs: 98, Bias: 221.022528, T: 76512160, Avg. loss: 1681.109065\n",
      "Total training time: 26.84 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 448.17, NNZs: 98, Bias: 220.945399, T: 76720640, Avg. loss: 1671.847830\n",
      "Total training time: 26.93 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 448.04, NNZs: 98, Bias: 220.891100, T: 76929120, Avg. loss: 1675.946769\n",
      "Total training time: 27.02 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 447.91, NNZs: 98, Bias: 220.822308, T: 77137600, Avg. loss: 1663.089326\n",
      "Total training time: 27.11 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 447.80, NNZs: 98, Bias: 220.740387, T: 77346080, Avg. loss: 1669.728979\n",
      "Total training time: 27.19 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 447.68, NNZs: 98, Bias: 220.674662, T: 77554560, Avg. loss: 1661.880619\n",
      "Total training time: 27.26 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 447.57, NNZs: 98, Bias: 220.592122, T: 77763040, Avg. loss: 1663.516989\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 447.46, NNZs: 98, Bias: 220.515694, T: 77971520, Avg. loss: 1650.237273\n",
      "Total training time: 27.39 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 447.34, NNZs: 98, Bias: 220.441931, T: 78180000, Avg. loss: 1639.347782\n",
      "Total training time: 27.46 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 447.22, NNZs: 98, Bias: 220.375518, T: 78388480, Avg. loss: 1643.379315\n",
      "Total training time: 27.54 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 447.12, NNZs: 98, Bias: 220.287348, T: 78596960, Avg. loss: 1634.797875\n",
      "Total training time: 27.63 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 446.99, NNZs: 98, Bias: 220.215692, T: 78805440, Avg. loss: 1641.356041\n",
      "Total training time: 27.72 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 446.87, NNZs: 98, Bias: 220.135872, T: 79013920, Avg. loss: 1630.159513\n",
      "Total training time: 27.79 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 446.75, NNZs: 98, Bias: 220.047774, T: 79222400, Avg. loss: 1629.987384\n",
      "Total training time: 27.86 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 446.61, NNZs: 98, Bias: 219.974036, T: 79430880, Avg. loss: 1615.218684\n",
      "Total training time: 27.93 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 446.48, NNZs: 98, Bias: 219.908929, T: 79639360, Avg. loss: 1619.518652\n",
      "Total training time: 28.02 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 446.35, NNZs: 98, Bias: 219.838720, T: 79847840, Avg. loss: 1605.476471\n",
      "Total training time: 28.10 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 446.23, NNZs: 98, Bias: 219.782297, T: 80056320, Avg. loss: 1615.578275\n",
      "Total training time: 28.17 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 446.09, NNZs: 98, Bias: 219.725043, T: 80264800, Avg. loss: 1598.518336\n",
      "Total training time: 28.24 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 445.98, NNZs: 98, Bias: 219.639321, T: 80473280, Avg. loss: 1595.130382\n",
      "Total training time: 28.32 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 445.85, NNZs: 98, Bias: 219.547862, T: 80681760, Avg. loss: 1593.212573\n",
      "Total training time: 28.39 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 445.72, NNZs: 98, Bias: 219.484009, T: 80890240, Avg. loss: 1593.373403\n",
      "Total training time: 28.46 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 445.58, NNZs: 98, Bias: 219.409447, T: 81098720, Avg. loss: 1588.317211\n",
      "Total training time: 28.53 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 445.46, NNZs: 98, Bias: 219.345306, T: 81307200, Avg. loss: 1585.724844\n",
      "Total training time: 28.61 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 445.33, NNZs: 98, Bias: 219.289513, T: 81515680, Avg. loss: 1575.436371\n",
      "Total training time: 28.68 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 445.21, NNZs: 98, Bias: 219.217612, T: 81724160, Avg. loss: 1568.188604\n",
      "Total training time: 28.75 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 445.08, NNZs: 98, Bias: 219.161880, T: 81932640, Avg. loss: 1566.211636\n",
      "Total training time: 28.82 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 444.96, NNZs: 98, Bias: 219.075683, T: 82141120, Avg. loss: 1559.310435\n",
      "Total training time: 28.90 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 444.82, NNZs: 98, Bias: 219.020497, T: 82349600, Avg. loss: 1571.792287\n",
      "Total training time: 28.97 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 444.69, NNZs: 98, Bias: 218.935842, T: 82558080, Avg. loss: 1553.255321\n",
      "Total training time: 29.04 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 444.56, NNZs: 98, Bias: 218.864008, T: 82766560, Avg. loss: 1555.744154\n",
      "Total training time: 29.11 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 444.45, NNZs: 98, Bias: 218.778597, T: 82975040, Avg. loss: 1548.164836\n",
      "Total training time: 29.18 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 444.31, NNZs: 98, Bias: 218.712526, T: 83183520, Avg. loss: 1537.012953\n",
      "Total training time: 29.26 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 444.18, NNZs: 98, Bias: 218.669191, T: 83392000, Avg. loss: 1548.953813\n",
      "Total training time: 29.33 seconds.\n",
      "-- Epoch 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 444.07, NNZs: 98, Bias: 218.596488, T: 83600480, Avg. loss: 1538.776778\n",
      "Total training time: 29.40 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 443.96, NNZs: 98, Bias: 218.511198, T: 83808960, Avg. loss: 1536.913853\n",
      "Total training time: 29.47 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 443.82, NNZs: 98, Bias: 218.438964, T: 84017440, Avg. loss: 1530.447620\n",
      "Total training time: 29.56 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 443.68, NNZs: 98, Bias: 218.383801, T: 84225920, Avg. loss: 1524.173921\n",
      "Total training time: 29.65 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 443.54, NNZs: 98, Bias: 218.304104, T: 84434400, Avg. loss: 1522.985467\n",
      "Total training time: 29.74 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 443.41, NNZs: 98, Bias: 218.245912, T: 84642880, Avg. loss: 1513.311680\n",
      "Total training time: 29.82 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 443.30, NNZs: 98, Bias: 218.175469, T: 84851360, Avg. loss: 1511.428604\n",
      "Total training time: 29.91 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 443.18, NNZs: 98, Bias: 218.112265, T: 85059840, Avg. loss: 1507.299287\n",
      "Total training time: 29.98 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 443.06, NNZs: 98, Bias: 218.046503, T: 85268320, Avg. loss: 1498.704822\n",
      "Total training time: 30.04 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 442.92, NNZs: 98, Bias: 217.978203, T: 85476800, Avg. loss: 1497.159961\n",
      "Total training time: 30.11 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 442.82, NNZs: 98, Bias: 217.918629, T: 85685280, Avg. loss: 1507.470263\n",
      "Total training time: 30.17 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 442.69, NNZs: 98, Bias: 217.874584, T: 85893760, Avg. loss: 1499.411345\n",
      "Total training time: 30.24 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 442.57, NNZs: 98, Bias: 217.809942, T: 86102240, Avg. loss: 1488.465055\n",
      "Total training time: 30.31 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 442.45, NNZs: 98, Bias: 217.749291, T: 86310720, Avg. loss: 1489.645581\n",
      "Total training time: 30.39 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 442.34, NNZs: 98, Bias: 217.688063, T: 86519200, Avg. loss: 1482.880097\n",
      "Total training time: 30.48 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 442.22, NNZs: 98, Bias: 217.620060, T: 86727680, Avg. loss: 1490.715405\n",
      "Total training time: 30.55 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 442.10, NNZs: 98, Bias: 217.548427, T: 86936160, Avg. loss: 1479.235920\n",
      "Total training time: 30.62 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 441.98, NNZs: 98, Bias: 217.466522, T: 87144640, Avg. loss: 1478.036009\n",
      "Total training time: 30.69 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 441.84, NNZs: 98, Bias: 217.399946, T: 87353120, Avg. loss: 1470.248964\n",
      "Total training time: 30.76 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 441.73, NNZs: 98, Bias: 217.345642, T: 87561600, Avg. loss: 1463.033789\n",
      "Total training time: 30.82 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 441.62, NNZs: 98, Bias: 217.262836, T: 87770080, Avg. loss: 1461.626016\n",
      "Total training time: 30.89 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 441.48, NNZs: 98, Bias: 217.201049, T: 87978560, Avg. loss: 1465.355043\n",
      "Total training time: 30.95 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 441.37, NNZs: 98, Bias: 217.143624, T: 88187040, Avg. loss: 1463.104336\n",
      "Total training time: 31.02 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 441.25, NNZs: 98, Bias: 217.077364, T: 88395520, Avg. loss: 1453.750446\n",
      "Total training time: 31.09 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 441.12, NNZs: 98, Bias: 217.005951, T: 88604000, Avg. loss: 1438.855265\n",
      "Total training time: 31.15 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 441.00, NNZs: 98, Bias: 216.933698, T: 88812480, Avg. loss: 1444.720382\n",
      "Total training time: 31.22 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 440.88, NNZs: 98, Bias: 216.853967, T: 89020960, Avg. loss: 1449.917297\n",
      "Total training time: 31.29 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 440.75, NNZs: 98, Bias: 216.791037, T: 89229440, Avg. loss: 1447.117263\n",
      "Total training time: 31.35 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 440.63, NNZs: 98, Bias: 216.730372, T: 89437920, Avg. loss: 1436.156948\n",
      "Total training time: 31.43 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 440.52, NNZs: 98, Bias: 216.660142, T: 89646400, Avg. loss: 1432.579485\n",
      "Total training time: 31.52 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 440.40, NNZs: 98, Bias: 216.598092, T: 89854880, Avg. loss: 1431.960624\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 440.28, NNZs: 98, Bias: 216.516301, T: 90063360, Avg. loss: 1419.871589\n",
      "Total training time: 31.69 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 440.18, NNZs: 98, Bias: 216.451979, T: 90271840, Avg. loss: 1431.035987\n",
      "Total training time: 31.75 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 440.05, NNZs: 98, Bias: 216.393434, T: 90480320, Avg. loss: 1418.811370\n",
      "Total training time: 31.82 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 439.95, NNZs: 98, Bias: 216.327201, T: 90688800, Avg. loss: 1419.291423\n",
      "Total training time: 31.88 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 439.84, NNZs: 98, Bias: 216.257373, T: 90897280, Avg. loss: 1415.904054\n",
      "Total training time: 31.95 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 439.71, NNZs: 98, Bias: 216.197814, T: 91105760, Avg. loss: 1410.334452\n",
      "Total training time: 32.01 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 439.59, NNZs: 98, Bias: 216.142652, T: 91314240, Avg. loss: 1400.292694\n",
      "Total training time: 32.08 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 439.47, NNZs: 98, Bias: 216.094263, T: 91522720, Avg. loss: 1403.121641\n",
      "Total training time: 32.15 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 439.36, NNZs: 98, Bias: 216.033128, T: 91731200, Avg. loss: 1401.566431\n",
      "Total training time: 32.21 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 439.25, NNZs: 98, Bias: 215.978679, T: 91939680, Avg. loss: 1402.255264\n",
      "Total training time: 32.28 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 439.14, NNZs: 98, Bias: 215.917176, T: 92148160, Avg. loss: 1392.632171\n",
      "Total training time: 32.34 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 439.01, NNZs: 98, Bias: 215.853778, T: 92356640, Avg. loss: 1385.462268\n",
      "Total training time: 32.41 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 438.89, NNZs: 98, Bias: 215.799262, T: 92565120, Avg. loss: 1386.087350\n",
      "Total training time: 32.47 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 438.78, NNZs: 98, Bias: 215.747571, T: 92773600, Avg. loss: 1379.953959\n",
      "Total training time: 32.54 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 438.66, NNZs: 98, Bias: 215.702133, T: 92982080, Avg. loss: 1381.972051\n",
      "Total training time: 32.61 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 438.55, NNZs: 98, Bias: 215.641226, T: 93190560, Avg. loss: 1381.341337\n",
      "Total training time: 32.67 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 438.42, NNZs: 98, Bias: 215.571867, T: 93399040, Avg. loss: 1373.286361\n",
      "Total training time: 32.74 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 438.32, NNZs: 98, Bias: 215.519885, T: 93607520, Avg. loss: 1375.753064\n",
      "Total training time: 32.80 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 438.20, NNZs: 98, Bias: 215.457355, T: 93816000, Avg. loss: 1363.432867\n",
      "Total training time: 32.87 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 438.08, NNZs: 98, Bias: 215.396135, T: 94024480, Avg. loss: 1376.371724\n",
      "Total training time: 32.93 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 437.96, NNZs: 98, Bias: 215.347160, T: 94232960, Avg. loss: 1361.496686\n",
      "Total training time: 33.00 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 437.85, NNZs: 98, Bias: 215.278900, T: 94441440, Avg. loss: 1352.190178\n",
      "Total training time: 33.06 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 437.73, NNZs: 98, Bias: 215.224423, T: 94649920, Avg. loss: 1353.592475\n",
      "Total training time: 33.13 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 437.62, NNZs: 98, Bias: 215.157829, T: 94858400, Avg. loss: 1354.731838\n",
      "Total training time: 33.19 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 437.51, NNZs: 98, Bias: 215.105614, T: 95066880, Avg. loss: 1347.566930\n",
      "Total training time: 33.26 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 437.39, NNZs: 98, Bias: 215.035857, T: 95275360, Avg. loss: 1343.314237\n",
      "Total training time: 33.33 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 437.28, NNZs: 98, Bias: 214.973892, T: 95483840, Avg. loss: 1345.599429\n",
      "Total training time: 33.39 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 437.18, NNZs: 98, Bias: 214.923063, T: 95692320, Avg. loss: 1340.071409\n",
      "Total training time: 33.46 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 437.08, NNZs: 98, Bias: 214.860127, T: 95900800, Avg. loss: 1343.109417\n",
      "Total training time: 33.52 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 436.98, NNZs: 98, Bias: 214.794088, T: 96109280, Avg. loss: 1338.595028\n",
      "Total training time: 33.59 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 436.88, NNZs: 98, Bias: 214.735991, T: 96317760, Avg. loss: 1335.000273\n",
      "Total training time: 33.65 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 436.76, NNZs: 98, Bias: 214.686417, T: 96526240, Avg. loss: 1327.446171\n",
      "Total training time: 33.72 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 436.65, NNZs: 98, Bias: 214.620497, T: 96734720, Avg. loss: 1324.970155\n",
      "Total training time: 33.79 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 436.54, NNZs: 98, Bias: 214.561630, T: 96943200, Avg. loss: 1319.590135\n",
      "Total training time: 33.85 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 436.43, NNZs: 98, Bias: 214.505971, T: 97151680, Avg. loss: 1318.552045\n",
      "Total training time: 33.92 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 436.31, NNZs: 98, Bias: 214.446121, T: 97360160, Avg. loss: 1320.152482\n",
      "Total training time: 33.98 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 436.20, NNZs: 98, Bias: 214.396360, T: 97568640, Avg. loss: 1313.080619\n",
      "Total training time: 34.05 seconds.\n",
      "-- Epoch 469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 436.09, NNZs: 98, Bias: 214.338004, T: 97777120, Avg. loss: 1311.279762\n",
      "Total training time: 34.11 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 435.98, NNZs: 98, Bias: 214.292335, T: 97985600, Avg. loss: 1304.884544\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 435.86, NNZs: 98, Bias: 214.241757, T: 98194080, Avg. loss: 1301.289676\n",
      "Total training time: 34.24 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 435.76, NNZs: 98, Bias: 214.176732, T: 98402560, Avg. loss: 1306.031569\n",
      "Total training time: 34.31 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 435.63, NNZs: 98, Bias: 214.127296, T: 98611040, Avg. loss: 1301.085948\n",
      "Total training time: 34.37 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 435.53, NNZs: 98, Bias: 214.062152, T: 98819520, Avg. loss: 1289.564619\n",
      "Total training time: 34.44 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 435.41, NNZs: 98, Bias: 213.993412, T: 99028000, Avg. loss: 1293.936276\n",
      "Total training time: 34.50 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 435.30, NNZs: 98, Bias: 213.927451, T: 99236480, Avg. loss: 1289.688171\n",
      "Total training time: 34.57 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 435.18, NNZs: 98, Bias: 213.867155, T: 99444960, Avg. loss: 1294.935823\n",
      "Total training time: 34.64 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 435.09, NNZs: 98, Bias: 213.797436, T: 99653440, Avg. loss: 1288.751436\n",
      "Total training time: 34.70 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 434.97, NNZs: 98, Bias: 213.737996, T: 99861920, Avg. loss: 1284.993206\n",
      "Total training time: 34.77 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 434.86, NNZs: 98, Bias: 213.670668, T: 100070400, Avg. loss: 1286.480859\n",
      "Total training time: 34.83 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 434.75, NNZs: 98, Bias: 213.615062, T: 100278880, Avg. loss: 1272.558748\n",
      "Total training time: 34.90 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 434.64, NNZs: 98, Bias: 213.558372, T: 100487360, Avg. loss: 1272.255491\n",
      "Total training time: 34.96 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 434.52, NNZs: 98, Bias: 213.500813, T: 100695840, Avg. loss: 1270.361486\n",
      "Total training time: 35.03 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 434.40, NNZs: 98, Bias: 213.424113, T: 100904320, Avg. loss: 1267.237793\n",
      "Total training time: 35.09 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 434.30, NNZs: 98, Bias: 213.372847, T: 101112800, Avg. loss: 1268.406195\n",
      "Total training time: 35.16 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 434.19, NNZs: 98, Bias: 213.313369, T: 101321280, Avg. loss: 1266.346999\n",
      "Total training time: 35.23 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 434.07, NNZs: 98, Bias: 213.260626, T: 101529760, Avg. loss: 1258.838354\n",
      "Total training time: 35.31 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 433.95, NNZs: 98, Bias: 213.204445, T: 101738240, Avg. loss: 1258.291310\n",
      "Total training time: 35.38 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 433.84, NNZs: 98, Bias: 213.151913, T: 101946720, Avg. loss: 1258.223001\n",
      "Total training time: 35.44 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 433.73, NNZs: 98, Bias: 213.109966, T: 102155200, Avg. loss: 1257.031026\n",
      "Total training time: 35.51 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 433.61, NNZs: 98, Bias: 213.046021, T: 102363680, Avg. loss: 1252.652084\n",
      "Total training time: 35.57 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 433.51, NNZs: 98, Bias: 212.989021, T: 102572160, Avg. loss: 1246.589537\n",
      "Total training time: 35.64 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 433.38, NNZs: 98, Bias: 212.922797, T: 102780640, Avg. loss: 1244.562226\n",
      "Total training time: 35.70 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 433.27, NNZs: 98, Bias: 212.857687, T: 102989120, Avg. loss: 1240.904397\n",
      "Total training time: 35.77 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 433.17, NNZs: 98, Bias: 212.799195, T: 103197600, Avg. loss: 1239.547840\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 433.06, NNZs: 98, Bias: 212.745373, T: 103406080, Avg. loss: 1237.612630\n",
      "Total training time: 35.90 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 432.94, NNZs: 98, Bias: 212.680358, T: 103614560, Avg. loss: 1234.749407\n",
      "Total training time: 35.97 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 432.82, NNZs: 98, Bias: 212.602067, T: 103823040, Avg. loss: 1230.979046\n",
      "Total training time: 36.03 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 432.70, NNZs: 98, Bias: 212.555016, T: 104031520, Avg. loss: 1230.729402\n",
      "Total training time: 36.10 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 432.61, NNZs: 98, Bias: 212.492023, T: 104240000, Avg. loss: 1230.416322\n",
      "Total training time: 36.16 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 432.51, NNZs: 98, Bias: 212.433935, T: 104448480, Avg. loss: 1236.380408\n",
      "Total training time: 36.23 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 432.40, NNZs: 98, Bias: 212.373380, T: 104656960, Avg. loss: 1224.625467\n",
      "Total training time: 36.29 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 432.29, NNZs: 98, Bias: 212.312295, T: 104865440, Avg. loss: 1216.473303\n",
      "Total training time: 36.36 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 432.18, NNZs: 98, Bias: 212.265430, T: 105073920, Avg. loss: 1220.171017\n",
      "Total training time: 36.43 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 432.06, NNZs: 98, Bias: 212.204389, T: 105282400, Avg. loss: 1213.837254\n",
      "Total training time: 36.49 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 431.96, NNZs: 98, Bias: 212.150783, T: 105490880, Avg. loss: 1217.471202\n",
      "Total training time: 36.56 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 431.85, NNZs: 98, Bias: 212.092541, T: 105699360, Avg. loss: 1216.141514\n",
      "Total training time: 36.63 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 431.73, NNZs: 98, Bias: 212.041318, T: 105907840, Avg. loss: 1206.101898\n",
      "Total training time: 36.69 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 431.63, NNZs: 98, Bias: 211.985954, T: 106116320, Avg. loss: 1209.886032\n",
      "Total training time: 36.76 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 431.51, NNZs: 98, Bias: 211.930885, T: 106324800, Avg. loss: 1206.578206\n",
      "Total training time: 36.82 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 431.41, NNZs: 98, Bias: 211.891148, T: 106533280, Avg. loss: 1204.187907\n",
      "Total training time: 36.89 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 431.30, NNZs: 98, Bias: 211.842016, T: 106741760, Avg. loss: 1204.838860\n",
      "Total training time: 36.95 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 431.20, NNZs: 98, Bias: 211.785681, T: 106950240, Avg. loss: 1198.897875\n",
      "Total training time: 37.02 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 431.08, NNZs: 98, Bias: 211.735136, T: 107158720, Avg. loss: 1191.915583\n",
      "Total training time: 37.08 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 430.97, NNZs: 98, Bias: 211.681146, T: 107367200, Avg. loss: 1191.063580\n",
      "Total training time: 37.15 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 430.86, NNZs: 98, Bias: 211.631641, T: 107575680, Avg. loss: 1193.760282\n",
      "Total training time: 37.21 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 430.75, NNZs: 98, Bias: 211.573031, T: 107784160, Avg. loss: 1185.635648\n",
      "Total training time: 37.28 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 430.65, NNZs: 98, Bias: 211.518258, T: 107992640, Avg. loss: 1185.672675\n",
      "Total training time: 37.35 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 430.54, NNZs: 98, Bias: 211.463765, T: 108201120, Avg. loss: 1188.129550\n",
      "Total training time: 37.41 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 430.45, NNZs: 98, Bias: 211.409373, T: 108409600, Avg. loss: 1180.297577\n",
      "Total training time: 37.48 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 430.33, NNZs: 98, Bias: 211.355278, T: 108618080, Avg. loss: 1173.347972\n",
      "Total training time: 37.54 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 430.23, NNZs: 98, Bias: 211.291825, T: 108826560, Avg. loss: 1177.986822\n",
      "Total training time: 37.61 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 430.12, NNZs: 98, Bias: 211.236739, T: 109035040, Avg. loss: 1173.921444\n",
      "Total training time: 37.67 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 430.01, NNZs: 98, Bias: 211.167745, T: 109243520, Avg. loss: 1170.108493\n",
      "Total training time: 37.74 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 429.89, NNZs: 98, Bias: 211.121849, T: 109452000, Avg. loss: 1169.847521\n",
      "Total training time: 37.81 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 429.81, NNZs: 98, Bias: 211.059060, T: 109660480, Avg. loss: 1164.081612\n",
      "Total training time: 37.87 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 429.69, NNZs: 98, Bias: 211.018873, T: 109868960, Avg. loss: 1164.908188\n",
      "Total training time: 37.94 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 429.58, NNZs: 98, Bias: 210.966139, T: 110077440, Avg. loss: 1159.605084\n",
      "Total training time: 38.00 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 429.48, NNZs: 98, Bias: 210.907424, T: 110285920, Avg. loss: 1160.031502\n",
      "Total training time: 38.07 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 429.37, NNZs: 98, Bias: 210.844203, T: 110494400, Avg. loss: 1159.192060\n",
      "Total training time: 38.13 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 429.25, NNZs: 98, Bias: 210.791403, T: 110702880, Avg. loss: 1153.502395\n",
      "Total training time: 38.20 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 429.14, NNZs: 98, Bias: 210.739962, T: 110911360, Avg. loss: 1149.008614\n",
      "Total training time: 38.26 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 429.04, NNZs: 98, Bias: 210.690502, T: 111119840, Avg. loss: 1153.066096\n",
      "Total training time: 38.33 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 428.94, NNZs: 98, Bias: 210.632703, T: 111328320, Avg. loss: 1150.201376\n",
      "Total training time: 38.40 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 428.83, NNZs: 98, Bias: 210.586124, T: 111536800, Avg. loss: 1146.912930\n",
      "Total training time: 38.46 seconds.\n",
      "-- Epoch 536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 428.72, NNZs: 98, Bias: 210.534271, T: 111745280, Avg. loss: 1141.236993\n",
      "Total training time: 38.53 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 428.61, NNZs: 98, Bias: 210.469890, T: 111953760, Avg. loss: 1131.512071\n",
      "Total training time: 38.59 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 428.50, NNZs: 98, Bias: 210.403593, T: 112162240, Avg. loss: 1137.903327\n",
      "Total training time: 38.66 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 428.41, NNZs: 98, Bias: 210.345164, T: 112370720, Avg. loss: 1132.569332\n",
      "Total training time: 38.73 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 428.30, NNZs: 98, Bias: 210.289065, T: 112579200, Avg. loss: 1140.356367\n",
      "Total training time: 38.82 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 428.20, NNZs: 98, Bias: 210.236802, T: 112787680, Avg. loss: 1134.824216\n",
      "Total training time: 38.91 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 428.10, NNZs: 98, Bias: 210.175599, T: 112996160, Avg. loss: 1133.567542\n",
      "Total training time: 39.00 seconds.\n",
      "Convergence after 542 epochs took 39.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2287.09, NNZs: 98, Bias: -361.907276, T: 208480, Avg. loss: 3048455.635020\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2178.74, NNZs: 98, Bias: -400.425300, T: 416960, Avg. loss: 394218.718185\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 2069.85, NNZs: 98, Bias: -413.779660, T: 625440, Avg. loss: 231067.441646\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1946.56, NNZs: 98, Bias: -419.640512, T: 833920, Avg. loss: 164114.707519\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1873.44, NNZs: 98, Bias: -425.905796, T: 1042400, Avg. loss: 126871.892217\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1797.24, NNZs: 98, Bias: -426.860007, T: 1250880, Avg. loss: 103807.793938\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1739.63, NNZs: 98, Bias: -428.116967, T: 1459360, Avg. loss: 87436.239659\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1687.64, NNZs: 98, Bias: -429.133081, T: 1667840, Avg. loss: 75951.638874\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1644.24, NNZs: 98, Bias: -429.141905, T: 1876320, Avg. loss: 67084.262379\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1600.45, NNZs: 98, Bias: -428.721757, T: 2084800, Avg. loss: 59607.238919\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1564.34, NNZs: 98, Bias: -428.007022, T: 2293280, Avg. loss: 54085.949354\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1528.56, NNZs: 98, Bias: -426.908142, T: 2501760, Avg. loss: 49358.457532\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1498.04, NNZs: 98, Bias: -426.271645, T: 2710240, Avg. loss: 45304.538514\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1470.90, NNZs: 98, Bias: -425.719839, T: 2918720, Avg. loss: 41759.712444\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1441.63, NNZs: 98, Bias: -424.710784, T: 3127200, Avg. loss: 39062.611642\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1416.96, NNZs: 98, Bias: -424.125686, T: 3335680, Avg. loss: 36476.254487\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1393.90, NNZs: 98, Bias: -423.787644, T: 3544160, Avg. loss: 34150.686712\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1374.05, NNZs: 98, Bias: -423.109694, T: 3752640, Avg. loss: 32411.556355\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1357.81, NNZs: 98, Bias: -422.429427, T: 3961120, Avg. loss: 30727.972043\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1339.83, NNZs: 98, Bias: -421.490560, T: 4169600, Avg. loss: 29053.319648\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1325.07, NNZs: 98, Bias: -421.263620, T: 4378080, Avg. loss: 27591.896802\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1309.79, NNZs: 98, Bias: -420.715769, T: 4586560, Avg. loss: 26287.470185\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1296.84, NNZs: 98, Bias: -420.384398, T: 4795040, Avg. loss: 25250.065093\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1282.23, NNZs: 98, Bias: -419.641018, T: 5003520, Avg. loss: 24093.521306\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1269.90, NNZs: 98, Bias: -419.237694, T: 5212000, Avg. loss: 23088.241691\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1256.40, NNZs: 98, Bias: -418.582714, T: 5420480, Avg. loss: 22256.017700\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1244.26, NNZs: 98, Bias: -418.279798, T: 5628960, Avg. loss: 21289.236706\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1231.86, NNZs: 98, Bias: -417.934011, T: 5837440, Avg. loss: 20482.093128\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1220.34, NNZs: 98, Bias: -417.723788, T: 6045920, Avg. loss: 19790.037372\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1209.48, NNZs: 98, Bias: -417.244340, T: 6254400, Avg. loss: 19144.858656\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1200.13, NNZs: 98, Bias: -416.992526, T: 6462880, Avg. loss: 18411.592169\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1191.97, NNZs: 98, Bias: -416.954792, T: 6671360, Avg. loss: 17860.620294\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1184.26, NNZs: 98, Bias: -416.670714, T: 6879840, Avg. loss: 17372.534553\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1176.41, NNZs: 98, Bias: -416.486250, T: 7088320, Avg. loss: 16872.538999\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1168.53, NNZs: 98, Bias: -416.115869, T: 7296800, Avg. loss: 16415.162607\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1162.01, NNZs: 98, Bias: -415.955924, T: 7505280, Avg. loss: 15856.031241\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1155.14, NNZs: 98, Bias: -415.930874, T: 7713760, Avg. loss: 15432.268470\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1148.30, NNZs: 98, Bias: -415.835617, T: 7922240, Avg. loss: 15051.861175\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1141.38, NNZs: 98, Bias: -415.637789, T: 8130720, Avg. loss: 14663.846529\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1135.11, NNZs: 98, Bias: -415.573626, T: 8339200, Avg. loss: 14316.672064\n",
      "Total training time: 2.69 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1128.57, NNZs: 98, Bias: -415.330089, T: 8547680, Avg. loss: 13890.805273\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1123.31, NNZs: 98, Bias: -415.296610, T: 8756160, Avg. loss: 13544.858111\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1116.88, NNZs: 98, Bias: -415.291974, T: 8964640, Avg. loss: 13174.338714\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1111.18, NNZs: 98, Bias: -415.278936, T: 9173120, Avg. loss: 12932.817190\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1106.13, NNZs: 98, Bias: -415.322501, T: 9381600, Avg. loss: 12662.868815\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1100.39, NNZs: 98, Bias: -415.336099, T: 9590080, Avg. loss: 12280.641881\n",
      "Total training time: 3.07 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1095.45, NNZs: 98, Bias: -415.418896, T: 9798560, Avg. loss: 12044.515021\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1090.13, NNZs: 98, Bias: -415.399826, T: 10007040, Avg. loss: 11848.861447\n",
      "Total training time: 3.23 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1085.45, NNZs: 98, Bias: -415.465509, T: 10215520, Avg. loss: 11631.604521\n",
      "Total training time: 3.32 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1080.32, NNZs: 98, Bias: -415.598831, T: 10424000, Avg. loss: 11349.835199\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1076.07, NNZs: 98, Bias: -415.631202, T: 10632480, Avg. loss: 11110.497527\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1070.98, NNZs: 98, Bias: -415.654696, T: 10840960, Avg. loss: 10928.137587\n",
      "Total training time: 3.54 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1066.15, NNZs: 98, Bias: -415.577966, T: 11049440, Avg. loss: 10749.840281\n",
      "Total training time: 3.61 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1061.34, NNZs: 98, Bias: -415.696196, T: 11257920, Avg. loss: 10431.239341\n",
      "Total training time: 3.68 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1056.85, NNZs: 98, Bias: -415.728994, T: 11466400, Avg. loss: 10283.770790\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1052.57, NNZs: 98, Bias: -415.847921, T: 11674880, Avg. loss: 10109.279825\n",
      "Total training time: 3.83 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1049.26, NNZs: 98, Bias: -416.056500, T: 11883360, Avg. loss: 9966.920262\n",
      "Total training time: 3.90 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1045.17, NNZs: 98, Bias: -416.166857, T: 12091840, Avg. loss: 9745.645338\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1040.74, NNZs: 98, Bias: -416.259814, T: 12300320, Avg. loss: 9619.604868\n",
      "Total training time: 4.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1036.88, NNZs: 98, Bias: -416.376427, T: 12508800, Avg. loss: 9414.837031\n",
      "Total training time: 4.11 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1033.24, NNZs: 98, Bias: -416.343603, T: 12717280, Avg. loss: 9269.209599\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 1029.07, NNZs: 98, Bias: -416.588277, T: 12925760, Avg. loss: 9108.969847\n",
      "Total training time: 4.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 1025.46, NNZs: 98, Bias: -416.827933, T: 13134240, Avg. loss: 8999.283523\n",
      "Total training time: 4.32 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 1021.84, NNZs: 98, Bias: -416.984505, T: 13342720, Avg. loss: 8817.682504\n",
      "Total training time: 4.40 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 1018.31, NNZs: 98, Bias: -417.117864, T: 13551200, Avg. loss: 8723.455352\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 1015.48, NNZs: 98, Bias: -417.268399, T: 13759680, Avg. loss: 8569.849322\n",
      "Total training time: 4.54 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 1011.56, NNZs: 98, Bias: -417.418768, T: 13968160, Avg. loss: 8437.436948\n",
      "Total training time: 4.61 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 1008.37, NNZs: 98, Bias: -417.691641, T: 14176640, Avg. loss: 8294.343036\n",
      "Total training time: 4.68 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 1005.25, NNZs: 98, Bias: -417.790097, T: 14385120, Avg. loss: 8187.628395\n",
      "Total training time: 4.75 seconds.\n",
      "-- Epoch 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1002.48, NNZs: 98, Bias: -417.946546, T: 14593600, Avg. loss: 8037.433681\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 999.22, NNZs: 98, Bias: -418.076222, T: 14802080, Avg. loss: 7949.637004\n",
      "Total training time: 4.89 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 996.44, NNZs: 98, Bias: -418.285550, T: 15010560, Avg. loss: 7832.842340\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 993.59, NNZs: 98, Bias: -418.456890, T: 15219040, Avg. loss: 7730.249188\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 990.99, NNZs: 98, Bias: -418.685329, T: 15427520, Avg. loss: 7589.351841\n",
      "Total training time: 5.12 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 988.53, NNZs: 98, Bias: -418.760769, T: 15636000, Avg. loss: 7527.867133\n",
      "Total training time: 5.19 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 985.81, NNZs: 98, Bias: -418.909176, T: 15844480, Avg. loss: 7426.009731\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 982.80, NNZs: 98, Bias: -419.217246, T: 16052960, Avg. loss: 7323.979256\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 980.07, NNZs: 98, Bias: -419.331250, T: 16261440, Avg. loss: 7235.025405\n",
      "Total training time: 5.40 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 977.45, NNZs: 98, Bias: -419.512339, T: 16469920, Avg. loss: 7149.674204\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 974.90, NNZs: 98, Bias: -419.748413, T: 16678400, Avg. loss: 7043.532609\n",
      "Total training time: 5.54 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 972.55, NNZs: 98, Bias: -419.932041, T: 16886880, Avg. loss: 6923.446103\n",
      "Total training time: 5.61 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 969.91, NNZs: 98, Bias: -420.095889, T: 17095360, Avg. loss: 6855.337818\n",
      "Total training time: 5.68 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 967.69, NNZs: 98, Bias: -420.168910, T: 17303840, Avg. loss: 6785.297006\n",
      "Total training time: 5.75 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 965.36, NNZs: 98, Bias: -420.334113, T: 17512320, Avg. loss: 6709.344481\n",
      "Total training time: 5.82 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 962.85, NNZs: 98, Bias: -420.465932, T: 17720800, Avg. loss: 6626.466072\n",
      "Total training time: 5.89 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 960.46, NNZs: 98, Bias: -420.639132, T: 17929280, Avg. loss: 6526.374775\n",
      "Total training time: 5.96 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 958.06, NNZs: 98, Bias: -420.885302, T: 18137760, Avg. loss: 6456.946808\n",
      "Total training time: 6.03 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 955.72, NNZs: 98, Bias: -421.112028, T: 18346240, Avg. loss: 6376.143436\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 953.73, NNZs: 98, Bias: -421.311753, T: 18554720, Avg. loss: 6299.052308\n",
      "Total training time: 6.18 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 951.30, NNZs: 98, Bias: -421.560568, T: 18763200, Avg. loss: 6234.032364\n",
      "Total training time: 6.25 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 949.29, NNZs: 98, Bias: -421.752733, T: 18971680, Avg. loss: 6158.632484\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 947.34, NNZs: 98, Bias: -421.983519, T: 19180160, Avg. loss: 6120.322170\n",
      "Total training time: 6.39 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 945.41, NNZs: 98, Bias: -422.260985, T: 19388640, Avg. loss: 6022.184179\n",
      "Total training time: 6.46 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 943.22, NNZs: 98, Bias: -422.468778, T: 19597120, Avg. loss: 5944.495384\n",
      "Total training time: 6.53 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 941.09, NNZs: 98, Bias: -422.574920, T: 19805600, Avg. loss: 5867.343238\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 939.21, NNZs: 98, Bias: -422.724970, T: 20014080, Avg. loss: 5870.116253\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 937.35, NNZs: 98, Bias: -422.923089, T: 20222560, Avg. loss: 5767.197903\n",
      "Total training time: 6.75 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 935.38, NNZs: 98, Bias: -423.178917, T: 20431040, Avg. loss: 5757.511659\n",
      "Total training time: 6.84 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 933.18, NNZs: 98, Bias: -423.324945, T: 20639520, Avg. loss: 5635.994820\n",
      "Total training time: 6.92 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 931.22, NNZs: 98, Bias: -423.473426, T: 20848000, Avg. loss: 5602.112981\n",
      "Total training time: 6.99 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 929.18, NNZs: 98, Bias: -423.697542, T: 21056480, Avg. loss: 5570.844347\n",
      "Total training time: 7.07 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 927.07, NNZs: 98, Bias: -423.839204, T: 21264960, Avg. loss: 5479.489321\n",
      "Total training time: 7.14 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 925.28, NNZs: 98, Bias: -424.072257, T: 21473440, Avg. loss: 5439.504463\n",
      "Total training time: 7.21 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 923.37, NNZs: 98, Bias: -424.300667, T: 21681920, Avg. loss: 5399.142327\n",
      "Total training time: 7.28 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 921.74, NNZs: 98, Bias: -424.541412, T: 21890400, Avg. loss: 5313.918718\n",
      "Total training time: 7.35 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 919.91, NNZs: 98, Bias: -424.678201, T: 22098880, Avg. loss: 5282.251066\n",
      "Total training time: 7.42 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 918.16, NNZs: 98, Bias: -424.861776, T: 22307360, Avg. loss: 5236.793733\n",
      "Total training time: 7.49 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 916.44, NNZs: 98, Bias: -425.108227, T: 22515840, Avg. loss: 5175.313263\n",
      "Total training time: 7.56 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 914.68, NNZs: 98, Bias: -425.267309, T: 22724320, Avg. loss: 5143.110387\n",
      "Total training time: 7.64 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 912.98, NNZs: 98, Bias: -425.518691, T: 22932800, Avg. loss: 5067.988900\n",
      "Total training time: 7.71 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 911.02, NNZs: 98, Bias: -425.738478, T: 23141280, Avg. loss: 5023.731932\n",
      "Total training time: 7.78 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 909.50, NNZs: 98, Bias: -425.905492, T: 23349760, Avg. loss: 4978.182516\n",
      "Total training time: 7.86 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 908.01, NNZs: 98, Bias: -426.060081, T: 23558240, Avg. loss: 4966.042271\n",
      "Total training time: 7.94 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 906.45, NNZs: 98, Bias: -426.267515, T: 23766720, Avg. loss: 4900.635983\n",
      "Total training time: 8.02 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 904.92, NNZs: 98, Bias: -426.488083, T: 23975200, Avg. loss: 4882.387279\n",
      "Total training time: 8.09 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 903.31, NNZs: 98, Bias: -426.703627, T: 24183680, Avg. loss: 4804.100726\n",
      "Total training time: 8.15 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 901.71, NNZs: 98, Bias: -426.914345, T: 24392160, Avg. loss: 4770.238134\n",
      "Total training time: 8.22 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 900.20, NNZs: 98, Bias: -427.055527, T: 24600640, Avg. loss: 4748.194410\n",
      "Total training time: 8.28 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 898.86, NNZs: 98, Bias: -427.252160, T: 24809120, Avg. loss: 4686.500785\n",
      "Total training time: 8.35 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 897.40, NNZs: 98, Bias: -427.428712, T: 25017600, Avg. loss: 4644.835442\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 895.92, NNZs: 98, Bias: -427.562930, T: 25226080, Avg. loss: 4617.514688\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 894.67, NNZs: 98, Bias: -427.752807, T: 25434560, Avg. loss: 4586.349521\n",
      "Total training time: 8.54 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 893.33, NNZs: 98, Bias: -427.918398, T: 25643040, Avg. loss: 4537.693665\n",
      "Total training time: 8.61 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 891.90, NNZs: 98, Bias: -428.147933, T: 25851520, Avg. loss: 4494.225483\n",
      "Total training time: 8.67 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 890.67, NNZs: 98, Bias: -428.353516, T: 26060000, Avg. loss: 4454.897855\n",
      "Total training time: 8.74 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 889.29, NNZs: 98, Bias: -428.505214, T: 26268480, Avg. loss: 4432.160156\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 888.15, NNZs: 98, Bias: -428.639559, T: 26476960, Avg. loss: 4393.721362\n",
      "Total training time: 8.87 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 886.91, NNZs: 98, Bias: -428.804514, T: 26685440, Avg. loss: 4363.610861\n",
      "Total training time: 8.93 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 885.62, NNZs: 98, Bias: -429.012256, T: 26893920, Avg. loss: 4317.698883\n",
      "Total training time: 9.02 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 884.34, NNZs: 98, Bias: -429.201838, T: 27102400, Avg. loss: 4257.683457\n",
      "Total training time: 9.11 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 883.05, NNZs: 98, Bias: -429.364089, T: 27310880, Avg. loss: 4263.258941\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 881.77, NNZs: 98, Bias: -429.528903, T: 27519360, Avg. loss: 4222.693099\n",
      "Total training time: 9.26 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 880.45, NNZs: 98, Bias: -429.712023, T: 27727840, Avg. loss: 4197.292734\n",
      "Total training time: 9.33 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 879.26, NNZs: 98, Bias: -429.855815, T: 27936320, Avg. loss: 4143.632863\n",
      "Total training time: 9.41 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 878.15, NNZs: 98, Bias: -430.024413, T: 28144800, Avg. loss: 4135.778149\n",
      "Total training time: 9.48 seconds.\n",
      "-- Epoch 136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 877.07, NNZs: 98, Bias: -430.178669, T: 28353280, Avg. loss: 4106.756633\n",
      "Total training time: 9.55 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 875.72, NNZs: 98, Bias: -430.407663, T: 28561760, Avg. loss: 4052.329584\n",
      "Total training time: 9.62 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 874.61, NNZs: 98, Bias: -430.564963, T: 28770240, Avg. loss: 4039.558367\n",
      "Total training time: 9.70 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 873.41, NNZs: 98, Bias: -430.738870, T: 28978720, Avg. loss: 3986.902432\n",
      "Total training time: 9.78 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 872.37, NNZs: 98, Bias: -430.840818, T: 29187200, Avg. loss: 3976.039095\n",
      "Total training time: 9.86 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 871.15, NNZs: 98, Bias: -430.996355, T: 29395680, Avg. loss: 3951.511581\n",
      "Total training time: 9.93 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 869.90, NNZs: 98, Bias: -431.116075, T: 29604160, Avg. loss: 3894.321373\n",
      "Total training time: 10.00 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 869.00, NNZs: 98, Bias: -431.278429, T: 29812640, Avg. loss: 3865.423973\n",
      "Total training time: 10.07 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 867.72, NNZs: 98, Bias: -431.447945, T: 30021120, Avg. loss: 3860.260419\n",
      "Total training time: 10.14 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 866.65, NNZs: 98, Bias: -431.635174, T: 30229600, Avg. loss: 3856.010786\n",
      "Total training time: 10.20 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 865.55, NNZs: 98, Bias: -431.791737, T: 30438080, Avg. loss: 3792.296055\n",
      "Total training time: 10.27 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 864.56, NNZs: 98, Bias: -431.893322, T: 30646560, Avg. loss: 3774.617855\n",
      "Total training time: 10.34 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 863.47, NNZs: 98, Bias: -432.090378, T: 30855040, Avg. loss: 3734.085435\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 862.38, NNZs: 98, Bias: -432.249583, T: 31063520, Avg. loss: 3735.307678\n",
      "Total training time: 10.52 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 861.24, NNZs: 98, Bias: -432.424722, T: 31272000, Avg. loss: 3687.586631\n",
      "Total training time: 10.60 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 860.22, NNZs: 98, Bias: -432.581289, T: 31480480, Avg. loss: 3670.802687\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 859.21, NNZs: 98, Bias: -432.753085, T: 31688960, Avg. loss: 3668.030474\n",
      "Total training time: 10.76 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 858.25, NNZs: 98, Bias: -432.895309, T: 31897440, Avg. loss: 3619.199520\n",
      "Total training time: 10.83 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 857.33, NNZs: 98, Bias: -433.061908, T: 32105920, Avg. loss: 3596.962149\n",
      "Total training time: 10.89 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 856.36, NNZs: 98, Bias: -433.200640, T: 32314400, Avg. loss: 3559.479169\n",
      "Total training time: 10.96 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 855.41, NNZs: 98, Bias: -433.378618, T: 32522880, Avg. loss: 3555.583377\n",
      "Total training time: 11.03 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 854.35, NNZs: 98, Bias: -433.530955, T: 32731360, Avg. loss: 3541.700518\n",
      "Total training time: 11.09 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 853.34, NNZs: 98, Bias: -433.713656, T: 32939840, Avg. loss: 3519.284019\n",
      "Total training time: 11.16 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 852.36, NNZs: 98, Bias: -433.857319, T: 33148320, Avg. loss: 3470.570288\n",
      "Total training time: 11.22 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 851.44, NNZs: 98, Bias: -434.021773, T: 33356800, Avg. loss: 3463.668593\n",
      "Total training time: 11.29 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 850.59, NNZs: 98, Bias: -434.171817, T: 33565280, Avg. loss: 3455.684342\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 849.68, NNZs: 98, Bias: -434.340418, T: 33773760, Avg. loss: 3427.117875\n",
      "Total training time: 11.41 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 848.85, NNZs: 98, Bias: -434.525123, T: 33982240, Avg. loss: 3407.622813\n",
      "Total training time: 11.48 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 847.92, NNZs: 98, Bias: -434.675652, T: 34190720, Avg. loss: 3365.858433\n",
      "Total training time: 11.54 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 846.86, NNZs: 98, Bias: -434.867291, T: 34399200, Avg. loss: 3345.856927\n",
      "Total training time: 11.61 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 845.94, NNZs: 98, Bias: -435.011624, T: 34607680, Avg. loss: 3345.978105\n",
      "Total training time: 11.67 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 845.05, NNZs: 98, Bias: -435.137881, T: 34816160, Avg. loss: 3302.902922\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 844.20, NNZs: 98, Bias: -435.285686, T: 35024640, Avg. loss: 3296.891979\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 843.32, NNZs: 98, Bias: -435.460391, T: 35233120, Avg. loss: 3271.198667\n",
      "Total training time: 11.86 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 842.48, NNZs: 98, Bias: -435.602173, T: 35441600, Avg. loss: 3246.469556\n",
      "Total training time: 11.93 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 841.60, NNZs: 98, Bias: -435.745619, T: 35650080, Avg. loss: 3222.662140\n",
      "Total training time: 11.99 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 840.75, NNZs: 98, Bias: -435.916449, T: 35858560, Avg. loss: 3210.645581\n",
      "Total training time: 12.06 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 839.85, NNZs: 98, Bias: -436.069879, T: 36067040, Avg. loss: 3191.638413\n",
      "Total training time: 12.12 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 839.08, NNZs: 98, Bias: -436.204004, T: 36275520, Avg. loss: 3196.643777\n",
      "Total training time: 12.19 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 838.25, NNZs: 98, Bias: -436.347535, T: 36484000, Avg. loss: 3162.503182\n",
      "Total training time: 12.25 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 837.41, NNZs: 98, Bias: -436.467904, T: 36692480, Avg. loss: 3135.691700\n",
      "Total training time: 12.31 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 836.62, NNZs: 98, Bias: -436.625771, T: 36900960, Avg. loss: 3117.952594\n",
      "Total training time: 12.38 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 835.76, NNZs: 98, Bias: -436.777997, T: 37109440, Avg. loss: 3116.100401\n",
      "Total training time: 12.44 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 834.97, NNZs: 98, Bias: -436.883637, T: 37317920, Avg. loss: 3079.890558\n",
      "Total training time: 12.51 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 834.30, NNZs: 98, Bias: -437.030167, T: 37526400, Avg. loss: 3062.447951\n",
      "Total training time: 12.57 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 833.57, NNZs: 98, Bias: -437.182644, T: 37734880, Avg. loss: 3055.304106\n",
      "Total training time: 12.63 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 832.92, NNZs: 98, Bias: -437.320320, T: 37943360, Avg. loss: 3039.581577\n",
      "Total training time: 12.70 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 832.14, NNZs: 98, Bias: -437.435619, T: 38151840, Avg. loss: 3020.439354\n",
      "Total training time: 12.76 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 831.40, NNZs: 98, Bias: -437.563838, T: 38360320, Avg. loss: 2993.172792\n",
      "Total training time: 12.83 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 830.63, NNZs: 98, Bias: -437.694853, T: 38568800, Avg. loss: 2989.844490\n",
      "Total training time: 12.89 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 829.92, NNZs: 98, Bias: -437.824339, T: 38777280, Avg. loss: 2971.494210\n",
      "Total training time: 12.96 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 829.20, NNZs: 98, Bias: -437.961126, T: 38985760, Avg. loss: 2966.907966\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 828.45, NNZs: 98, Bias: -438.108936, T: 39194240, Avg. loss: 2931.671122\n",
      "Total training time: 13.08 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 827.73, NNZs: 98, Bias: -438.285330, T: 39402720, Avg. loss: 2908.563274\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 826.97, NNZs: 98, Bias: -438.452433, T: 39611200, Avg. loss: 2902.867058\n",
      "Total training time: 13.21 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 826.28, NNZs: 98, Bias: -438.541363, T: 39819680, Avg. loss: 2903.751822\n",
      "Total training time: 13.28 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 825.60, NNZs: 98, Bias: -438.663861, T: 40028160, Avg. loss: 2877.532083\n",
      "Total training time: 13.34 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 824.96, NNZs: 98, Bias: -438.793479, T: 40236640, Avg. loss: 2853.035191\n",
      "Total training time: 13.40 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 824.20, NNZs: 98, Bias: -438.924816, T: 40445120, Avg. loss: 2858.844330\n",
      "Total training time: 13.47 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 823.59, NNZs: 98, Bias: -439.070787, T: 40653600, Avg. loss: 2836.981438\n",
      "Total training time: 13.53 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 822.84, NNZs: 98, Bias: -439.174536, T: 40862080, Avg. loss: 2811.477395\n",
      "Total training time: 13.60 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 822.20, NNZs: 98, Bias: -439.274927, T: 41070560, Avg. loss: 2820.282634\n",
      "Total training time: 13.66 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 821.44, NNZs: 98, Bias: -439.398266, T: 41279040, Avg. loss: 2784.936210\n",
      "Total training time: 13.73 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 820.78, NNZs: 98, Bias: -439.536005, T: 41487520, Avg. loss: 2771.329931\n",
      "Total training time: 13.79 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 820.09, NNZs: 98, Bias: -439.667216, T: 41696000, Avg. loss: 2750.770731\n",
      "Total training time: 13.85 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 819.38, NNZs: 98, Bias: -439.800917, T: 41904480, Avg. loss: 2744.345131\n",
      "Total training time: 13.92 seconds.\n",
      "-- Epoch 202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 818.68, NNZs: 98, Bias: -439.898310, T: 42112960, Avg. loss: 2733.152039\n",
      "Total training time: 13.98 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 818.03, NNZs: 98, Bias: -440.015524, T: 42321440, Avg. loss: 2712.297165\n",
      "Total training time: 14.04 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 817.34, NNZs: 98, Bias: -440.165033, T: 42529920, Avg. loss: 2691.401509\n",
      "Total training time: 14.11 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 816.69, NNZs: 98, Bias: -440.318642, T: 42738400, Avg. loss: 2684.213794\n",
      "Total training time: 14.17 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 816.02, NNZs: 98, Bias: -440.401236, T: 42946880, Avg. loss: 2680.718650\n",
      "Total training time: 14.24 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 815.41, NNZs: 98, Bias: -440.578057, T: 43155360, Avg. loss: 2652.704655\n",
      "Total training time: 14.30 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 814.78, NNZs: 98, Bias: -440.677024, T: 43363840, Avg. loss: 2636.409164\n",
      "Total training time: 14.36 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 814.18, NNZs: 98, Bias: -440.801205, T: 43572320, Avg. loss: 2640.741922\n",
      "Total training time: 14.43 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 813.50, NNZs: 98, Bias: -440.903811, T: 43780800, Avg. loss: 2631.134595\n",
      "Total training time: 14.49 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 812.93, NNZs: 98, Bias: -441.040803, T: 43989280, Avg. loss: 2610.588437\n",
      "Total training time: 14.56 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 812.25, NNZs: 98, Bias: -441.148091, T: 44197760, Avg. loss: 2590.489248\n",
      "Total training time: 14.62 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 811.60, NNZs: 98, Bias: -441.296668, T: 44406240, Avg. loss: 2585.224185\n",
      "Total training time: 14.71 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 810.93, NNZs: 98, Bias: -441.453029, T: 44614720, Avg. loss: 2563.708401\n",
      "Total training time: 14.79 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 810.32, NNZs: 98, Bias: -441.585122, T: 44823200, Avg. loss: 2562.390494\n",
      "Total training time: 14.87 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 809.74, NNZs: 98, Bias: -441.699066, T: 45031680, Avg. loss: 2553.631888\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 809.12, NNZs: 98, Bias: -441.795293, T: 45240160, Avg. loss: 2531.996581\n",
      "Total training time: 15.03 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 808.57, NNZs: 98, Bias: -441.891704, T: 45448640, Avg. loss: 2529.637809\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 807.89, NNZs: 98, Bias: -442.020116, T: 45657120, Avg. loss: 2491.707606\n",
      "Total training time: 15.17 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 807.35, NNZs: 98, Bias: -442.115107, T: 45865600, Avg. loss: 2486.397680\n",
      "Total training time: 15.23 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 806.74, NNZs: 98, Bias: -442.261311, T: 46074080, Avg. loss: 2484.548538\n",
      "Total training time: 15.30 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 806.15, NNZs: 98, Bias: -442.391491, T: 46282560, Avg. loss: 2476.573868\n",
      "Total training time: 15.36 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 805.49, NNZs: 98, Bias: -442.535552, T: 46491040, Avg. loss: 2458.320109\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 804.88, NNZs: 98, Bias: -442.676760, T: 46699520, Avg. loss: 2441.050846\n",
      "Total training time: 15.50 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 804.28, NNZs: 98, Bias: -442.820996, T: 46908000, Avg. loss: 2445.739592\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 803.70, NNZs: 98, Bias: -442.934806, T: 47116480, Avg. loss: 2427.737069\n",
      "Total training time: 15.63 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 803.10, NNZs: 98, Bias: -443.037921, T: 47324960, Avg. loss: 2420.381040\n",
      "Total training time: 15.70 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 802.55, NNZs: 98, Bias: -443.127295, T: 47533440, Avg. loss: 2406.302052\n",
      "Total training time: 15.77 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 802.02, NNZs: 98, Bias: -443.251108, T: 47741920, Avg. loss: 2393.041167\n",
      "Total training time: 15.84 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 801.42, NNZs: 98, Bias: -443.363729, T: 47950400, Avg. loss: 2397.823709\n",
      "Total training time: 15.91 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 800.84, NNZs: 98, Bias: -443.474420, T: 48158880, Avg. loss: 2381.532828\n",
      "Total training time: 15.98 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 800.23, NNZs: 98, Bias: -443.608122, T: 48367360, Avg. loss: 2363.899391\n",
      "Total training time: 16.05 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 799.68, NNZs: 98, Bias: -443.730869, T: 48575840, Avg. loss: 2358.304449\n",
      "Total training time: 16.12 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 799.16, NNZs: 98, Bias: -443.847564, T: 48784320, Avg. loss: 2343.768349\n",
      "Total training time: 16.19 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 798.63, NNZs: 98, Bias: -443.979095, T: 48992800, Avg. loss: 2346.263714\n",
      "Total training time: 16.26 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 798.02, NNZs: 98, Bias: -444.091910, T: 49201280, Avg. loss: 2317.020514\n",
      "Total training time: 16.33 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 797.45, NNZs: 98, Bias: -444.249527, T: 49409760, Avg. loss: 2311.588511\n",
      "Total training time: 16.40 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 796.91, NNZs: 98, Bias: -444.372577, T: 49618240, Avg. loss: 2297.086947\n",
      "Total training time: 16.47 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 796.39, NNZs: 98, Bias: -444.486583, T: 49826720, Avg. loss: 2286.732896\n",
      "Total training time: 16.54 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 795.85, NNZs: 98, Bias: -444.584331, T: 50035200, Avg. loss: 2284.084811\n",
      "Total training time: 16.61 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 795.38, NNZs: 98, Bias: -444.664540, T: 50243680, Avg. loss: 2275.376070\n",
      "Total training time: 16.68 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 794.82, NNZs: 98, Bias: -444.772937, T: 50452160, Avg. loss: 2262.985172\n",
      "Total training time: 16.75 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 794.29, NNZs: 98, Bias: -444.862162, T: 50660640, Avg. loss: 2256.594669\n",
      "Total training time: 16.82 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 793.72, NNZs: 98, Bias: -444.990733, T: 50869120, Avg. loss: 2246.958441\n",
      "Total training time: 16.89 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 793.14, NNZs: 98, Bias: -445.070407, T: 51077600, Avg. loss: 2221.871170\n",
      "Total training time: 16.96 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 792.59, NNZs: 98, Bias: -445.165777, T: 51286080, Avg. loss: 2222.642926\n",
      "Total training time: 17.03 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 792.05, NNZs: 98, Bias: -445.282912, T: 51494560, Avg. loss: 2215.854250\n",
      "Total training time: 17.10 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 791.53, NNZs: 98, Bias: -445.388143, T: 51703040, Avg. loss: 2200.816825\n",
      "Total training time: 17.17 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 791.00, NNZs: 98, Bias: -445.521172, T: 51911520, Avg. loss: 2183.188064\n",
      "Total training time: 17.24 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 790.45, NNZs: 98, Bias: -445.648280, T: 52120000, Avg. loss: 2178.270179\n",
      "Total training time: 17.31 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 789.94, NNZs: 98, Bias: -445.754375, T: 52328480, Avg. loss: 2189.955037\n",
      "Total training time: 17.38 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 789.41, NNZs: 98, Bias: -445.890505, T: 52536960, Avg. loss: 2167.136005\n",
      "Total training time: 17.44 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 788.83, NNZs: 98, Bias: -445.975956, T: 52745440, Avg. loss: 2151.035530\n",
      "Total training time: 17.51 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 788.33, NNZs: 98, Bias: -446.053738, T: 52953920, Avg. loss: 2158.070129\n",
      "Total training time: 17.57 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 787.79, NNZs: 98, Bias: -446.166636, T: 53162400, Avg. loss: 2149.855783\n",
      "Total training time: 17.64 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 787.23, NNZs: 98, Bias: -446.255631, T: 53370880, Avg. loss: 2117.156720\n",
      "Total training time: 17.70 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 786.68, NNZs: 98, Bias: -446.370603, T: 53579360, Avg. loss: 2116.901397\n",
      "Total training time: 17.76 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 786.20, NNZs: 98, Bias: -446.484061, T: 53787840, Avg. loss: 2121.086717\n",
      "Total training time: 17.83 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 785.68, NNZs: 98, Bias: -446.572740, T: 53996320, Avg. loss: 2108.514729\n",
      "Total training time: 17.89 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 785.12, NNZs: 98, Bias: -446.698658, T: 54204800, Avg. loss: 2102.168069\n",
      "Total training time: 17.95 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 784.59, NNZs: 98, Bias: -446.805657, T: 54413280, Avg. loss: 2084.756428\n",
      "Total training time: 18.02 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 784.14, NNZs: 98, Bias: -446.914778, T: 54621760, Avg. loss: 2087.463380\n",
      "Total training time: 18.08 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 783.64, NNZs: 98, Bias: -447.034342, T: 54830240, Avg. loss: 2080.877862\n",
      "Total training time: 18.14 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 783.16, NNZs: 98, Bias: -447.117157, T: 55038720, Avg. loss: 2081.181329\n",
      "Total training time: 18.21 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 782.68, NNZs: 98, Bias: -447.166478, T: 55247200, Avg. loss: 2069.538068\n",
      "Total training time: 18.27 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 782.13, NNZs: 98, Bias: -447.275815, T: 55455680, Avg. loss: 2058.441385\n",
      "Total training time: 18.33 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 781.68, NNZs: 98, Bias: -447.359203, T: 55664160, Avg. loss: 2052.344279\n",
      "Total training time: 18.40 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 781.20, NNZs: 98, Bias: -447.462105, T: 55872640, Avg. loss: 2043.498191\n",
      "Total training time: 18.46 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 780.74, NNZs: 98, Bias: -447.562836, T: 56081120, Avg. loss: 2042.061232\n",
      "Total training time: 18.52 seconds.\n",
      "-- Epoch 270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 780.22, NNZs: 98, Bias: -447.645083, T: 56289600, Avg. loss: 2019.576943\n",
      "Total training time: 18.59 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 779.76, NNZs: 98, Bias: -447.726832, T: 56498080, Avg. loss: 2026.391510\n",
      "Total training time: 18.65 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 779.32, NNZs: 98, Bias: -447.835097, T: 56706560, Avg. loss: 2013.269012\n",
      "Total training time: 18.72 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 778.86, NNZs: 98, Bias: -447.935987, T: 56915040, Avg. loss: 1994.525728\n",
      "Total training time: 18.78 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 778.36, NNZs: 98, Bias: -448.029480, T: 57123520, Avg. loss: 1991.699042\n",
      "Total training time: 18.84 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 777.89, NNZs: 98, Bias: -448.135912, T: 57332000, Avg. loss: 1988.628167\n",
      "Total training time: 18.91 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 777.47, NNZs: 98, Bias: -448.234084, T: 57540480, Avg. loss: 1979.298597\n",
      "Total training time: 18.97 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 777.01, NNZs: 98, Bias: -448.332821, T: 57748960, Avg. loss: 1975.479421\n",
      "Total training time: 19.03 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 776.51, NNZs: 98, Bias: -448.427359, T: 57957440, Avg. loss: 1961.310266\n",
      "Total training time: 19.10 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 776.03, NNZs: 98, Bias: -448.504193, T: 58165920, Avg. loss: 1965.770684\n",
      "Total training time: 19.16 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 775.55, NNZs: 98, Bias: -448.578324, T: 58374400, Avg. loss: 1950.402123\n",
      "Total training time: 19.23 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 775.06, NNZs: 98, Bias: -448.698880, T: 58582880, Avg. loss: 1934.701064\n",
      "Total training time: 19.29 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 774.60, NNZs: 98, Bias: -448.783240, T: 58791360, Avg. loss: 1944.162786\n",
      "Total training time: 19.35 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 774.13, NNZs: 98, Bias: -448.868153, T: 58999840, Avg. loss: 1926.889914\n",
      "Total training time: 19.42 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 773.65, NNZs: 98, Bias: -448.959319, T: 59208320, Avg. loss: 1915.228127\n",
      "Total training time: 19.48 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 773.15, NNZs: 98, Bias: -449.074078, T: 59416800, Avg. loss: 1912.170787\n",
      "Total training time: 19.54 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 772.73, NNZs: 98, Bias: -449.166310, T: 59625280, Avg. loss: 1906.546004\n",
      "Total training time: 19.61 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 772.27, NNZs: 98, Bias: -449.269280, T: 59833760, Avg. loss: 1882.402752\n",
      "Total training time: 19.67 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 771.80, NNZs: 98, Bias: -449.376931, T: 60042240, Avg. loss: 1886.350189\n",
      "Total training time: 19.73 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 771.36, NNZs: 98, Bias: -449.474717, T: 60250720, Avg. loss: 1889.178583\n",
      "Total training time: 19.80 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 770.93, NNZs: 98, Bias: -449.575797, T: 60459200, Avg. loss: 1875.659828\n",
      "Total training time: 19.86 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 770.49, NNZs: 98, Bias: -449.651422, T: 60667680, Avg. loss: 1864.035474\n",
      "Total training time: 19.92 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 770.05, NNZs: 98, Bias: -449.733179, T: 60876160, Avg. loss: 1864.047541\n",
      "Total training time: 19.99 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 769.62, NNZs: 98, Bias: -449.819420, T: 61084640, Avg. loss: 1862.208704\n",
      "Total training time: 20.05 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 769.21, NNZs: 98, Bias: -449.882673, T: 61293120, Avg. loss: 1851.948954\n",
      "Total training time: 20.11 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 768.78, NNZs: 98, Bias: -449.970966, T: 61501600, Avg. loss: 1850.119015\n",
      "Total training time: 20.18 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 768.35, NNZs: 98, Bias: -450.034773, T: 61710080, Avg. loss: 1834.272229\n",
      "Total training time: 20.24 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 767.93, NNZs: 98, Bias: -450.120837, T: 61918560, Avg. loss: 1816.293146\n",
      "Total training time: 20.30 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 767.53, NNZs: 98, Bias: -450.223684, T: 62127040, Avg. loss: 1821.300867\n",
      "Total training time: 20.37 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 767.14, NNZs: 98, Bias: -450.323269, T: 62335520, Avg. loss: 1826.339711\n",
      "Total training time: 20.43 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 766.71, NNZs: 98, Bias: -450.435246, T: 62544000, Avg. loss: 1815.522438\n",
      "Total training time: 20.49 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 766.29, NNZs: 98, Bias: -450.542656, T: 62752480, Avg. loss: 1812.453148\n",
      "Total training time: 20.56 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 765.89, NNZs: 98, Bias: -450.644647, T: 62960960, Avg. loss: 1794.356634\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 765.42, NNZs: 98, Bias: -450.738333, T: 63169440, Avg. loss: 1798.442136\n",
      "Total training time: 20.69 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 765.00, NNZs: 98, Bias: -450.830781, T: 63377920, Avg. loss: 1787.978172\n",
      "Total training time: 20.75 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 764.57, NNZs: 98, Bias: -450.912714, T: 63586400, Avg. loss: 1791.332505\n",
      "Total training time: 20.81 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 764.12, NNZs: 98, Bias: -450.987634, T: 63794880, Avg. loss: 1775.856392\n",
      "Total training time: 20.88 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 763.65, NNZs: 98, Bias: -451.077431, T: 64003360, Avg. loss: 1771.074753\n",
      "Total training time: 20.94 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 763.30, NNZs: 98, Bias: -451.163228, T: 64211840, Avg. loss: 1761.137090\n",
      "Total training time: 21.00 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 762.85, NNZs: 98, Bias: -451.266474, T: 64420320, Avg. loss: 1758.294911\n",
      "Total training time: 21.06 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 762.43, NNZs: 98, Bias: -451.358376, T: 64628800, Avg. loss: 1757.682207\n",
      "Total training time: 21.13 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 762.05, NNZs: 98, Bias: -451.424655, T: 64837280, Avg. loss: 1762.113173\n",
      "Total training time: 21.19 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 761.64, NNZs: 98, Bias: -451.504084, T: 65045760, Avg. loss: 1747.261264\n",
      "Total training time: 21.26 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 761.17, NNZs: 98, Bias: -451.596030, T: 65254240, Avg. loss: 1730.095488\n",
      "Total training time: 21.32 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 760.74, NNZs: 98, Bias: -451.671917, T: 65462720, Avg. loss: 1729.907443\n",
      "Total training time: 21.39 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 760.29, NNZs: 98, Bias: -451.769361, T: 65671200, Avg. loss: 1726.641513\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 759.83, NNZs: 98, Bias: -451.854502, T: 65879680, Avg. loss: 1715.187843\n",
      "Total training time: 21.51 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 759.44, NNZs: 98, Bias: -451.920251, T: 66088160, Avg. loss: 1718.668444\n",
      "Total training time: 21.57 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 759.04, NNZs: 98, Bias: -452.015867, T: 66296640, Avg. loss: 1713.560451\n",
      "Total training time: 21.64 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 758.65, NNZs: 98, Bias: -452.114190, T: 66505120, Avg. loss: 1704.351980\n",
      "Total training time: 21.70 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 758.21, NNZs: 98, Bias: -452.207700, T: 66713600, Avg. loss: 1685.653555\n",
      "Total training time: 21.76 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 757.81, NNZs: 98, Bias: -452.277595, T: 66922080, Avg. loss: 1695.282167\n",
      "Total training time: 21.83 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 757.41, NNZs: 98, Bias: -452.361118, T: 67130560, Avg. loss: 1692.241979\n",
      "Total training time: 21.90 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 757.02, NNZs: 98, Bias: -452.438772, T: 67339040, Avg. loss: 1677.757343\n",
      "Total training time: 21.98 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 756.62, NNZs: 98, Bias: -452.521066, T: 67547520, Avg. loss: 1674.970154\n",
      "Total training time: 22.07 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 756.22, NNZs: 98, Bias: -452.614487, T: 67756000, Avg. loss: 1660.337120\n",
      "Total training time: 22.15 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 755.87, NNZs: 98, Bias: -452.693161, T: 67964480, Avg. loss: 1663.341631\n",
      "Total training time: 22.23 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 755.46, NNZs: 98, Bias: -452.773373, T: 68172960, Avg. loss: 1649.967924\n",
      "Total training time: 22.30 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 755.06, NNZs: 98, Bias: -452.871791, T: 68381440, Avg. loss: 1652.301691\n",
      "Total training time: 22.38 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 754.63, NNZs: 98, Bias: -452.964510, T: 68589920, Avg. loss: 1637.583022\n",
      "Total training time: 22.46 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 754.25, NNZs: 98, Bias: -453.055515, T: 68798400, Avg. loss: 1641.267266\n",
      "Total training time: 22.54 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 753.83, NNZs: 98, Bias: -453.135324, T: 69006880, Avg. loss: 1638.170510\n",
      "Total training time: 22.62 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 753.42, NNZs: 98, Bias: -453.204034, T: 69215360, Avg. loss: 1639.066053\n",
      "Total training time: 22.69 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 753.06, NNZs: 98, Bias: -453.274420, T: 69423840, Avg. loss: 1630.460493\n",
      "Total training time: 22.76 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 752.69, NNZs: 98, Bias: -453.373803, T: 69632320, Avg. loss: 1633.578996\n",
      "Total training time: 22.82 seconds.\n",
      "-- Epoch 335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 752.28, NNZs: 98, Bias: -453.453102, T: 69840800, Avg. loss: 1623.443704\n",
      "Total training time: 22.89 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 751.92, NNZs: 98, Bias: -453.533280, T: 70049280, Avg. loss: 1615.369081\n",
      "Total training time: 22.95 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 751.55, NNZs: 98, Bias: -453.627482, T: 70257760, Avg. loss: 1606.036461\n",
      "Total training time: 23.04 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 751.20, NNZs: 98, Bias: -453.700660, T: 70466240, Avg. loss: 1608.184930\n",
      "Total training time: 23.12 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 750.82, NNZs: 98, Bias: -453.764140, T: 70674720, Avg. loss: 1594.336583\n",
      "Total training time: 23.20 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 750.45, NNZs: 98, Bias: -453.846507, T: 70883200, Avg. loss: 1598.602275\n",
      "Total training time: 23.27 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 750.07, NNZs: 98, Bias: -453.919747, T: 71091680, Avg. loss: 1580.726338\n",
      "Total training time: 23.33 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 749.72, NNZs: 98, Bias: -453.994617, T: 71300160, Avg. loss: 1574.426822\n",
      "Total training time: 23.40 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 749.33, NNZs: 98, Bias: -454.067702, T: 71508640, Avg. loss: 1578.376055\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 748.96, NNZs: 98, Bias: -454.143250, T: 71717120, Avg. loss: 1579.827276\n",
      "Total training time: 23.54 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 748.58, NNZs: 98, Bias: -454.212304, T: 71925600, Avg. loss: 1573.902567\n",
      "Total training time: 23.62 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 748.21, NNZs: 98, Bias: -454.298926, T: 72134080, Avg. loss: 1566.519025\n",
      "Total training time: 23.68 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 747.85, NNZs: 98, Bias: -454.363708, T: 72342560, Avg. loss: 1558.566453\n",
      "Total training time: 23.75 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 747.49, NNZs: 98, Bias: -454.436022, T: 72551040, Avg. loss: 1552.436658\n",
      "Total training time: 23.82 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 747.09, NNZs: 98, Bias: -454.520098, T: 72759520, Avg. loss: 1548.673220\n",
      "Total training time: 23.92 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 746.74, NNZs: 98, Bias: -454.593117, T: 72968000, Avg. loss: 1555.509506\n",
      "Total training time: 24.00 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 746.37, NNZs: 98, Bias: -454.651253, T: 73176480, Avg. loss: 1540.479058\n",
      "Total training time: 24.07 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 746.02, NNZs: 98, Bias: -454.722916, T: 73384960, Avg. loss: 1541.476245\n",
      "Total training time: 24.13 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 745.66, NNZs: 98, Bias: -454.791086, T: 73593440, Avg. loss: 1535.595368\n",
      "Total training time: 24.22 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 745.25, NNZs: 98, Bias: -454.876441, T: 73801920, Avg. loss: 1525.338539\n",
      "Total training time: 24.32 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 744.88, NNZs: 98, Bias: -454.938694, T: 74010400, Avg. loss: 1524.986205\n",
      "Total training time: 24.38 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 744.48, NNZs: 98, Bias: -455.015051, T: 74218880, Avg. loss: 1518.601344\n",
      "Total training time: 24.45 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 744.15, NNZs: 98, Bias: -455.066853, T: 74427360, Avg. loss: 1522.098111\n",
      "Total training time: 24.53 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 743.73, NNZs: 98, Bias: -455.144803, T: 74635840, Avg. loss: 1505.320697\n",
      "Total training time: 24.63 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 743.38, NNZs: 98, Bias: -455.191773, T: 74844320, Avg. loss: 1502.413287\n",
      "Total training time: 24.71 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 743.03, NNZs: 98, Bias: -455.259813, T: 75052800, Avg. loss: 1505.265309\n",
      "Total training time: 24.77 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 742.66, NNZs: 98, Bias: -455.320108, T: 75261280, Avg. loss: 1499.288400\n",
      "Total training time: 24.84 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 742.29, NNZs: 98, Bias: -455.405159, T: 75469760, Avg. loss: 1490.537328\n",
      "Total training time: 24.93 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 741.92, NNZs: 98, Bias: -455.477554, T: 75678240, Avg. loss: 1487.531811\n",
      "Total training time: 25.03 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 741.58, NNZs: 98, Bias: -455.538009, T: 75886720, Avg. loss: 1484.440637\n",
      "Total training time: 25.10 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 741.24, NNZs: 98, Bias: -455.566832, T: 76095200, Avg. loss: 1475.656760\n",
      "Total training time: 25.17 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 740.91, NNZs: 98, Bias: -455.627340, T: 76303680, Avg. loss: 1472.378615\n",
      "Total training time: 25.24 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 740.58, NNZs: 98, Bias: -455.701704, T: 76512160, Avg. loss: 1468.896617\n",
      "Total training time: 25.34 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 740.23, NNZs: 98, Bias: -455.756134, T: 76720640, Avg. loss: 1471.489450\n",
      "Total training time: 25.42 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 739.87, NNZs: 98, Bias: -455.828768, T: 76929120, Avg. loss: 1462.476236\n",
      "Total training time: 25.48 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 739.51, NNZs: 98, Bias: -455.908214, T: 77137600, Avg. loss: 1458.438225\n",
      "Total training time: 25.55 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 739.17, NNZs: 98, Bias: -456.001297, T: 77346080, Avg. loss: 1456.314248\n",
      "Total training time: 25.64 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 738.80, NNZs: 98, Bias: -456.089742, T: 77554560, Avg. loss: 1449.810596\n",
      "Total training time: 25.73 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 738.43, NNZs: 98, Bias: -456.139582, T: 77763040, Avg. loss: 1444.638950\n",
      "Total training time: 25.81 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 738.08, NNZs: 98, Bias: -456.206605, T: 77971520, Avg. loss: 1449.050854\n",
      "Total training time: 25.88 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 737.72, NNZs: 98, Bias: -456.284616, T: 78180000, Avg. loss: 1432.644147\n",
      "Total training time: 25.95 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 737.37, NNZs: 98, Bias: -456.360761, T: 78388480, Avg. loss: 1443.124631\n",
      "Total training time: 26.04 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 737.04, NNZs: 98, Bias: -456.431229, T: 78596960, Avg. loss: 1434.375632\n",
      "Total training time: 26.14 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 736.66, NNZs: 98, Bias: -456.493766, T: 78805440, Avg. loss: 1425.235174\n",
      "Total training time: 26.20 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 736.35, NNZs: 98, Bias: -456.561674, T: 79013920, Avg. loss: 1424.769355\n",
      "Total training time: 26.26 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 735.99, NNZs: 98, Bias: -456.607039, T: 79222400, Avg. loss: 1424.211831\n",
      "Total training time: 26.33 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 735.63, NNZs: 98, Bias: -456.662742, T: 79430880, Avg. loss: 1417.374570\n",
      "Total training time: 26.44 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 735.31, NNZs: 98, Bias: -456.740330, T: 79639360, Avg. loss: 1416.153678\n",
      "Total training time: 26.53 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 734.99, NNZs: 98, Bias: -456.784375, T: 79847840, Avg. loss: 1412.922306\n",
      "Total training time: 26.60 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 734.66, NNZs: 98, Bias: -456.840904, T: 80056320, Avg. loss: 1410.256835\n",
      "Total training time: 26.67 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 734.34, NNZs: 98, Bias: -456.891430, T: 80264800, Avg. loss: 1400.328376\n",
      "Total training time: 26.75 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 734.02, NNZs: 98, Bias: -456.963095, T: 80473280, Avg. loss: 1406.016421\n",
      "Total training time: 26.84 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 733.71, NNZs: 98, Bias: -457.041512, T: 80681760, Avg. loss: 1390.966205\n",
      "Total training time: 26.93 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 733.36, NNZs: 98, Bias: -457.116023, T: 80890240, Avg. loss: 1398.693087\n",
      "Total training time: 26.99 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 733.04, NNZs: 98, Bias: -457.171197, T: 81098720, Avg. loss: 1382.305231\n",
      "Total training time: 27.05 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 732.70, NNZs: 98, Bias: -457.253086, T: 81307200, Avg. loss: 1379.547759\n",
      "Total training time: 27.15 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 732.40, NNZs: 98, Bias: -457.315360, T: 81515680, Avg. loss: 1391.962736\n",
      "Total training time: 27.24 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 732.05, NNZs: 98, Bias: -457.360563, T: 81724160, Avg. loss: 1371.130826\n",
      "Total training time: 27.30 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 731.72, NNZs: 98, Bias: -457.404058, T: 81932640, Avg. loss: 1373.750932\n",
      "Total training time: 27.37 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 731.37, NNZs: 98, Bias: -457.448792, T: 82141120, Avg. loss: 1360.141531\n",
      "Total training time: 27.46 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 731.03, NNZs: 98, Bias: -457.511168, T: 82349600, Avg. loss: 1361.723184\n",
      "Total training time: 27.55 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 730.72, NNZs: 98, Bias: -457.573152, T: 82558080, Avg. loss: 1362.328712\n",
      "Total training time: 27.63 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 730.37, NNZs: 98, Bias: -457.637987, T: 82766560, Avg. loss: 1348.697923\n",
      "Total training time: 27.69 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 730.04, NNZs: 98, Bias: -457.724498, T: 82975040, Avg. loss: 1347.747695\n",
      "Total training time: 27.76 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 729.71, NNZs: 98, Bias: -457.780246, T: 83183520, Avg. loss: 1351.308547\n",
      "Total training time: 27.86 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 729.39, NNZs: 98, Bias: -457.836086, T: 83392000, Avg. loss: 1344.438052\n",
      "Total training time: 27.93 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 729.02, NNZs: 98, Bias: -457.905550, T: 83600480, Avg. loss: 1343.813934\n",
      "Total training time: 27.99 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 728.72, NNZs: 98, Bias: -457.945677, T: 83808960, Avg. loss: 1340.385631\n",
      "Total training time: 28.06 seconds.\n",
      "-- Epoch 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 728.39, NNZs: 98, Bias: -458.020666, T: 84017440, Avg. loss: 1339.466473\n",
      "Total training time: 28.16 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 728.05, NNZs: 98, Bias: -458.082238, T: 84225920, Avg. loss: 1325.519626\n",
      "Total training time: 28.23 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 727.70, NNZs: 98, Bias: -458.152445, T: 84434400, Avg. loss: 1325.053833\n",
      "Total training time: 28.30 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 727.41, NNZs: 98, Bias: -458.191254, T: 84642880, Avg. loss: 1321.682988\n",
      "Total training time: 28.38 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 727.09, NNZs: 98, Bias: -458.260273, T: 84851360, Avg. loss: 1319.176070\n",
      "Total training time: 28.47 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 726.80, NNZs: 98, Bias: -458.329963, T: 85059840, Avg. loss: 1316.892009\n",
      "Total training time: 28.54 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 726.46, NNZs: 98, Bias: -458.385627, T: 85268320, Avg. loss: 1315.065752\n",
      "Total training time: 28.61 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 726.12, NNZs: 98, Bias: -458.457300, T: 85476800, Avg. loss: 1309.688685\n",
      "Total training time: 28.71 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 725.80, NNZs: 98, Bias: -458.509777, T: 85685280, Avg. loss: 1313.327289\n",
      "Total training time: 28.78 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 725.46, NNZs: 98, Bias: -458.583211, T: 85893760, Avg. loss: 1304.327390\n",
      "Total training time: 28.85 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 725.16, NNZs: 98, Bias: -458.640895, T: 86102240, Avg. loss: 1301.902141\n",
      "Total training time: 28.93 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 724.86, NNZs: 98, Bias: -458.688679, T: 86310720, Avg. loss: 1305.596655\n",
      "Total training time: 29.03 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 724.54, NNZs: 98, Bias: -458.747806, T: 86519200, Avg. loss: 1293.965423\n",
      "Total training time: 29.10 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 724.24, NNZs: 98, Bias: -458.798484, T: 86727680, Avg. loss: 1290.277856\n",
      "Total training time: 29.16 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 723.91, NNZs: 98, Bias: -458.840956, T: 86936160, Avg. loss: 1282.840993\n",
      "Total training time: 29.25 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 723.57, NNZs: 98, Bias: -458.890150, T: 87144640, Avg. loss: 1278.570067\n",
      "Total training time: 29.34 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 723.26, NNZs: 98, Bias: -458.942890, T: 87353120, Avg. loss: 1282.212266\n",
      "Total training time: 29.40 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 722.96, NNZs: 98, Bias: -459.001684, T: 87561600, Avg. loss: 1278.495804\n",
      "Total training time: 29.47 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 722.63, NNZs: 98, Bias: -459.060196, T: 87770080, Avg. loss: 1270.362662\n",
      "Total training time: 29.56 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 722.34, NNZs: 98, Bias: -459.104806, T: 87978560, Avg. loss: 1275.210207\n",
      "Total training time: 29.64 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 722.03, NNZs: 98, Bias: -459.159525, T: 88187040, Avg. loss: 1275.440975\n",
      "Total training time: 29.71 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 721.73, NNZs: 98, Bias: -459.199856, T: 88395520, Avg. loss: 1266.931013\n",
      "Total training time: 29.78 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 721.40, NNZs: 98, Bias: -459.251716, T: 88604000, Avg. loss: 1264.672672\n",
      "Total training time: 29.87 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 721.09, NNZs: 98, Bias: -459.322395, T: 88812480, Avg. loss: 1255.563566\n",
      "Total training time: 29.96 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 720.75, NNZs: 98, Bias: -459.386510, T: 89020960, Avg. loss: 1256.778164\n",
      "Total training time: 30.03 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 720.44, NNZs: 98, Bias: -459.440268, T: 89229440, Avg. loss: 1253.274109\n",
      "Total training time: 30.09 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 720.13, NNZs: 98, Bias: -459.486061, T: 89437920, Avg. loss: 1250.977768\n",
      "Total training time: 30.19 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 719.84, NNZs: 98, Bias: -459.546266, T: 89646400, Avg. loss: 1247.013051\n",
      "Total training time: 30.27 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 719.55, NNZs: 98, Bias: -459.608547, T: 89854880, Avg. loss: 1249.891280\n",
      "Total training time: 30.34 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 719.26, NNZs: 98, Bias: -459.662470, T: 90063360, Avg. loss: 1245.309137\n",
      "Total training time: 30.40 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 718.95, NNZs: 98, Bias: -459.721360, T: 90271840, Avg. loss: 1238.750642\n",
      "Total training time: 30.49 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 718.64, NNZs: 98, Bias: -459.784323, T: 90480320, Avg. loss: 1238.539212\n",
      "Total training time: 30.58 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 718.35, NNZs: 98, Bias: -459.849893, T: 90688800, Avg. loss: 1237.371284\n",
      "Total training time: 30.65 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 718.04, NNZs: 98, Bias: -459.891967, T: 90897280, Avg. loss: 1227.850988\n",
      "Total training time: 30.72 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 717.75, NNZs: 98, Bias: -459.962296, T: 91105760, Avg. loss: 1225.724645\n",
      "Total training time: 30.81 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 717.46, NNZs: 98, Bias: -460.013164, T: 91314240, Avg. loss: 1218.029278\n",
      "Total training time: 30.89 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 717.16, NNZs: 98, Bias: -460.062491, T: 91522720, Avg. loss: 1225.124597\n",
      "Total training time: 30.96 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 716.88, NNZs: 98, Bias: -460.111503, T: 91731200, Avg. loss: 1215.142086\n",
      "Total training time: 31.03 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 716.58, NNZs: 98, Bias: -460.147325, T: 91939680, Avg. loss: 1215.758585\n",
      "Total training time: 31.13 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 716.28, NNZs: 98, Bias: -460.201955, T: 92148160, Avg. loss: 1211.610898\n",
      "Total training time: 31.21 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 715.98, NNZs: 98, Bias: -460.250517, T: 92356640, Avg. loss: 1209.802754\n",
      "Total training time: 31.27 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 715.65, NNZs: 98, Bias: -460.286102, T: 92565120, Avg. loss: 1205.000152\n",
      "Total training time: 31.35 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 715.34, NNZs: 98, Bias: -460.348606, T: 92773600, Avg. loss: 1203.433238\n",
      "Total training time: 31.45 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 715.03, NNZs: 98, Bias: -460.395235, T: 92982080, Avg. loss: 1206.005722\n",
      "Total training time: 31.51 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 714.73, NNZs: 98, Bias: -460.445618, T: 93190560, Avg. loss: 1201.004662\n",
      "Total training time: 31.58 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 714.43, NNZs: 98, Bias: -460.518182, T: 93399040, Avg. loss: 1196.983679\n",
      "Total training time: 31.66 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 714.15, NNZs: 98, Bias: -460.564706, T: 93607520, Avg. loss: 1195.258496\n",
      "Total training time: 31.75 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 713.84, NNZs: 98, Bias: -460.625643, T: 93816000, Avg. loss: 1191.429773\n",
      "Total training time: 31.82 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 713.53, NNZs: 98, Bias: -460.688995, T: 94024480, Avg. loss: 1192.501836\n",
      "Total training time: 31.89 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 713.22, NNZs: 98, Bias: -460.748797, T: 94232960, Avg. loss: 1177.646201\n",
      "Total training time: 31.96 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 712.91, NNZs: 98, Bias: -460.799347, T: 94441440, Avg. loss: 1178.656470\n",
      "Total training time: 32.06 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 712.64, NNZs: 98, Bias: -460.852134, T: 94649920, Avg. loss: 1179.143017\n",
      "Total training time: 32.13 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 712.36, NNZs: 98, Bias: -460.913861, T: 94858400, Avg. loss: 1182.361516\n",
      "Total training time: 32.20 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 712.08, NNZs: 98, Bias: -460.960716, T: 95066880, Avg. loss: 1177.303043\n",
      "Total training time: 32.26 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 711.81, NNZs: 98, Bias: -461.012424, T: 95275360, Avg. loss: 1169.949658\n",
      "Total training time: 32.36 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 711.53, NNZs: 98, Bias: -461.071773, T: 95483840, Avg. loss: 1169.698211\n",
      "Total training time: 32.44 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 711.27, NNZs: 98, Bias: -461.109124, T: 95692320, Avg. loss: 1166.332262\n",
      "Total training time: 32.52 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 710.96, NNZs: 98, Bias: -461.161947, T: 95900800, Avg. loss: 1159.146087\n",
      "Total training time: 32.62 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 710.70, NNZs: 98, Bias: -461.208924, T: 96109280, Avg. loss: 1158.759678\n",
      "Total training time: 32.71 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 710.41, NNZs: 98, Bias: -461.243531, T: 96317760, Avg. loss: 1157.600095\n",
      "Total training time: 32.81 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 710.11, NNZs: 98, Bias: -461.308769, T: 96526240, Avg. loss: 1155.663143\n",
      "Total training time: 32.88 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 709.82, NNZs: 98, Bias: -461.359684, T: 96734720, Avg. loss: 1154.464633\n",
      "Total training time: 32.94 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 709.54, NNZs: 98, Bias: -461.415447, T: 96943200, Avg. loss: 1153.694865\n",
      "Total training time: 33.03 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 709.27, NNZs: 98, Bias: -461.458830, T: 97151680, Avg. loss: 1148.231567\n",
      "Total training time: 33.13 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 708.98, NNZs: 98, Bias: -461.508180, T: 97360160, Avg. loss: 1142.282638\n",
      "Total training time: 33.21 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 708.72, NNZs: 98, Bias: -461.540904, T: 97568640, Avg. loss: 1144.215244\n",
      "Total training time: 33.30 seconds.\n",
      "-- Epoch 469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 708.43, NNZs: 98, Bias: -461.591184, T: 97777120, Avg. loss: 1138.996953\n",
      "Total training time: 33.40 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 708.14, NNZs: 98, Bias: -461.641050, T: 97985600, Avg. loss: 1136.411956\n",
      "Total training time: 33.49 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 707.86, NNZs: 98, Bias: -461.696000, T: 98194080, Avg. loss: 1134.498210\n",
      "Total training time: 33.56 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 707.59, NNZs: 98, Bias: -461.739035, T: 98402560, Avg. loss: 1132.399281\n",
      "Total training time: 33.62 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 707.30, NNZs: 98, Bias: -461.797929, T: 98611040, Avg. loss: 1127.239823\n",
      "Total training time: 33.71 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 707.02, NNZs: 98, Bias: -461.838954, T: 98819520, Avg. loss: 1128.819959\n",
      "Total training time: 33.80 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 706.75, NNZs: 98, Bias: -461.893131, T: 99028000, Avg. loss: 1126.657029\n",
      "Total training time: 33.87 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 706.46, NNZs: 98, Bias: -461.935604, T: 99236480, Avg. loss: 1121.146905\n",
      "Total training time: 33.94 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 706.19, NNZs: 98, Bias: -461.978986, T: 99444960, Avg. loss: 1124.983101\n",
      "Total training time: 34.01 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 705.92, NNZs: 98, Bias: -462.023500, T: 99653440, Avg. loss: 1122.891537\n",
      "Total training time: 34.08 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 705.64, NNZs: 98, Bias: -462.082153, T: 99861920, Avg. loss: 1113.453829\n",
      "Total training time: 34.15 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 705.33, NNZs: 98, Bias: -462.127173, T: 100070400, Avg. loss: 1108.855561\n",
      "Total training time: 34.22 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 705.05, NNZs: 98, Bias: -462.186370, T: 100278880, Avg. loss: 1112.331238\n",
      "Total training time: 34.30 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 704.80, NNZs: 98, Bias: -462.237177, T: 100487360, Avg. loss: 1111.931571\n",
      "Total training time: 34.37 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 704.53, NNZs: 98, Bias: -462.296814, T: 100695840, Avg. loss: 1102.939164\n",
      "Total training time: 34.43 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 704.24, NNZs: 98, Bias: -462.334104, T: 100904320, Avg. loss: 1104.129469\n",
      "Total training time: 34.51 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 703.94, NNZs: 98, Bias: -462.386263, T: 101112800, Avg. loss: 1104.587989\n",
      "Total training time: 34.58 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 703.67, NNZs: 98, Bias: -462.447417, T: 101321280, Avg. loss: 1101.298284\n",
      "Total training time: 34.65 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 703.39, NNZs: 98, Bias: -462.482708, T: 101529760, Avg. loss: 1095.632074\n",
      "Total training time: 34.72 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 703.13, NNZs: 98, Bias: -462.529743, T: 101738240, Avg. loss: 1095.201789\n",
      "Total training time: 34.79 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 702.86, NNZs: 98, Bias: -462.584540, T: 101946720, Avg. loss: 1089.432454\n",
      "Total training time: 34.86 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 702.59, NNZs: 98, Bias: -462.623531, T: 102155200, Avg. loss: 1084.174119\n",
      "Total training time: 34.92 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 702.31, NNZs: 98, Bias: -462.667240, T: 102363680, Avg. loss: 1084.274254\n",
      "Total training time: 35.00 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 702.04, NNZs: 98, Bias: -462.710175, T: 102572160, Avg. loss: 1082.231319\n",
      "Total training time: 35.06 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 701.79, NNZs: 98, Bias: -462.748934, T: 102780640, Avg. loss: 1081.523679\n",
      "Total training time: 35.13 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 701.52, NNZs: 98, Bias: -462.795401, T: 102989120, Avg. loss: 1079.408296\n",
      "Total training time: 35.21 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 701.25, NNZs: 98, Bias: -462.840590, T: 103197600, Avg. loss: 1079.846888\n",
      "Total training time: 35.27 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 700.98, NNZs: 98, Bias: -462.893920, T: 103406080, Avg. loss: 1074.935330\n",
      "Total training time: 35.34 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 700.72, NNZs: 98, Bias: -462.919229, T: 103614560, Avg. loss: 1073.761840\n",
      "Total training time: 35.41 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 700.46, NNZs: 98, Bias: -462.955663, T: 103823040, Avg. loss: 1069.256033\n",
      "Total training time: 35.48 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 700.19, NNZs: 98, Bias: -462.997617, T: 104031520, Avg. loss: 1070.118864\n",
      "Total training time: 35.55 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 699.93, NNZs: 98, Bias: -463.032579, T: 104240000, Avg. loss: 1067.486584\n",
      "Total training time: 35.62 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 699.66, NNZs: 98, Bias: -463.081846, T: 104448480, Avg. loss: 1062.321876\n",
      "Total training time: 35.69 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 699.40, NNZs: 98, Bias: -463.114170, T: 104656960, Avg. loss: 1059.512410\n",
      "Total training time: 35.76 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 699.12, NNZs: 98, Bias: -463.158557, T: 104865440, Avg. loss: 1059.198461\n",
      "Total training time: 35.83 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 698.86, NNZs: 98, Bias: -463.200861, T: 105073920, Avg. loss: 1060.877953\n",
      "Total training time: 35.90 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 698.60, NNZs: 98, Bias: -463.238794, T: 105282400, Avg. loss: 1061.217911\n",
      "Total training time: 35.97 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 698.34, NNZs: 98, Bias: -463.277319, T: 105490880, Avg. loss: 1056.034033\n",
      "Total training time: 36.04 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 698.07, NNZs: 98, Bias: -463.315963, T: 105699360, Avg. loss: 1048.998062\n",
      "Total training time: 36.11 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 697.80, NNZs: 98, Bias: -463.370595, T: 105907840, Avg. loss: 1053.055458\n",
      "Total training time: 36.18 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 697.55, NNZs: 98, Bias: -463.423236, T: 106116320, Avg. loss: 1048.125254\n",
      "Total training time: 36.25 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 697.29, NNZs: 98, Bias: -463.459856, T: 106324800, Avg. loss: 1037.810759\n",
      "Total training time: 36.32 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 697.02, NNZs: 98, Bias: -463.497541, T: 106533280, Avg. loss: 1039.542738\n",
      "Total training time: 36.38 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 696.76, NNZs: 98, Bias: -463.543586, T: 106741760, Avg. loss: 1042.953198\n",
      "Total training time: 36.46 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 696.49, NNZs: 98, Bias: -463.579703, T: 106950240, Avg. loss: 1040.391727\n",
      "Total training time: 36.52 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 696.23, NNZs: 98, Bias: -463.615303, T: 107158720, Avg. loss: 1035.026828\n",
      "Total training time: 36.59 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 695.99, NNZs: 98, Bias: -463.643466, T: 107367200, Avg. loss: 1036.591252\n",
      "Total training time: 36.66 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 695.73, NNZs: 98, Bias: -463.682257, T: 107575680, Avg. loss: 1032.601959\n",
      "Total training time: 36.73 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 695.46, NNZs: 98, Bias: -463.733709, T: 107784160, Avg. loss: 1032.985671\n",
      "Total training time: 36.80 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 695.21, NNZs: 98, Bias: -463.772554, T: 107992640, Avg. loss: 1037.013236\n",
      "Total training time: 36.87 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 694.95, NNZs: 98, Bias: -463.821865, T: 108201120, Avg. loss: 1033.103928\n",
      "Total training time: 36.94 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 694.68, NNZs: 98, Bias: -463.870796, T: 108409600, Avg. loss: 1026.300424\n",
      "Total training time: 37.01 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 694.41, NNZs: 98, Bias: -463.902774, T: 108618080, Avg. loss: 1024.856580\n",
      "Total training time: 37.08 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 694.15, NNZs: 98, Bias: -463.947932, T: 108826560, Avg. loss: 1022.685331\n",
      "Total training time: 37.15 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 693.91, NNZs: 98, Bias: -463.981267, T: 109035040, Avg. loss: 1014.772024\n",
      "Total training time: 37.21 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 693.64, NNZs: 98, Bias: -464.033217, T: 109243520, Avg. loss: 1013.982323\n",
      "Total training time: 37.29 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 693.38, NNZs: 98, Bias: -464.069891, T: 109452000, Avg. loss: 1014.400567\n",
      "Total training time: 37.35 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 693.09, NNZs: 98, Bias: -464.121825, T: 109660480, Avg. loss: 1004.854090\n",
      "Total training time: 37.42 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 692.81, NNZs: 98, Bias: -464.163911, T: 109868960, Avg. loss: 1007.770529\n",
      "Total training time: 37.50 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 692.55, NNZs: 98, Bias: -464.212023, T: 110077440, Avg. loss: 1011.069666\n",
      "Total training time: 37.56 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 692.28, NNZs: 98, Bias: -464.251237, T: 110285920, Avg. loss: 1006.583926\n",
      "Total training time: 37.64 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 692.03, NNZs: 98, Bias: -464.296893, T: 110494400, Avg. loss: 1005.653558\n",
      "Total training time: 37.71 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 691.76, NNZs: 98, Bias: -464.339025, T: 110702880, Avg. loss: 1004.006466\n",
      "Total training time: 37.78 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 691.52, NNZs: 98, Bias: -464.373135, T: 110911360, Avg. loss: 1001.110904\n",
      "Total training time: 37.84 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 691.27, NNZs: 98, Bias: -464.408893, T: 111119840, Avg. loss: 992.949360\n",
      "Total training time: 37.92 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 691.03, NNZs: 98, Bias: -464.440361, T: 111328320, Avg. loss: 992.555978\n",
      "Total training time: 37.98 seconds.\n",
      "-- Epoch 535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 690.78, NNZs: 98, Bias: -464.477069, T: 111536800, Avg. loss: 988.403770\n",
      "Total training time: 38.05 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 690.52, NNZs: 98, Bias: -464.531426, T: 111745280, Avg. loss: 991.269724\n",
      "Total training time: 38.13 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 690.30, NNZs: 98, Bias: -464.566394, T: 111953760, Avg. loss: 988.400785\n",
      "Total training time: 38.19 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 690.06, NNZs: 98, Bias: -464.603431, T: 112162240, Avg. loss: 985.713174\n",
      "Total training time: 38.26 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 689.80, NNZs: 98, Bias: -464.640484, T: 112370720, Avg. loss: 985.060020\n",
      "Total training time: 38.33 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 689.54, NNZs: 98, Bias: -464.675511, T: 112579200, Avg. loss: 982.880741\n",
      "Total training time: 38.40 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 689.27, NNZs: 98, Bias: -464.723073, T: 112787680, Avg. loss: 980.775503\n",
      "Total training time: 38.47 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 689.02, NNZs: 98, Bias: -464.761423, T: 112996160, Avg. loss: 976.402394\n",
      "Total training time: 38.54 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 688.77, NNZs: 98, Bias: -464.790864, T: 113204640, Avg. loss: 979.057375\n",
      "Total training time: 38.61 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 688.53, NNZs: 98, Bias: -464.829260, T: 113413120, Avg. loss: 974.849915\n",
      "Total training time: 38.71 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 688.28, NNZs: 98, Bias: -464.865200, T: 113621600, Avg. loss: 974.711123\n",
      "Total training time: 38.80 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 688.03, NNZs: 98, Bias: -464.902309, T: 113830080, Avg. loss: 971.327356\n",
      "Total training time: 38.87 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 687.77, NNZs: 98, Bias: -464.933550, T: 114038560, Avg. loss: 972.360359\n",
      "Total training time: 38.94 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 687.53, NNZs: 98, Bias: -464.971920, T: 114247040, Avg. loss: 971.184705\n",
      "Total training time: 39.01 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 687.28, NNZs: 98, Bias: -465.000690, T: 114455520, Avg. loss: 968.489322\n",
      "Total training time: 39.08 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 687.03, NNZs: 98, Bias: -465.040674, T: 114664000, Avg. loss: 963.203539\n",
      "Total training time: 39.15 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 686.78, NNZs: 98, Bias: -465.080920, T: 114872480, Avg. loss: 965.565452\n",
      "Total training time: 39.22 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 686.53, NNZs: 98, Bias: -465.118060, T: 115080960, Avg. loss: 963.610553\n",
      "Total training time: 39.29 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 686.28, NNZs: 98, Bias: -465.144623, T: 115289440, Avg. loss: 959.290606\n",
      "Total training time: 39.36 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 686.04, NNZs: 98, Bias: -465.188473, T: 115497920, Avg. loss: 962.619434\n",
      "Total training time: 39.43 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 685.80, NNZs: 98, Bias: -465.221514, T: 115706400, Avg. loss: 955.142621\n",
      "Total training time: 39.50 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 685.55, NNZs: 98, Bias: -465.258985, T: 115914880, Avg. loss: 957.755977\n",
      "Total training time: 39.57 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 685.33, NNZs: 98, Bias: -465.287255, T: 116123360, Avg. loss: 954.924575\n",
      "Total training time: 39.64 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 685.10, NNZs: 98, Bias: -465.323382, T: 116331840, Avg. loss: 955.399303\n",
      "Total training time: 39.71 seconds.\n",
      "-- Epoch 559\n",
      "Norm: 684.86, NNZs: 98, Bias: -465.347259, T: 116540320, Avg. loss: 945.372690\n",
      "Total training time: 39.78 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 684.62, NNZs: 98, Bias: -465.376663, T: 116748800, Avg. loss: 947.707292\n",
      "Total training time: 39.85 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 684.39, NNZs: 98, Bias: -465.415428, T: 116957280, Avg. loss: 948.135300\n",
      "Total training time: 39.92 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 684.14, NNZs: 98, Bias: -465.461915, T: 117165760, Avg. loss: 944.285591\n",
      "Total training time: 39.99 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 683.89, NNZs: 98, Bias: -465.484431, T: 117374240, Avg. loss: 944.933749\n",
      "Total training time: 40.06 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 683.64, NNZs: 98, Bias: -465.518135, T: 117582720, Avg. loss: 942.216527\n",
      "Total training time: 40.12 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 683.40, NNZs: 98, Bias: -465.552628, T: 117791200, Avg. loss: 937.042917\n",
      "Total training time: 40.19 seconds.\n",
      "-- Epoch 566\n",
      "Norm: 683.15, NNZs: 98, Bias: -465.592082, T: 117999680, Avg. loss: 930.278682\n",
      "Total training time: 40.27 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 682.90, NNZs: 98, Bias: -465.633577, T: 118208160, Avg. loss: 936.135087\n",
      "Total training time: 40.33 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 682.66, NNZs: 98, Bias: -465.678379, T: 118416640, Avg. loss: 937.797698\n",
      "Total training time: 40.40 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 682.41, NNZs: 98, Bias: -465.716427, T: 118625120, Avg. loss: 932.208624\n",
      "Total training time: 40.47 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 682.16, NNZs: 98, Bias: -465.758702, T: 118833600, Avg. loss: 935.959605\n",
      "Total training time: 40.54 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 681.91, NNZs: 98, Bias: -465.795104, T: 119042080, Avg. loss: 926.216770\n",
      "Total training time: 40.61 seconds.\n",
      "-- Epoch 572\n",
      "Norm: 681.66, NNZs: 98, Bias: -465.830698, T: 119250560, Avg. loss: 920.445436\n",
      "Total training time: 40.68 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 681.42, NNZs: 98, Bias: -465.876029, T: 119459040, Avg. loss: 927.770087\n",
      "Total training time: 40.74 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 681.19, NNZs: 98, Bias: -465.900290, T: 119667520, Avg. loss: 917.964652\n",
      "Total training time: 40.81 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 680.96, NNZs: 98, Bias: -465.927920, T: 119876000, Avg. loss: 922.735677\n",
      "Total training time: 40.88 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 680.71, NNZs: 98, Bias: -465.957348, T: 120084480, Avg. loss: 918.851543\n",
      "Total training time: 40.95 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 680.47, NNZs: 98, Bias: -465.981567, T: 120292960, Avg. loss: 920.446135\n",
      "Total training time: 41.01 seconds.\n",
      "-- Epoch 578\n",
      "Norm: 680.22, NNZs: 98, Bias: -466.018280, T: 120501440, Avg. loss: 916.358523\n",
      "Total training time: 41.08 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 680.01, NNZs: 98, Bias: -466.058078, T: 120709920, Avg. loss: 912.566477\n",
      "Total training time: 41.15 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 679.78, NNZs: 98, Bias: -466.094658, T: 120918400, Avg. loss: 909.853534\n",
      "Total training time: 41.21 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 679.54, NNZs: 98, Bias: -466.126388, T: 121126880, Avg. loss: 909.633223\n",
      "Total training time: 41.28 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 679.30, NNZs: 98, Bias: -466.162844, T: 121335360, Avg. loss: 910.102001\n",
      "Total training time: 41.35 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 679.07, NNZs: 98, Bias: -466.199479, T: 121543840, Avg. loss: 910.514896\n",
      "Total training time: 41.41 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 678.82, NNZs: 98, Bias: -466.224390, T: 121752320, Avg. loss: 906.887959\n",
      "Total training time: 41.48 seconds.\n",
      "-- Epoch 585\n",
      "Norm: 678.58, NNZs: 98, Bias: -466.254917, T: 121960800, Avg. loss: 905.668851\n",
      "Total training time: 41.55 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 678.36, NNZs: 98, Bias: -466.277862, T: 122169280, Avg. loss: 905.467678\n",
      "Total training time: 41.61 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 678.13, NNZs: 98, Bias: -466.316050, T: 122377760, Avg. loss: 900.709824\n",
      "Total training time: 41.68 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 677.88, NNZs: 98, Bias: -466.350506, T: 122586240, Avg. loss: 898.950565\n",
      "Total training time: 41.75 seconds.\n",
      "-- Epoch 589\n",
      "Norm: 677.63, NNZs: 98, Bias: -466.385157, T: 122794720, Avg. loss: 896.865317\n",
      "Total training time: 41.81 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 677.41, NNZs: 98, Bias: -466.408185, T: 123003200, Avg. loss: 893.431223\n",
      "Total training time: 41.87 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 677.18, NNZs: 98, Bias: -466.437351, T: 123211680, Avg. loss: 899.469294\n",
      "Total training time: 41.94 seconds.\n",
      "-- Epoch 592\n",
      "Norm: 676.95, NNZs: 98, Bias: -466.467192, T: 123420160, Avg. loss: 895.948674\n",
      "Total training time: 42.00 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 676.73, NNZs: 98, Bias: -466.502490, T: 123628640, Avg. loss: 895.352473\n",
      "Total training time: 42.06 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 676.50, NNZs: 98, Bias: -466.538614, T: 123837120, Avg. loss: 891.161982\n",
      "Total training time: 42.13 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 676.27, NNZs: 98, Bias: -466.563870, T: 124045600, Avg. loss: 890.941460\n",
      "Total training time: 42.19 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 676.04, NNZs: 98, Bias: -466.605913, T: 124254080, Avg. loss: 887.021532\n",
      "Total training time: 42.25 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 675.82, NNZs: 98, Bias: -466.639290, T: 124462560, Avg. loss: 884.553824\n",
      "Total training time: 42.32 seconds.\n",
      "-- Epoch 598\n",
      "Norm: 675.61, NNZs: 98, Bias: -466.661364, T: 124671040, Avg. loss: 887.359136\n",
      "Total training time: 42.38 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 675.38, NNZs: 98, Bias: -466.700226, T: 124879520, Avg. loss: 879.346331\n",
      "Total training time: 42.44 seconds.\n",
      "-- Epoch 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 675.16, NNZs: 98, Bias: -466.735993, T: 125088000, Avg. loss: 884.037359\n",
      "Total training time: 42.50 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 674.94, NNZs: 98, Bias: -466.764672, T: 125296480, Avg. loss: 875.999310\n",
      "Total training time: 42.57 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 674.72, NNZs: 98, Bias: -466.791382, T: 125504960, Avg. loss: 882.394654\n",
      "Total training time: 42.63 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 674.49, NNZs: 98, Bias: -466.823785, T: 125713440, Avg. loss: 875.244993\n",
      "Total training time: 42.69 seconds.\n",
      "-- Epoch 604\n",
      "Norm: 674.26, NNZs: 98, Bias: -466.860579, T: 125921920, Avg. loss: 871.543336\n",
      "Total training time: 42.76 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 674.04, NNZs: 98, Bias: -466.898037, T: 126130400, Avg. loss: 876.316594\n",
      "Total training time: 42.82 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 673.80, NNZs: 98, Bias: -466.931625, T: 126338880, Avg. loss: 870.773656\n",
      "Total training time: 42.88 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 673.58, NNZs: 98, Bias: -466.950290, T: 126547360, Avg. loss: 873.249828\n",
      "Total training time: 42.94 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 673.36, NNZs: 98, Bias: -466.988270, T: 126755840, Avg. loss: 872.811144\n",
      "Total training time: 43.01 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 673.11, NNZs: 98, Bias: -467.026973, T: 126964320, Avg. loss: 870.538083\n",
      "Total training time: 43.07 seconds.\n",
      "-- Epoch 610\n",
      "Norm: 672.90, NNZs: 98, Bias: -467.055924, T: 127172800, Avg. loss: 865.182061\n",
      "Total training time: 43.13 seconds.\n",
      "-- Epoch 611\n",
      "Norm: 672.68, NNZs: 98, Bias: -467.079568, T: 127381280, Avg. loss: 865.859383\n",
      "Total training time: 43.19 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 672.46, NNZs: 98, Bias: -467.100987, T: 127589760, Avg. loss: 867.299839\n",
      "Total training time: 43.26 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 672.23, NNZs: 98, Bias: -467.127379, T: 127798240, Avg. loss: 862.171317\n",
      "Total training time: 43.32 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 672.00, NNZs: 98, Bias: -467.167878, T: 128006720, Avg. loss: 861.912963\n",
      "Total training time: 43.38 seconds.\n",
      "-- Epoch 615\n",
      "Norm: 671.78, NNZs: 98, Bias: -467.201208, T: 128215200, Avg. loss: 858.514193\n",
      "Total training time: 43.44 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 671.55, NNZs: 98, Bias: -467.239007, T: 128423680, Avg. loss: 857.307503\n",
      "Total training time: 43.51 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 671.33, NNZs: 98, Bias: -467.263359, T: 128632160, Avg. loss: 857.725770\n",
      "Total training time: 43.57 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 671.11, NNZs: 98, Bias: -467.297069, T: 128840640, Avg. loss: 854.550521\n",
      "Total training time: 43.63 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 670.90, NNZs: 98, Bias: -467.318632, T: 129049120, Avg. loss: 851.268364\n",
      "Total training time: 43.70 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 670.68, NNZs: 98, Bias: -467.345187, T: 129257600, Avg. loss: 850.675944\n",
      "Total training time: 43.76 seconds.\n",
      "-- Epoch 621\n",
      "Norm: 670.49, NNZs: 98, Bias: -467.370617, T: 129466080, Avg. loss: 855.048272\n",
      "Total training time: 43.82 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 670.26, NNZs: 98, Bias: -467.398013, T: 129674560, Avg. loss: 845.158181\n",
      "Total training time: 43.89 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 670.04, NNZs: 98, Bias: -467.430597, T: 129883040, Avg. loss: 848.853975\n",
      "Total training time: 43.95 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 669.81, NNZs: 98, Bias: -467.454908, T: 130091520, Avg. loss: 848.491338\n",
      "Total training time: 44.01 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 669.59, NNZs: 98, Bias: -467.482021, T: 130300000, Avg. loss: 846.708981\n",
      "Total training time: 44.07 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 669.38, NNZs: 98, Bias: -467.498211, T: 130508480, Avg. loss: 840.627608\n",
      "Total training time: 44.14 seconds.\n",
      "-- Epoch 627\n",
      "Norm: 669.15, NNZs: 98, Bias: -467.521401, T: 130716960, Avg. loss: 844.160324\n",
      "Total training time: 44.20 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 668.92, NNZs: 98, Bias: -467.547618, T: 130925440, Avg. loss: 840.105412\n",
      "Total training time: 44.26 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 668.71, NNZs: 98, Bias: -467.576622, T: 131133920, Avg. loss: 841.345868\n",
      "Total training time: 44.33 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 668.50, NNZs: 98, Bias: -467.605043, T: 131342400, Avg. loss: 834.541718\n",
      "Total training time: 44.39 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 668.29, NNZs: 98, Bias: -467.626419, T: 131550880, Avg. loss: 834.162920\n",
      "Total training time: 44.45 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 668.05, NNZs: 98, Bias: -467.663790, T: 131759360, Avg. loss: 833.267584\n",
      "Total training time: 44.51 seconds.\n",
      "-- Epoch 633\n",
      "Norm: 667.83, NNZs: 98, Bias: -467.682900, T: 131967840, Avg. loss: 835.423445\n",
      "Total training time: 44.57 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 667.61, NNZs: 98, Bias: -467.717880, T: 132176320, Avg. loss: 827.178297\n",
      "Total training time: 44.64 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 667.37, NNZs: 98, Bias: -467.757638, T: 132384800, Avg. loss: 829.101315\n",
      "Total training time: 44.70 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 667.14, NNZs: 98, Bias: -467.795522, T: 132593280, Avg. loss: 827.948709\n",
      "Total training time: 44.76 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 666.92, NNZs: 98, Bias: -467.825817, T: 132801760, Avg. loss: 827.859805\n",
      "Total training time: 44.82 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 666.70, NNZs: 98, Bias: -467.856135, T: 133010240, Avg. loss: 825.171004\n",
      "Total training time: 44.89 seconds.\n",
      "-- Epoch 639\n",
      "Norm: 666.49, NNZs: 98, Bias: -467.868159, T: 133218720, Avg. loss: 826.950164\n",
      "Total training time: 44.95 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 666.27, NNZs: 98, Bias: -467.900940, T: 133427200, Avg. loss: 829.146107\n",
      "Total training time: 45.01 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 666.06, NNZs: 98, Bias: -467.922884, T: 133635680, Avg. loss: 826.468912\n",
      "Total training time: 45.08 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 665.85, NNZs: 98, Bias: -467.947188, T: 133844160, Avg. loss: 817.738864\n",
      "Total training time: 45.14 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 665.64, NNZs: 98, Bias: -467.979813, T: 134052640, Avg. loss: 819.487909\n",
      "Total training time: 45.20 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 665.41, NNZs: 98, Bias: -468.017383, T: 134261120, Avg. loss: 813.497392\n",
      "Total training time: 45.27 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 665.21, NNZs: 98, Bias: -468.046935, T: 134469600, Avg. loss: 821.441898\n",
      "Total training time: 45.33 seconds.\n",
      "-- Epoch 646\n",
      "Norm: 665.00, NNZs: 98, Bias: -468.071974, T: 134678080, Avg. loss: 819.528412\n",
      "Total training time: 45.39 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 664.78, NNZs: 98, Bias: -468.106021, T: 134886560, Avg. loss: 812.516876\n",
      "Total training time: 45.45 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 664.57, NNZs: 98, Bias: -468.127283, T: 135095040, Avg. loss: 812.833901\n",
      "Total training time: 45.52 seconds.\n",
      "-- Epoch 649\n",
      "Norm: 664.35, NNZs: 98, Bias: -468.148954, T: 135303520, Avg. loss: 812.281881\n",
      "Total training time: 45.58 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 664.13, NNZs: 98, Bias: -468.180196, T: 135512000, Avg. loss: 808.663688\n",
      "Total training time: 45.65 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 663.90, NNZs: 98, Bias: -468.214847, T: 135720480, Avg. loss: 807.477231\n",
      "Total training time: 45.71 seconds.\n",
      "-- Epoch 652\n",
      "Norm: 663.70, NNZs: 98, Bias: -468.248562, T: 135928960, Avg. loss: 810.276093\n",
      "Total training time: 45.78 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 663.48, NNZs: 98, Bias: -468.264218, T: 136137440, Avg. loss: 808.939894\n",
      "Total training time: 45.85 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 663.26, NNZs: 98, Bias: -468.293360, T: 136345920, Avg. loss: 808.668803\n",
      "Total training time: 45.92 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 663.05, NNZs: 98, Bias: -468.312856, T: 136554400, Avg. loss: 803.432973\n",
      "Total training time: 45.98 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 662.82, NNZs: 98, Bias: -468.337291, T: 136762880, Avg. loss: 804.938240\n",
      "Total training time: 46.06 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 662.60, NNZs: 98, Bias: -468.364257, T: 136971360, Avg. loss: 801.679046\n",
      "Total training time: 46.12 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 662.39, NNZs: 98, Bias: -468.391395, T: 137179840, Avg. loss: 803.569595\n",
      "Total training time: 46.18 seconds.\n",
      "-- Epoch 659\n",
      "Norm: 662.20, NNZs: 98, Bias: -468.415145, T: 137388320, Avg. loss: 799.472332\n",
      "Total training time: 46.25 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 661.98, NNZs: 98, Bias: -468.434633, T: 137596800, Avg. loss: 800.708279\n",
      "Total training time: 46.31 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 661.77, NNZs: 98, Bias: -468.462369, T: 137805280, Avg. loss: 799.725669\n",
      "Total training time: 46.37 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 661.58, NNZs: 98, Bias: -468.480065, T: 138013760, Avg. loss: 795.697053\n",
      "Total training time: 46.44 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 661.37, NNZs: 98, Bias: -468.509607, T: 138222240, Avg. loss: 793.779740\n",
      "Total training time: 46.50 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 661.16, NNZs: 98, Bias: -468.532016, T: 138430720, Avg. loss: 790.850418\n",
      "Total training time: 46.56 seconds.\n",
      "-- Epoch 665\n",
      "Norm: 660.96, NNZs: 98, Bias: -468.563706, T: 138639200, Avg. loss: 791.172541\n",
      "Total training time: 46.62 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 660.74, NNZs: 98, Bias: -468.590665, T: 138847680, Avg. loss: 792.808007\n",
      "Total training time: 46.69 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 660.52, NNZs: 98, Bias: -468.612827, T: 139056160, Avg. loss: 785.614020\n",
      "Total training time: 46.75 seconds.\n",
      "-- Epoch 668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 660.32, NNZs: 98, Bias: -468.634744, T: 139264640, Avg. loss: 788.656242\n",
      "Total training time: 46.82 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 660.11, NNZs: 98, Bias: -468.661945, T: 139473120, Avg. loss: 787.427826\n",
      "Total training time: 46.89 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 659.91, NNZs: 98, Bias: -468.695691, T: 139681600, Avg. loss: 789.048236\n",
      "Total training time: 46.95 seconds.\n",
      "-- Epoch 671\n",
      "Norm: 659.70, NNZs: 98, Bias: -468.717153, T: 139890080, Avg. loss: 778.741747\n",
      "Total training time: 47.02 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 659.49, NNZs: 98, Bias: -468.741370, T: 140098560, Avg. loss: 785.387688\n",
      "Total training time: 47.09 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 659.31, NNZs: 98, Bias: -468.761055, T: 140307040, Avg. loss: 785.016338\n",
      "Total training time: 47.15 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 659.10, NNZs: 98, Bias: -468.782066, T: 140515520, Avg. loss: 780.313505\n",
      "Total training time: 47.22 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 658.90, NNZs: 98, Bias: -468.812645, T: 140724000, Avg. loss: 782.772792\n",
      "Total training time: 47.29 seconds.\n",
      "-- Epoch 676\n",
      "Norm: 658.68, NNZs: 98, Bias: -468.842964, T: 140932480, Avg. loss: 773.493774\n",
      "Total training time: 47.35 seconds.\n",
      "-- Epoch 677\n",
      "Norm: 658.47, NNZs: 98, Bias: -468.857216, T: 141140960, Avg. loss: 779.107772\n",
      "Total training time: 47.43 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 658.26, NNZs: 98, Bias: -468.883695, T: 141349440, Avg. loss: 779.346649\n",
      "Total training time: 47.51 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 658.05, NNZs: 98, Bias: -468.911972, T: 141557920, Avg. loss: 777.507334\n",
      "Total training time: 47.58 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 657.84, NNZs: 98, Bias: -468.933288, T: 141766400, Avg. loss: 770.369393\n",
      "Total training time: 47.65 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 657.64, NNZs: 98, Bias: -468.957750, T: 141974880, Avg. loss: 772.793144\n",
      "Total training time: 47.72 seconds.\n",
      "-- Epoch 682\n",
      "Norm: 657.43, NNZs: 98, Bias: -468.984284, T: 142183360, Avg. loss: 770.064042\n",
      "Total training time: 47.79 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 657.23, NNZs: 98, Bias: -469.000028, T: 142391840, Avg. loss: 771.690041\n",
      "Total training time: 47.86 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 657.03, NNZs: 98, Bias: -469.027963, T: 142600320, Avg. loss: 766.893681\n",
      "Total training time: 47.93 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 656.82, NNZs: 98, Bias: -469.058372, T: 142808800, Avg. loss: 769.174125\n",
      "Total training time: 48.00 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 656.61, NNZs: 98, Bias: -469.079368, T: 143017280, Avg. loss: 765.316312\n",
      "Total training time: 48.08 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 656.40, NNZs: 98, Bias: -469.096559, T: 143225760, Avg. loss: 765.962174\n",
      "Total training time: 48.15 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 656.18, NNZs: 98, Bias: -469.126981, T: 143434240, Avg. loss: 766.791987\n",
      "Total training time: 48.21 seconds.\n",
      "-- Epoch 689\n",
      "Norm: 655.98, NNZs: 98, Bias: -469.149421, T: 143642720, Avg. loss: 759.440036\n",
      "Total training time: 48.28 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 655.77, NNZs: 98, Bias: -469.170292, T: 143851200, Avg. loss: 759.696874\n",
      "Total training time: 48.36 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 655.56, NNZs: 98, Bias: -469.195370, T: 144059680, Avg. loss: 757.201754\n",
      "Total training time: 48.43 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 655.37, NNZs: 98, Bias: -469.214166, T: 144268160, Avg. loss: 758.920011\n",
      "Total training time: 48.50 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 655.16, NNZs: 98, Bias: -469.239866, T: 144476640, Avg. loss: 758.220993\n",
      "Total training time: 48.57 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 654.96, NNZs: 98, Bias: -469.265462, T: 144685120, Avg. loss: 758.286859\n",
      "Total training time: 48.66 seconds.\n",
      "-- Epoch 695\n",
      "Norm: 654.75, NNZs: 98, Bias: -469.289640, T: 144893600, Avg. loss: 755.515503\n",
      "Total training time: 48.73 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 654.56, NNZs: 98, Bias: -469.314605, T: 145102080, Avg. loss: 756.372836\n",
      "Total training time: 48.81 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 654.35, NNZs: 98, Bias: -469.333613, T: 145310560, Avg. loss: 754.263341\n",
      "Total training time: 48.88 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 654.15, NNZs: 98, Bias: -469.357614, T: 145519040, Avg. loss: 749.477622\n",
      "Total training time: 48.96 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 653.96, NNZs: 98, Bias: -469.375051, T: 145727520, Avg. loss: 753.806796\n",
      "Total training time: 49.03 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 653.76, NNZs: 98, Bias: -469.400700, T: 145936000, Avg. loss: 751.626336\n",
      "Total training time: 49.12 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 653.56, NNZs: 98, Bias: -469.427198, T: 146144480, Avg. loss: 748.082725\n",
      "Total training time: 49.19 seconds.\n",
      "-- Epoch 702\n",
      "Norm: 653.35, NNZs: 98, Bias: -469.442310, T: 146352960, Avg. loss: 745.576271\n",
      "Total training time: 49.27 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 653.15, NNZs: 98, Bias: -469.468392, T: 146561440, Avg. loss: 742.527812\n",
      "Total training time: 49.35 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 652.95, NNZs: 98, Bias: -469.495185, T: 146769920, Avg. loss: 748.981378\n",
      "Total training time: 49.41 seconds.\n",
      "-- Epoch 705\n",
      "Norm: 652.75, NNZs: 98, Bias: -469.513567, T: 146978400, Avg. loss: 739.580853\n",
      "Total training time: 49.48 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 652.56, NNZs: 98, Bias: -469.533076, T: 147186880, Avg. loss: 746.494881\n",
      "Total training time: 49.55 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 652.36, NNZs: 98, Bias: -469.552900, T: 147395360, Avg. loss: 742.842339\n",
      "Total training time: 49.63 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 652.16, NNZs: 98, Bias: -469.579946, T: 147603840, Avg. loss: 741.486615\n",
      "Total training time: 49.72 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 651.95, NNZs: 98, Bias: -469.600051, T: 147812320, Avg. loss: 737.358000\n",
      "Total training time: 49.80 seconds.\n",
      "-- Epoch 710\n",
      "Norm: 651.75, NNZs: 98, Bias: -469.624525, T: 148020800, Avg. loss: 738.808563\n",
      "Total training time: 49.87 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 651.54, NNZs: 98, Bias: -469.644109, T: 148229280, Avg. loss: 737.384681\n",
      "Total training time: 49.94 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 651.32, NNZs: 98, Bias: -469.673640, T: 148437760, Avg. loss: 738.191313\n",
      "Total training time: 50.02 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 651.11, NNZs: 98, Bias: -469.696125, T: 148646240, Avg. loss: 735.610642\n",
      "Total training time: 50.09 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 650.90, NNZs: 98, Bias: -469.715550, T: 148854720, Avg. loss: 732.283742\n",
      "Total training time: 50.15 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 650.71, NNZs: 98, Bias: -469.728501, T: 149063200, Avg. loss: 736.523546\n",
      "Total training time: 50.22 seconds.\n",
      "-- Epoch 716\n",
      "Norm: 650.52, NNZs: 98, Bias: -469.743382, T: 149271680, Avg. loss: 733.380603\n",
      "Total training time: 50.28 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 650.32, NNZs: 98, Bias: -469.764001, T: 149480160, Avg. loss: 727.060000\n",
      "Total training time: 50.35 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 650.13, NNZs: 98, Bias: -469.780108, T: 149688640, Avg. loss: 730.298435\n",
      "Total training time: 50.42 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 649.94, NNZs: 98, Bias: -469.806142, T: 149897120, Avg. loss: 730.704449\n",
      "Total training time: 50.49 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 649.72, NNZs: 98, Bias: -469.826741, T: 150105600, Avg. loss: 723.112152\n",
      "Total training time: 50.55 seconds.\n",
      "-- Epoch 721\n",
      "Norm: 649.53, NNZs: 98, Bias: -469.845185, T: 150314080, Avg. loss: 728.189392\n",
      "Total training time: 50.63 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 649.35, NNZs: 98, Bias: -469.862409, T: 150522560, Avg. loss: 730.703431\n",
      "Total training time: 50.72 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 649.15, NNZs: 98, Bias: -469.883057, T: 150731040, Avg. loss: 724.602753\n",
      "Total training time: 50.79 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 648.97, NNZs: 98, Bias: -469.901353, T: 150939520, Avg. loss: 725.840491\n",
      "Total training time: 50.87 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 648.79, NNZs: 98, Bias: -469.921614, T: 151148000, Avg. loss: 725.446842\n",
      "Total training time: 50.96 seconds.\n",
      "Convergence after 725 epochs took 50.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(verbose=3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1191cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5723220966596957"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = sgd_model.predict(X_test)\n",
    "f1_score(y_test, y_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d33b248e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>roof_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300051</th>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99355</th>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890251</th>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745817</th>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421793</th>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310028</th>\n",
       "      <td>4</td>\n",
       "      <td>605</td>\n",
       "      <td>3623</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663567</th>\n",
       "      <td>10</td>\n",
       "      <td>1407</td>\n",
       "      <td>11907</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049160</th>\n",
       "      <td>22</td>\n",
       "      <td>1136</td>\n",
       "      <td>7712</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442785</th>\n",
       "      <td>6</td>\n",
       "      <td>1041</td>\n",
       "      <td>912</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501372</th>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>6436</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86868 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "building_id                                                   \n",
       "300051                   17             596           11307   \n",
       "99355                     6             141           11987   \n",
       "890251                   22              19           10044   \n",
       "745817                   26              39             633   \n",
       "421793                   17             289            7970   \n",
       "...                     ...             ...             ...   \n",
       "310028                    4             605            3623   \n",
       "663567                   10            1407           11907   \n",
       "1049160                  22            1136            7712   \n",
       "442785                    6            1041             912   \n",
       "501372                   26              36            6436   \n",
       "\n",
       "             count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "building_id                                                                 \n",
       "300051                         3   20                7                  6   \n",
       "99355                          2   25               13                  5   \n",
       "890251                         2    5                4                  5   \n",
       "745817                         1    0               19                  3   \n",
       "421793                         3   15                8                  7   \n",
       "...                          ...  ...              ...                ...   \n",
       "310028                         3   70               20                  6   \n",
       "663567                         3   25                6                  7   \n",
       "1049160                        1   50                3                  3   \n",
       "442785                         2    5                9                  5   \n",
       "501372                         2   10               11                  4   \n",
       "\n",
       "            land_surface_condition foundation_type roof_type  ...  \\\n",
       "building_id                                                   ...   \n",
       "300051                           t               r         n  ...   \n",
       "99355                            t               r         n  ...   \n",
       "890251                           t               r         n  ...   \n",
       "745817                           t               r         x  ...   \n",
       "421793                           t               r         q  ...   \n",
       "...                            ...             ...       ...  ...   \n",
       "310028                           t               r         q  ...   \n",
       "663567                           n               r         n  ...   \n",
       "1049160                          t               r         n  ...   \n",
       "442785                           t               r         n  ...   \n",
       "501372                           t               r         q  ...   \n",
       "\n",
       "            has_secondary_use_agriculture has_secondary_use_hotel  \\\n",
       "building_id                                                         \n",
       "300051                                  0                       0   \n",
       "99355                                   1                       0   \n",
       "890251                                  0                       0   \n",
       "745817                                  0                       0   \n",
       "421793                                  0                       0   \n",
       "...                                   ...                     ...   \n",
       "310028                                  1                       0   \n",
       "663567                                  0                       0   \n",
       "1049160                                 0                       0   \n",
       "442785                                  0                       0   \n",
       "501372                                  0                       0   \n",
       "\n",
       "            has_secondary_use_rental has_secondary_use_institution  \\\n",
       "building_id                                                          \n",
       "300051                             0                             0   \n",
       "99355                              0                             0   \n",
       "890251                             0                             0   \n",
       "745817                             1                             0   \n",
       "421793                             0                             0   \n",
       "...                              ...                           ...   \n",
       "310028                             0                             0   \n",
       "663567                             0                             0   \n",
       "1049160                            0                             0   \n",
       "442785                             0                             0   \n",
       "501372                             0                             0   \n",
       "\n",
       "             has_secondary_use_school  has_secondary_use_industry  \\\n",
       "building_id                                                         \n",
       "300051                              0                           0   \n",
       "99355                               0                           0   \n",
       "890251                              0                           0   \n",
       "745817                              0                           0   \n",
       "421793                              0                           0   \n",
       "...                               ...                         ...   \n",
       "310028                              0                           0   \n",
       "663567                              0                           0   \n",
       "1049160                             0                           0   \n",
       "442785                              0                           0   \n",
       "501372                              0                           0   \n",
       "\n",
       "             has_secondary_use_health_post  has_secondary_use_gov_office  \\\n",
       "building_id                                                                \n",
       "300051                                   0                             0   \n",
       "99355                                    0                             0   \n",
       "890251                                   0                             0   \n",
       "745817                                   0                             0   \n",
       "421793                                   0                             0   \n",
       "...                                    ...                           ...   \n",
       "310028                                   0                             0   \n",
       "663567                                   0                             0   \n",
       "1049160                                  0                             0   \n",
       "442785                                   0                             0   \n",
       "501372                                   0                             0   \n",
       "\n",
       "             has_secondary_use_use_police  has_secondary_use_other  \n",
       "building_id                                                         \n",
       "300051                                  0                        0  \n",
       "99355                                   0                        0  \n",
       "890251                                  0                        0  \n",
       "745817                                  0                        0  \n",
       "421793                                  0                        0  \n",
       "...                                   ...                      ...  \n",
       "310028                                  0                        0  \n",
       "663567                                  0                        0  \n",
       "1049160                                 0                        0  \n",
       "442785                                  0                        0  \n",
       "501372                                  0                        0  \n",
       "\n",
       "[86868 rows x 38 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values = pd.read_csv('../../csv/test_values.csv', index_col = \"building_id\")\n",
    "test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb50d875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>roof_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300051</th>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99355</th>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890251</th>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745817</th>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421793</th>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310028</th>\n",
       "      <td>4</td>\n",
       "      <td>605</td>\n",
       "      <td>3623</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663567</th>\n",
       "      <td>10</td>\n",
       "      <td>1407</td>\n",
       "      <td>11907</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049160</th>\n",
       "      <td>22</td>\n",
       "      <td>1136</td>\n",
       "      <td>7712</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442785</th>\n",
       "      <td>6</td>\n",
       "      <td>1041</td>\n",
       "      <td>912</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501372</th>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>6436</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86868 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "building_id                                                  \n",
       "300051                  17             596           11307   \n",
       "99355                    6             141           11987   \n",
       "890251                  22              19           10044   \n",
       "745817                  26              39             633   \n",
       "421793                  17             289            7970   \n",
       "...                    ...             ...             ...   \n",
       "310028                   4             605            3623   \n",
       "663567                  10            1407           11907   \n",
       "1049160                 22            1136            7712   \n",
       "442785                   6            1041             912   \n",
       "501372                  26              36            6436   \n",
       "\n",
       "             count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "building_id                                                                 \n",
       "300051                         3   20                7                  6   \n",
       "99355                          2   25               13                  5   \n",
       "890251                         2    5                4                  5   \n",
       "745817                         1    0               19                  3   \n",
       "421793                         3   15                8                  7   \n",
       "...                          ...  ...              ...                ...   \n",
       "310028                         3   70               20                  6   \n",
       "663567                         3   25                6                  7   \n",
       "1049160                        1   50                3                  3   \n",
       "442785                         2    5                9                  5   \n",
       "501372                         2   10               11                  4   \n",
       "\n",
       "            land_surface_condition foundation_type roof_type  ...  \\\n",
       "building_id                                                   ...   \n",
       "300051                           t               r         n  ...   \n",
       "99355                            t               r         n  ...   \n",
       "890251                           t               r         n  ...   \n",
       "745817                           t               r         x  ...   \n",
       "421793                           t               r         q  ...   \n",
       "...                            ...             ...       ...  ...   \n",
       "310028                           t               r         q  ...   \n",
       "663567                           n               r         n  ...   \n",
       "1049160                          t               r         n  ...   \n",
       "442785                           t               r         n  ...   \n",
       "501372                           t               r         q  ...   \n",
       "\n",
       "            has_secondary_use_agriculture has_secondary_use_hotel  \\\n",
       "building_id                                                         \n",
       "300051                                  0                       0   \n",
       "99355                                   1                       0   \n",
       "890251                                  0                       0   \n",
       "745817                                  0                       0   \n",
       "421793                                  0                       0   \n",
       "...                                   ...                     ...   \n",
       "310028                                  1                       0   \n",
       "663567                                  0                       0   \n",
       "1049160                                 0                       0   \n",
       "442785                                  0                       0   \n",
       "501372                                  0                       0   \n",
       "\n",
       "            has_secondary_use_rental has_secondary_use_institution  \\\n",
       "building_id                                                          \n",
       "300051                             0                             0   \n",
       "99355                              0                             0   \n",
       "890251                             0                             0   \n",
       "745817                             1                             0   \n",
       "421793                             0                             0   \n",
       "...                              ...                           ...   \n",
       "310028                             0                             0   \n",
       "663567                             0                             0   \n",
       "1049160                            0                             0   \n",
       "442785                             0                             0   \n",
       "501372                             0                             0   \n",
       "\n",
       "             has_secondary_use_school  has_secondary_use_industry  \\\n",
       "building_id                                                         \n",
       "300051                              0                           0   \n",
       "99355                               0                           0   \n",
       "890251                              0                           0   \n",
       "745817                              0                           0   \n",
       "421793                              0                           0   \n",
       "...                               ...                         ...   \n",
       "310028                              0                           0   \n",
       "663567                              0                           0   \n",
       "1049160                             0                           0   \n",
       "442785                              0                           0   \n",
       "501372                              0                           0   \n",
       "\n",
       "             has_secondary_use_health_post  has_secondary_use_gov_office  \\\n",
       "building_id                                                                \n",
       "300051                                   0                             0   \n",
       "99355                                    0                             0   \n",
       "890251                                   0                             0   \n",
       "745817                                   0                             0   \n",
       "421793                                   0                             0   \n",
       "...                                    ...                           ...   \n",
       "310028                                   0                             0   \n",
       "663567                                   0                             0   \n",
       "1049160                                  0                             0   \n",
       "442785                                   0                             0   \n",
       "501372                                   0                             0   \n",
       "\n",
       "             has_secondary_use_use_police  has_secondary_use_other  \n",
       "building_id                                                         \n",
       "300051                                  0                        0  \n",
       "99355                                   0                        0  \n",
       "890251                                  0                        0  \n",
       "745817                                  0                        0  \n",
       "421793                                  0                        0  \n",
       "...                                   ...                      ...  \n",
       "310028                                  0                        0  \n",
       "663567                                  0                        0  \n",
       "1049160                                 0                        0  \n",
       "442785                                  0                        0  \n",
       "501372                                  0                        0  \n",
       "\n",
       "[86868 rows x 38 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values_subset = test_values\n",
    "test_values_subset[\"geo_level_1_id\"] = test_values_subset[\"geo_level_1_id\"].astype(\"category\")\n",
    "test_values_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "572a642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_m</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_a</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300051</th>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99355</th>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890251</th>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745817</th>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421793</th>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310028</th>\n",
       "      <td>605</td>\n",
       "      <td>3623</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663567</th>\n",
       "      <td>1407</td>\n",
       "      <td>11907</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049160</th>\n",
       "      <td>1136</td>\n",
       "      <td>7712</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442785</th>\n",
       "      <td>1041</td>\n",
       "      <td>912</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501372</th>\n",
       "      <td>36</td>\n",
       "      <td>6436</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86868 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             geo_level_2_id  geo_level_3_id  count_floors_pre_eq  age  \\\n",
       "building_id                                                             \n",
       "300051                  596           11307                    3   20   \n",
       "99355                   141           11987                    2   25   \n",
       "890251                   19           10044                    2    5   \n",
       "745817                   39             633                    1    0   \n",
       "421793                  289            7970                    3   15   \n",
       "...                     ...             ...                  ...  ...   \n",
       "310028                  605            3623                    3   70   \n",
       "663567                 1407           11907                    3   25   \n",
       "1049160                1136            7712                    1   50   \n",
       "442785                 1041             912                    2    5   \n",
       "501372                   36            6436                    2   10   \n",
       "\n",
       "             area_percentage  height_percentage  has_superstructure_adobe_mud  \\\n",
       "building_id                                                                     \n",
       "300051                     7                  6                             0   \n",
       "99355                     13                  5                             0   \n",
       "890251                     4                  5                             0   \n",
       "745817                    19                  3                             0   \n",
       "421793                     8                  7                             0   \n",
       "...                      ...                ...                           ...   \n",
       "310028                    20                  6                             0   \n",
       "663567                     6                  7                             1   \n",
       "1049160                    3                  3                             0   \n",
       "442785                     9                  5                             1   \n",
       "501372                    11                  4                             0   \n",
       "\n",
       "             has_superstructure_mud_mortar_stone  \\\n",
       "building_id                                        \n",
       "300051                                         1   \n",
       "99355                                          1   \n",
       "890251                                         1   \n",
       "745817                                         0   \n",
       "421793                                         1   \n",
       "...                                          ...   \n",
       "310028                                         1   \n",
       "663567                                         1   \n",
       "1049160                                        1   \n",
       "442785                                         1   \n",
       "501372                                         0   \n",
       "\n",
       "             has_superstructure_stone_flag  \\\n",
       "building_id                                  \n",
       "300051                                   0   \n",
       "99355                                    0   \n",
       "890251                                   0   \n",
       "745817                                   0   \n",
       "421793                                   0   \n",
       "...                                    ...   \n",
       "310028                                   0   \n",
       "663567                                   1   \n",
       "1049160                                  0   \n",
       "442785                                   0   \n",
       "501372                                   0   \n",
       "\n",
       "             has_superstructure_cement_mortar_stone  ...  \\\n",
       "building_id                                          ...   \n",
       "300051                                            0  ...   \n",
       "99355                                             0  ...   \n",
       "890251                                            0  ...   \n",
       "745817                                            0  ...   \n",
       "421793                                            0  ...   \n",
       "...                                             ...  ...   \n",
       "310028                                            0  ...   \n",
       "663567                                            0  ...   \n",
       "1049160                                           0  ...   \n",
       "442785                                            0  ...   \n",
       "501372                                            0  ...   \n",
       "\n",
       "             plan_configuration_m  plan_configuration_n  plan_configuration_o  \\\n",
       "building_id                                                                     \n",
       "300051                          0                     0                     0   \n",
       "99355                           0                     0                     0   \n",
       "890251                          0                     0                     0   \n",
       "745817                          0                     0                     0   \n",
       "421793                          0                     0                     0   \n",
       "...                           ...                   ...                   ...   \n",
       "310028                          0                     0                     0   \n",
       "663567                          0                     0                     0   \n",
       "1049160                         0                     0                     0   \n",
       "442785                          0                     0                     0   \n",
       "501372                          0                     0                     0   \n",
       "\n",
       "             plan_configuration_q  plan_configuration_s  plan_configuration_u  \\\n",
       "building_id                                                                     \n",
       "300051                          0                     0                     0   \n",
       "99355                           0                     0                     0   \n",
       "890251                          0                     0                     0   \n",
       "745817                          0                     0                     0   \n",
       "421793                          0                     0                     0   \n",
       "...                           ...                   ...                   ...   \n",
       "310028                          0                     0                     0   \n",
       "663567                          0                     0                     0   \n",
       "1049160                         0                     0                     0   \n",
       "442785                          0                     0                     0   \n",
       "501372                          0                     0                     0   \n",
       "\n",
       "             legal_ownership_status_a  legal_ownership_status_r  \\\n",
       "building_id                                                       \n",
       "300051                              0                         0   \n",
       "99355                               0                         0   \n",
       "890251                              0                         0   \n",
       "745817                              0                         0   \n",
       "421793                              0                         0   \n",
       "...                               ...                       ...   \n",
       "310028                              0                         0   \n",
       "663567                              0                         0   \n",
       "1049160                             0                         0   \n",
       "442785                              1                         0   \n",
       "501372                              0                         0   \n",
       "\n",
       "             legal_ownership_status_v  legal_ownership_status_w  \n",
       "building_id                                                      \n",
       "300051                              1                         0  \n",
       "99355                               1                         0  \n",
       "890251                              1                         0  \n",
       "745817                              1                         0  \n",
       "421793                              1                         0  \n",
       "...                               ...                       ...  \n",
       "310028                              0                         1  \n",
       "663567                              1                         0  \n",
       "1049160                             1                         0  \n",
       "442785                              0                         0  \n",
       "501372                              1                         0  \n",
       "\n",
       "[86868 rows x 98 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return(res) \n",
    "\n",
    "features_to_encode = [\"geo_level_1_id\", \"land_surface_condition\", \"foundation_type\", \"roof_type\",\\\n",
    "                     \"position\", \"ground_floor_type\", \"other_floor_type\",\\\n",
    "                     \"plan_configuration\", \"legal_ownership_status\"]\n",
    "for feature in features_to_encode:\n",
    "    test_values_subset = encode_and_bind(test_values_subset, feature)\n",
    "test_values_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "551c27e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868, 98)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b803c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Genero las predicciones para los test.\n",
    "preds = vc_model.predict(test_values_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c0657bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv('../../csv/submission_format.csv', index_col = \"building_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c56a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=preds,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00649f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300051</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99355</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890251</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745817</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421793</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             damage_grade\n",
       "building_id              \n",
       "300051                  3\n",
       "99355                   2\n",
       "890251                  2\n",
       "745817                  1\n",
       "421793                  3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cc3b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('../../csv/predictions/jf/vote/jf-model-3-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "155693dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id,damage_grade\r\n",
      "300051,3\r\n",
      "99355,2\r\n",
      "890251,2\r\n",
      "745817,1\r\n",
      "421793,3\r\n",
      "871976,2\r\n",
      "691228,1\r\n",
      "896100,3\r\n",
      "343471,2\r\n"
     ]
    }
   ],
   "source": [
    "!head ../../csv/predictions/jf/vote/jf-model-3-submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
